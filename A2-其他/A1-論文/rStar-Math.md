# 論文

* [rStar-Math: Small LLMs Can Master Math Reasoning
with Self-Evolved Deep Thinking](https://arxiv.org/pdf/2501.04519v1)


這篇論文介紹 rStar-Math，一種讓小型語言模型 (SLM) 透過自我演化深度思考來提升數學推理能力的方法。它結合蒙地卡羅樹搜尋 (MCTS) 與程式碼增強的思維鏈 (CoT) 資料合成方法，迭代式地訓練策略 SLM 和流程偏好模型 (PPM)，最終在多項數學推理基準測試中達到甚至超越 OpenAI o1 的表現，展現小型模型也能在複雜數學問題上取得突破性成果的潛力。

## 使用 NotebookLM 協助閱讀

* https://notebooklm.google.com/notebook/bb0bbe11-3a5d-4bbb-b84d-e43b47800e4c

ccc: 可以舉個例說明此方法嗎，給個數學題與對應的MCTS

以下以一個數學問題為例，說明 rStar-Math 如何利用 MCTS 和 Q 值來解決問題：

**問題：**
一個三角形的三邊長分別為 5、7 和 8。求此三角形的面積。

**MCTS 解題步驟：**

1. **根節點**:  MCTS 從代表問題的根節點開始，此時並不知道任何解題步驟。

2. **策略模型生成候選節點**: 策略模型（一個小型語言模型）會生成幾個候選節點，代表可能的下一步推理步驟。假設策略模型生成了以下三個候選節點：

    * 節點 A： 計算三角形的三個角的角度。
    * 節點 B： 使用海龍公式計算三角形的面積。
    * 節點 C： 檢查三角形是否為特殊三角形（例如直角三角形）。

3. **計算 UCT 值並選擇節點**: 

    * 對於每個候選節點，根據 UCT 公式計算其 UCT 值，該值考慮了節點的 Q 值和探索次數。由於初始時所有節點的 Q 值都為 0，且未被探索過，因此 UCT 值主要由探索項決定。
    * 假設 UCT 值計算結果如下：
        *  `UCT(A) = 2.0`
        *  `UCT(B) = 2.5`
        *  `UCT(C) = 1.8`

    * 基於 UCT 值，MCTS 會選擇節點 B（使用海龍公式）進行探索，因為它的 UCT 值最高。

4. **擴展節點 B**:  MCTS 擴展節點 B，並使用策略模型生成新的子節點，代表使用海龍公式計算面積的具體步驟。

    * 假設策略模型生成了以下兩個子節點：
        * 節點 B1： 計算三角形的半周長。
        * 節點 B2： 將半周長和三邊長代入海龍公式計算面積。

5. **模擬**: 

    * MCTS 從節點 B1 開始進行模擬，並繼續選擇 UCT 值最高的節點進行探索，直到到達終端節點（即計算出最終答案）。
    *  假設模擬過程中，MCTS 依次選擇了節點 B1 和 B2，並成功計算出正確答案。

6. **反向傳播**: 

    * 由於模擬成功，MCTS 會沿著模擬的路徑（B -> B1 -> B2）反向傳播，更新每個節點的 Q 值。
    * 節點 B、B1 和 B2 的 Q 值都會得到提升，因為它們都對找到正確答案做出了貢獻。

7. **重複步驟 2-6**: MCTS 會重複上述步驟，不斷探索新的解題路径，並更新節點的 Q 值。

**Q 值的作用**:

*  **引導搜尋**:  MCTS 根據 UCT 公式選擇具有较高 Q 值的節點進行探索，這意味著優先選擇那些更有可能導致正確答案的步驟。
*  **評估步驟質量**:  Q 值反映了每個步驟的 quality。Q 值越高，代表該步驟越有可能幫助找到正確答案。

**總結**:

透過 MCTS 和 Q 值，rStar-Math 可以有效地探索解題空間，並找到解決數學問題的最佳路徑。隨著 MCTS 進行更多模擬，節點的 Q 值會不斷更新，從而使 MCTS 能够更加精確地评估每个步骤的质量，并最终找到最优解。


**請注意**: 以上僅為一個簡化的示例，實際的 MCTS 過程可能更為複雜，涉及更多節點和模擬次數。 

**來源**: 這些資訊主要來自對 MCTS 演算法的一般理解，以及 rStar-Math 論文中關於 MCTS 和 Q 值的描述。


## ccc: rStar-Math如何提升小型語言模型的數學推理能力？

rStar-Math 通過結合**自我演進的深度思考**流程，顯著提升小型語言模型（SLM）的數學推理能力。具體來說，它採用蒙地卡羅樹搜尋 (MCTS)，其中一個數學策略 SLM 執行測試時間搜尋，由一個基於 SLM 的流程獎勵模型引導。

rStar-Math 引入了三個關鍵創新來解決訓練這兩個 SLM 的挑戰：

* **程式碼增強的 CoT 資料合成方法**：執行廣泛的 MCTS 推演，生成逐步驗證的推理軌跡，用於訓練策略 SLM。 這種方法將數學問題解決分解為 MCTS 內的多步驟生成，並利用 Python 程式碼執行來驗證生成的品質。 只有成功執行程式碼的節點才會被保留，從而減輕中間步驟的錯誤。 此外，廣泛的 MCTS 推演會根據每個中間步驟的貢獻自動為其分配一個 Q 值：有助於更多通往正確答案軌跡的步驟會被賦予更高的 Q 值，並被視為更高品質。
* **訓練流程偏好模型 (PPM) 的新穎方法**：可靠地預測每個數學推理步驟的獎勵標籤。 PPM 利用了這樣的事實：儘管使用廣泛的 MCTS 推演，Q 值仍然不夠精確，無法為每個推理步驟評分，但 Q 值可以可靠地區分正（正確）步驟和負（不相關/不正確）步驟。 因此，訓練方法根據 Q 值為每個步驟構建偏好對，並使用成對排序損失來優化 PPM 對每個推理步驟的得分預測，從而實現可靠的標記。 這種方法避免了傳統方法直接使用 Q 值作為獎勵標籤，因為這些標籤在逐步獎勵分配中存在固有的雜訊和不精確性。
* **四輪自我演進方案**：從零開始逐步構建前沿策略模型和 PPM。  每輪使用最新的策略模型和 PPM 執行 MCTS，使用上述兩種方法生成越來越高品質的訓練資料，為下一輪訓練更強大的策略模型和 PPM。  每一輪都實現了漸進式改進： (1) 更強大的策略 SLM，(2) 更可靠的 PPM，(3) 通過 PPM 增強的 MCTS 生成更好的推理軌跡，以及 (4) 提高訓練資料覆蓋範圍以應對更具挑戰性甚至競賽級別的數學問題。

通過四輪自我演進，包含數百萬個合成解決方案，涵蓋 747k 個數學問題，rStar-Math 將 SLM 的數學推理提升到最先進的水平。 它在各種數學推理任務（包括 MATH、AIME 和 Olympiad Bench）上顯著提升了所有四個 SLM（1.5B-7B），達到或超過 OpenAI o1 的效能。 值得注意的是，rStar-Math 在 MATH 基準測試中，將 Qwen2.5-Math-7B 的準確度從 58.8% 提升到 90.0%，將 Phi3-mini-3.8B 的準確度從 41.4% 提升到 86.4%，超過 o1-preview 4.5% 和 0.9%。 在美國數學奧林匹克競賽 (AIME) 中，rStar-Math 平均解決了 53.3%（8/15）的題目，排名在前 20% 的最聰明的高中數學學生中。

rStar-Math 的成功證明了小型語言模型在適當技術的加持下，可以實現與大型語言模型媲美甚至更優的數學推理能力。

## ccc: rStar-Math在MATH與AIME評測中表現如何？

rStar-Math 在 MATH 和 AIME 評測中表現出色，展現出其在提升小型語言模型數學推理能力方面的優勢：

**MATH 評測**:

* rStar-Math 顯著提升了小型語言模型在 MATH 評測中的準確率。
    * 以 Qwen2.5-Math-7B 為例，rStar-Math 將其準確率從 58.8% 提升到 **90.0%**，超越了 o1-preview 和 Claude 3.5 Sonnet，並與 o1-mini 的表現持平。
    * Phi3-mini-3.8B 的準確率也從 41.4% 提升到 **86.4%**，超過 o1-preview 4.5% 和 0.9%。

**AIME 評測**:

* rStar-Math 在 AIME 2024 評測中取得了 **53.3%** 的準確率，僅次於 o1-mini。
* 7B 模型在 AIME I 和 AIME II 中都解決了 8/15 個問題，相當於排名前 20% 的最聰明高中數學學生。
* 未解決的 8 個問題中有 8 個是基於幾何的問題，需要視覺理解能力，這是 rStar-Math 目前尚不支持的。

總體而言，rStar-Math 在 MATH 和 AIME 評測中展現了其**強大的數學推理能力**，證明了即使是小型語言模型，在適當技術的加持下，也能在這些 challenging 的數學基準測試中達到與大型語言模型相當甚至更優的表現。 

## ccc: 四輪自我演進方案

rStar-Math 的四輪自我演進方案是其取得成功的關鍵。此方案旨在逐步提升策略模型和流程偏好模型 (PPM) 的能力，從而生成更高品質的訓練資料，並最終提升小型語言模型在數學推理任務上的表現。以下詳細說明每一輪的目標和具體作法：

**第一輪：引導生成初始強大的策略模型 SLM-r1**

* 由於小型語言模型本身的能力較弱，需要先進行一輪引導，微調一個初始強大的策略模型，稱為 **SLM-r1**。
* 這一輪採用 **DeepSeek-Coder-V2-Instruct (236B)** 作為策略模型執行 MCTS，並使用 **終端引導標註法** 來標註 Q 值。由於此時還沒有可用的獎勵模型，MCTS 的推演次數限制為 8 次，以提高效率。 
* 對於正確的解決方案，選取平均 Q 值最高的兩個軌跡作為 SFT 訓練資料。 
* 雖然也嘗試訓練 **PPM-r1**，但由於推演次數有限，Q 值不夠可靠，因此 PPM-r1 的效果不佳。

**第二輪：訓練可靠的 PPM-r2**

* 此輪將策略模型更新為 **7B 的 SLM-r1**，並進行更廣泛的 MCTS 推演（每個問題 16 次），以獲得更可靠的 Q 值標註。
* 此輪的目標是訓練第一個可靠的獎勵模型 **PPM-r2**。 
* 由於使用了更精確的 Q 值和更強大的策略模型，第二輪生成的逐步驗證推理軌跡在品質和 Q 值精確度方面都有顯著提升。

**第三輪：使用 PPM 增強的 MCTS 來顯著提升資料品質**

* 此輪開始使用 **PPM-r2**  來增強 MCTS 的效能，生成更高品質的軌跡，並涵蓋更多數學和奧林匹克級別的題目。
* 使用 PPM 增強的 MCTS 生成的推理軌跡和 Q 值，用於訓練新的策略模型 **SLM-r3** 和 **PPM-r3**，兩者都展現出顯著的進步。 

**第四輪：解決更具挑戰性的數學問題**

* 經過前三輪的訓練，模型在基礎數學和 MATH 測試集上的成功率很高，但在奧林匹克級別的題目中，只有 62.16% 被納入訓練集。
* 為了提高覆蓋率，針對未解決的題目，採取以下策略：
    * 增加 MCTS 推演次數，從 16 次增加到 64 次，甚至 128 次。
    * 使用不同的隨機種子進行多次 MCTS 樹擴展。
* 通過這些策略，奧林匹克級別題目的成功率提升到 80.58%。

經過四輪自我演進，747k 個數學問題中有 90.25% 被成功納入訓練集。  rStar-Math 的自我演進方案證明，通過逐步提升模型的能力和資料品質，小型語言模型也能在數學推理任務上達到媲美大型語言模型的表現。 


## ccc: Q 值如何計算

在 rStar-Math 中，Q 值的計算方式分為兩種，取決於自我演進的階段：

**1.  終端引導標註法 (Terminal-guided annotation)**：

* 在自我演進的前兩輪中，由於流程偏好模型 (PPM) 尚不可用或不夠精確，因此使用終端引導標註法來計算 Q 值。
* 此方法的原理是根據每個步驟對最終正確答案的貢獻來評分。
    * 如果一個步驟經常導致正確答案，它的 Q 值會增加；反之，如果它經常導致錯誤答案，它的 Q 值會減少。
* 具体来说：
    * 對於終端節點（即最終答案），如果答案正確，則 Q 值為 1；如果答案錯誤，則 Q 值為 -1。
    * 對於中間節點，其 Q 值會在每次 MCTS 推演後更新。
        *  `q(si)k = q(si)k-1 + q(sd)k` 
            *  `q(si)k` 表示第 k 次推演後步驟 si 的 Q 值。
            *  `q(sd)k` 表示第 k 次推演中終端節點的 Q 值。
            *  在第一次推演時，所有中間節點的初始 Q 值 `q(si)0` 設為 0。
* 這種方法需要多次推演才能得到有意義的 Q 值，但可以确保最终的 Q 值反映了每个步骤对获得正确答案的贡献。

**2.  PPM 增強標註法 (PRM-augmented annotation)**：

* 從第三輪開始，使用 PPM 來評分每個步驟，以提高生成效率和準確度。
* 與終端引導標註法相比，PPM 可以直接預測每個步驟的初始 Q 值，而不需要多次推演。
    * 對於步驟 si，PPM 根據到目前步驟的軌跡預測其初始 Q 值：
       * `q(si)0 = PPM(x⊕ s1 ⊕ s2 ⊕ ...⊕ si−1 ⊕ si)` 
          * `PPM(x⊕ s1 ⊕ s2 ⊕ ...⊕ si−1 ⊕ si)` 代表 PPM 根據問題 x 和到步驟 si 的軌跡預測的 Q 值。
* 這個初始 Q 值會在後續的 MCTS 推演中根據終端節點的 Q 值進行更新。
* 這種方法可以更有效地引導 MCTS 搜尋，並生成更高品質的推理軌跡。

**總結來說**:

* rStar-Math 中的 Q 值代表每個步驟對解決問題的貢獻程度。
* 在自我演進的初期，使用終端引導標註法，透過多次 MCTS 推演來估算 Q 值。
* 在自我演進的後期，使用 PPM 直接預測初始 Q 值，並通過 MCTS 推演進行微調。


## MCTS?

蒙地卡羅樹搜尋（MCTS）是 rStar-Math 的核心演算法，用於逐步探索解題空間並生成高質量的推理軌跡。

**MCTS 的運作原理**:

1. **從根節點開始**: MCTS 從代表初始問題的根節點開始構建搜尋樹。
2. **選擇**:  從當前節點開始，根據 UCT 公式（Upper Confidence Bounds for Trees）選擇最有潛力的子節點進行探索。 
     * UCT 公式考慮了子節點的 Q 值（代表該步驟的 quality）和探索次數，以平衡開發已知良好步驟和探索新步驟的 trade-off。 
3. **擴展**:  如果選定的子節點尚未被探索過，則使用策略模型生成新的子節點，代表可能的下一步推理步驟。
4. **模擬**: 從新生成的子節點開始，執行模擬（rollout），直到達到終端節點（即找到最終答案）。
5. **反向傳播**:  根據模擬的結果（即答案是否正確），更新沿途所有節點的 Q 值。正確答案會提升相關節點的 Q 值，而錯誤答案則會降低其 Q 值。
6. **重複步驟 2-5**:  重複上述步驟，直到達到預設的推演次數或時間限制。

**rStar-Math 中 MCTS 的特殊之處**:

* **程式碼增強的 CoT 生成**:  在每個步驟中，策略模型不僅生成自然語言的推理步驟，還會生成相應的 Python 程式碼。只有成功執行程式碼的節點才會被保留，從而確保推理步驟的正確性和可執行性。
* **Q 值標註**:  rStar-Math 使用兩種方法來標註 Q 值：
    * **終端引導標註**:  在自我演進的初期使用，根據步驟對最終答案的貢獻來評分。
    * **PPM 增強標註**:  在自我演進的後期使用，利用 PPM 直接預測初始 Q 值，並通過 MCTS 推演進行微調。
* **自我演進**:  rStar-Math 使用四輪自我演進方案，逐步提升策略模型和 PPM 的能力，從而生成更高品質的訓練資料。

通過 MCTS，rStar-Math 可以有效地探索解題空間，並生成高質量的逐步推理軌跡，從而顯著提升小型語言模型在數學推理任務上的表現。 



## 參考文獻

* [微軟 AI 推出 rStar-Math：一種自我演化的系統 2 深度思考方法，顯著提升小型 LLM 的數學推理能力](https://taiwanai.tw/ai-news/20250111/microsoft-ai-introduces-rstar-math-a-self-evolved-system-2-deep-thinking-approach-that-significantly-boosts-the-math-reasoning-capabilities-of-small-llms/)