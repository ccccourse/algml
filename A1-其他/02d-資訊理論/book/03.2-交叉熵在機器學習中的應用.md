### 交叉熵在機器學習中的應用

---

### **1. 交叉熵的定義**

交叉熵 (\(H(P, Q)\)) 是衡量兩個概率分布 \(P\) 和 \(Q\) 之間差異的一種度量，特別是在信息編碼中，交叉熵描述了使用分布 \(Q\) 來編碼分布 \(P\) 時的期望編碼長度。

#### 數學公式：
對於離散隨機變數 \(X\)：
\[
H(P, Q) = - \sum_{x \in X} P(x) \log Q(x)
\]
對於連續隨機變數：
\[
H(P, Q) = - \int P(x) \log Q(x) \, dx
\]

- \(P(x)\)：實際分布。
- \(Q(x)\)：模型分布或預測分布。

---

### **2. 交叉熵在分類問題中的應用**

#### **2.1. 二元分類**

在二元分類中，模型的預測分布 \(Q(x)\) 通常由 \(y_{\text{pred}} = \sigma(z)\) 表示（\(z\) 是模型的輸出，\(\sigma(z)\) 是 Sigmoid 函數）。交叉熵損失函數定義為：
\[
\mathcal{L} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log y_{\text{pred}, i} + (1 - y_i) \log (1 - y_{\text{pred}, i}) \right]
\]

- \(y_i\)：真實標籤（\(0\) 或 \(1\)）。
- \(y_{\text{pred}, i}\)：模型對 \(i\)-th 樣本的預測概率。

交叉熵損失在二元分類中表達了模型預測與真實標籤之間的不確定性。

---

#### **2.2. 多分類**

在多分類問題中，預測分布 \(Q(x)\) 通常由 Softmax 函數計算，表示每個類別的概率分布：
\[
Q(x) = \text{Softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^{C} \exp(z_j)}
\]

多分類交叉熵損失定義為：
\[
\mathcal{L} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{i, j} \log y_{\text{pred}, i, j}
\]

- \(C\)：類別數。
- \(y_{i, j}\)：第 \(i\)-th 樣本對第 \(j\)-th 類別的真實標籤（通常為 one-hot 表示）。
- \(y_{\text{pred}, i, j}\)：模型對第 \(j\)-th 類別的預測概率。

---

### **3. 為什麼使用交叉熵？**

1. **對數懲罰**：
   - 使用對數函數，對錯誤預測進行更大的懲罰，從而加速模型學習。

2. **概率分布的自然度量**：
   - 交叉熵損失直接衡量模型預測的分布與真實分布之間的差異。

3. **數學性質**：
   - 最小化交叉熵損失等價於最大化似然估計（MLE），這是一種統計學上的標準方法。

---

### **4. 在深度學習中的應用場景**

#### **4.1. 圖像分類**
- CNN 模型通常用於處理圖像分類任務。輸出層使用 Softmax 函數生成類別概率分布，交叉熵損失用於訓練模型。
- 例如，在 MNIST 手寫數字分類中，目標是最小化交叉熵損失以提高準確率。

#### **4.2. 文本分類與語言建模**
- 在 NLP 任務中（如情感分析、主題分類），模型的輸出為各類別的概率分布，交叉熵損失衡量模型預測與真實標籤的差異。

#### **4.3. 目標檢測與分割**
- 在目標檢測任務中（如 YOLO 或 Faster R-CNN），分類分支使用交叉熵損失來預測對應框的目標類別。

#### **4.4. 時序數據與語音識別**
- 在 RNN 或 Transformer 模型中，交叉熵損失用於訓練語音識別和語言生成模型（如 GPT 或 BERT）。

---

### **5. 訓練中的技巧與挑戰**

1. **數值穩定性**：
   - 為避免 \(\log(0)\) 問題，實現中常加入小的數值偏移量 \(\epsilon\)，如 \(\log(y + \epsilon)\)。

2. **類別不平衡**：
   - 當類別分布不均衡時，可引入加權交叉熵損失，對不同類別賦予不同權重：
     \[
     \mathcal{L} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} \alpha_j y_{i, j} \log y_{\text{pred}, i, j}
     \]

3. **標籤平滑**：
   - 防止模型過度自信，將真實標籤 \(y\) 轉化為平滑分布：
     \[
     y' = (1 - \epsilon) y + \frac{\epsilon}{C}
     \]
     其中 \(\epsilon\) 是平滑參數。

---

### **6. 範例**

#### **Python 中的實現：PyTorch**
```python
import torch
import torch.nn as nn

# 模擬輸出（logits）和真實標籤
logits = torch.tensor([[2.0, 0.5, 0.3], [0.1, 1.0, 0.2]])
labels = torch.tensor([0, 1])  # 真實類別索引

# 定義交叉熵損失
criterion = nn.CrossEntropyLoss()

# 計算損失
loss = criterion(logits, labels)
print(f"交叉熵損失: {loss.item()}")
```

---

### **7. 延伸與相關損失函數**

1. **Binary Cross-Entropy (BCE)**：
   - 用於二元分類。

2. **Categorical Cross-Entropy**：
   - 用於多分類。

3. **KL 散度**：
   - 可以視為交叉熵與真實分布熵的差值，常用於正則化或模型評估。

4. **Focal Loss**：
   - 改進交叉熵損失，用於處理類別不平衡的問題：
     \[
     \mathcal{L}_{\text{focal}} = - \alpha (1 - p_t)^\gamma \log(p_t)
     \]

交叉熵損失作為機器學習中的核心損失函數，適用於多種應用場景，其理論基礎和實踐意義對深度學習的發展具有深遠影響。