在資訊理論中，**資訊**和**熵**是兩個核心概念，幫助我們理解如何量化訊息的內容和不確定性。

### **1. 資訊的基本概念**
**資訊**是用來消除不確定性、降低未知性的一種度量。在資訊理論中，資訊通常是指一條訊息中所包含的"新知識"。當一個事件發生時，若這個事件的發生對我們來說是完全預期的，那麼它所帶來的資訊量會很少；反之，如果這個事件是完全不確定的，則它帶來的資訊量較大。

#### **資訊量 (Information Content)**
資訊的量通常用**比特**（bit）來衡量。一條訊息的資訊量與該事件發生的概率有關。對於一個概率為 \( p \) 的事件，其資訊量 \( I \) 可以用下式表示：
\[
I(x) = -\log_2(p(x))
\]
其中 \( p(x) \) 是事件 \( x \) 發生的概率。

### **2. 熵的基本概念**
**熵**（Entropy）是用來量化一個隨機變數的**不確定性**或**信息的平均量**。它反映了系統中所有可能狀態的平均資訊量。在資訊理論中，熵的概念由香农（Shannon）提出，用來衡量一個隨機變數的分布不確定性。

對於一個離散隨機變數 \( X \)，其熵 \( H(X) \) 定義為：
\[
H(X) = - \sum_{i=1}^{n} p(x_i) \log_2(p(x_i))
\]
其中，\( p(x_i) \) 是隨機變數 \( X \) 取到狀態 \( x_i \) 的概率，\( n \) 是可能的狀態總數。

熵的單位是比特（bit），表示隨機變數每次取值所帶來的不確定性的量。熵越大，系統的不確定性越高，反之則越小。

### **3. 熵的直觀理解**
- 當所有事件的發生概率相同時，熵達到最大值，因為我們無法預測每個事件的結果（即最不確定）。
- 當某一事件的概率接近 1，而其他事件的概率接近 0 時，熵的值較小，系統的不確定性較低，訊息量也較少。

### **例子**
假設有一個硬幣，若硬幣是公平的（即正反兩面機率各 50%），那麼每次擲硬幣的熵是：
\[
H(X) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ bit}
\]
這表示每次擲硬幣都提供 1 bit 的資訊。

如果硬幣是偏的，例如正面朝上的機率是 0.9，而反面朝上的機率是 0.1，那麼熵將會較小：
\[
H(X) = - (0.9 \log_2 0.9 + 0.1 \log_2 0.1) \approx 0.468 \text{ bit}
\]
這意味著我們對於硬幣的結果更有預測性，因此熵較低。

### **4. 熵與其他概念的關係**
熵是信息理論中的基礎，許多進階概念都與熵密切相關，例如：
- **互信息**（Mutual Information）：衡量兩個隨機變數之間共享的資訊。
- **條件熵**（Conditional Entropy）：在知道某些條件下，隨機變數的不確定性。
- **訊息量**：衡量單一事件帶來的資訊量。

這些概念在後續學習和應用中將會深入討論。