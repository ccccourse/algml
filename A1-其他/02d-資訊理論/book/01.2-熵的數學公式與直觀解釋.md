### 熵的數學公式與直觀解釋

---

**1. 熵的數學公式**  

熵（Entropy）是資訊理論中用來量化系統不確定性的一個核心概念。克勞德·香農提出熵的公式如下：

\[
H(X) = -\sum_{i} P(x_i) \log P(x_i)
\]

其中：
- \( X \) 是一個隨機變數，表示可能的事件集合。
- \( P(x_i) \) 是事件 \( x_i \) 的概率。
- \( \log \) 是對數運算，通常以2為底（結果單位為比特）或自然對數（結果單位為 nat）。

**計算流程：**
1. 計算每個事件的概率 \( P(x_i) \)。
2. 計算對應的資訊量 \( I(x_i) = -\log P(x_i) \)。
3. 以概率作為權重加總所有事件的資訊量。

---

**2. 熵的直觀解釋**  

熵描述的是隨機變數的不確定性程度，或者說「預測的困難程度」。高熵意味著系統的不確定性高，低熵意味著系統較為確定。以下用幾個情境來解釋：

### 2.1 完全確定的系統
假設 \( X \) 是一個總是發生某事件 \( x_1 \) 的系統，則：
- \( P(x_1) = 1 \)，其他事件的概率 \( P(x_i) = 0 \)。
- 熵計算為：
  \[
  H(X) = -[P(x_1) \log P(x_1)] = -[1 \cdot \log 1] = 0
  \]
- **直觀解釋**：系統完全確定，沒有不確定性，因此熵為 0。

---

### 2.2 完全隨機的系統
假設 \( X \) 是一個均勻分布的系統（如擲一枚公平硬幣），則：
- 對於硬幣的兩個結果 \( \{正面, 反面\} \)，各自的概率為 \( P(正面) = P(反面) = 0.5 \)。
- 熵計算為：
  \[
  H(X) = -[0.5 \log 0.5 + 0.5 \log 0.5] = -[0.5 \cdot (-1) + 0.5 \cdot (-1)] = 1 \, \text{比特}
  \]
- **直觀解釋**：每次擲硬幣都存在等概率的不確定性，因此熵達到最高。

---

### 2.3 偏向的系統
假設 \( X \) 是一個偏向分布的系統（如擲一枚不公平硬幣），\( P(正面) = 0.9 \)，\( P(反面) = 0.1 \)：
- 熵計算為：
  \[
  H(X) = -[0.9 \log 0.9 + 0.1 \log 0.1]
  \]
  - 計算對應值：
    \[
    H(X) \approx -[0.9 \cdot (-0.0458) + 0.1 \cdot (-3.3219)] \approx 0.469 \, \text{比特}
    \]
- **直觀解釋**：硬幣偏向一側，不確定性降低，因此熵比公平硬幣小。

---

**3. 熵的性質**  

1. **非負性（Non-negativity）**：  
   熵永遠不為負，因為概率 \( P(x_i) \) 位於 \( (0, 1] \)，其對數為負，乘上負號後為正。  
   \[
   H(X) \geq 0
   \]

2. **最大值與均勻分布**：  
   當所有事件的概率均相等（均勻分布）時，熵達到最大值。對於 \( n \) 個事件：
   \[
   H_{\text{max}} = \log n
   \]

3. **加性（Additivity）**：  
   對於兩個獨立變數 \( X \) 和 \( Y \)，聯合熵等於兩者的熵之和：  
   \[
   H(X, Y) = H(X) + H(Y)
   \]

---

**4. 熵的應用場景**  

1. **壓縮**：熵表示理論上最少需要多少比特來表示隨機變數的信息量，是壓縮技術的基礎。
2. **機器學習**：交叉熵（Cross-Entropy）與信息增益（Information Gain）是分類問題中的關鍵概念。
3. **通訊**：在信道傳輸中，熵用來衡量信息量及評估通訊效率。

熵是一個橋接數學與實際應用的關鍵工具，揭示了不確定性與信息的深層聯繫。