### 資訊的定義與性質

**1. 資訊的定義**  
資訊（Information）的概念源自對「不確定性」的理解，是用來描述對某個事件或系統不確定性減少的程度。克勞德·香農（Claude Shannon）在其開創性的資訊理論中，將資訊形式化為數學表達式，用來定量描述訊息內容。

- **事件與概率的關聯**：  
  資訊的量與事件的概率有關。發生概率越低的事件，提供的資訊量越大。資訊量可定義為：  
  \[
  I(x) = -\log P(x)
  \]  
  其中，\( I(x) \) 是事件 \( x \) 所帶來的資訊量，\( P(x) \) 是事件 \( x \) 的概率，\(\log\) 是對數（通常以2為底，表示以比特為單位）。

- **範例**：  
  1. 擲一枚公平硬幣（概率 \( P = 0.5 \)）：  
     資訊量 \( I(x) = -\log_2(0.5) = 1 \) 比特。
  2. 擲一個六面骰子（概率 \( P = \frac{1}{6} \)）：  
     資訊量 \( I(x) = -\log_2(\frac{1}{6}) \approx 2.585 \) 比特。

---

**2. 資訊的性質**  
資訊具有以下幾個重要性質：

1. **非負性（Non-negativity）**：  
   資訊量總是非負的，因為事件的概率 \( P(x) \) 位於區間 \( (0, 1] \)，其對數值為負，取負號後結果為正。

   \[
   I(x) = -\log P(x) \geq 0
   \]

2. **概率越低，資訊量越大**：  
   發生概率小的事件不常見，因此包含更多資訊。反之，概率高的事件資訊量較少。

3. **加性（Additivity）**：  
   如果兩個事件是獨立的，其資訊量可以相加：  
   \[
   I(x, y) = I(x) + I(y)
   \]

4. **對稱性（Symmetry）**：  
   資訊量的計算與事件本身無關，只與其概率有關，因此對不同的事件，只要概率相同，資訊量也相同。

---

**3. 資訊的直觀意義**  
- 資訊的本質是消除不確定性。例如，在一個完全未知的情況下（所有事件等可能），獲得一條訊息後，能明確識別哪個事件發生，這條訊息就有很高的價值。
- 信息的單位通常是 **比特（bit）** 或 **nat**，依據對數的底數選擇（2為底為比特，自然對數為nat）。

這些定義和性質構成了資訊理論的基石，也為後續熵（Entropy）、互信息（Mutual Information）等概念的建立提供了基礎。