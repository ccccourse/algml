以下是「03-反傳遞算法」章節的範例結構：

---

# 03-反傳遞算法

## 3.1 反傳遞算法簡介
反傳遞算法（Backpropagation）是訓練多層神經網絡的核心技術。它是基於梯度下降法的優化算法，用於計算神經網絡中每個權重的梯度，從而更新權重，最小化損失函數。該算法由Rumelhart、Hinton和Williams於1986年提出，為深度學習的發展奠定了基礎。

## 3.2 神經網絡中的前向傳播
在介紹反傳遞之前，首先了解前向傳播（Forward Propagation）。前向傳播是指輸入數據通過網絡層逐層傳遞，直到輸出層產生預測值的過程。每一層的輸出可以表示為：
\[
a^{[l]} = \sigma(z^{[l]})
\]
其中：
- \(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\)
- \(W^{[l]}\) 是權重矩陣，\(b^{[l]}\) 是偏置，\(a^{[l-1]}\) 是前一層的輸出，\(\sigma\) 是激活函數。

## 3.3 反傳遞算法的基本思想
反傳遞算法的核心目標是根據損失函數的梯度來更新神經網絡中的權重和偏置。當網絡結構較為複雜時，直接計算每個權重對損失函數的偏導數變得困難，因此反傳遞算法引入了鏈式法則（Chain Rule），通過逐層反向計算每個節點的梯度。

### 3.3.1 損失函數
常見的損失函數是均方誤差（MSE）或交叉熵（Cross Entropy）。以均方誤差為例，其定義為：
\[
L = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2
\]
其中，\(y^{(i)}\) 是真實值，\(\hat{y}^{(i)}\) 是預測值，\(m\) 是樣本數量。

### 3.3.2 鏈式法則
鏈式法則用於計算復合函數的導數。在反傳遞算法中，我們使用鏈式法則從輸出層開始逐層反向傳播誤差，計算每個權重和偏置的梯度。

## 3.4 反傳遞算法的步驟
反傳遞算法的流程可以分為以下幾個步驟：

### 3.4.1 前向傳播
首先，將輸入數據通過網絡進行前向傳播，計算每一層的輸出，直到最終得到預測值。

### 3.4.2 計算輸出層的誤差
對於輸出層，我們可以根據損失函數計算誤差：
\[
\delta^{[L]} = \hat{y} - y
\]
其中，\(\delta^{[L]}\) 是輸出層的誤差，\(\hat{y}\) 是預測值，\(y\) 是真實標籤。

### 3.4.3 反向計算隱藏層的誤差
根據輸出層的誤差，我們利用鏈式法則逐層反向計算隱藏層的誤差：
\[
\delta^{[l]} = (W^{[l+1]})^T \delta^{[l+1]} \odot \sigma'(z^{[l]})
\]
其中，\(\odot\) 表示逐元素相乘，\(\sigma'(z^{[l]})\) 是激活函數的導數。

### 3.4.4 更新權重和偏置
使用計算出的誤差來更新每一層的權重和偏置。更新公式如下：
\[
W^{[l]} = W^{[l]} - \eta \cdot \frac{\partial L}{\partial W^{[l]}}
\]
\[
b^{[l]} = b^{[l]} - \eta \cdot \frac{\partial L}{\partial b^{[l]}}
\]
其中，\(\eta\) 是學習率。

## 3.5 反傳遞算法的Python實現
以下是一個簡單的反傳遞算法實現，用於訓練一個具有單個隱藏層的神經網絡：

```python
import numpy as np

# 激活函數及其導數
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# 初始化參數
def initialize_parameters(input_size, hidden_size, output_size):
    W1 = np.random.randn(hidden_size, input_size)
    b1 = np.zeros((hidden_size, 1))
    W2 = np.random.randn(output_size, hidden_size)
    b2 = np.zeros((output_size, 1))
    return W1, b1, W2, b2

# 前向傳播
def forward_propagation(X, W1, b1, W2, b2):
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    return Z1, A1, Z2, A2

# 反向傳播
def backward_propagation(X, Y, Z1, A1, Z2, A2, W2):
    m = X.shape[1]
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    dZ1 = np.dot(W2.T, dZ2) * sigmoid_derivative(Z1)
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    return dW1, db1, dW2, db2

# 更新參數
def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    return W1, b1, W2, b2

# 訓練神經網絡
def train(X, Y, input_size, hidden_size, output_size, iterations=1000, learning_rate=0.1):
    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)
    
    for i in range(iterations):
        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)
        dW1, db1, dW2, db2 = backward_propagation(X, Y, Z1, A1, Z2, A2, W2)
        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)
        
        if i % 100 == 0:
            loss = np.mean((A2 - Y) ** 2)
            print(f"Iteration {i}, Loss: {loss}")
    
    return W1, b1, W2, b2
```

## 3.6 結論
反傳遞算法通過有效地計算每個參數的梯度，讓神經網絡能夠從訓練數據中學習，並逐步逼近目標函數的最小值。它是深度學習中不可或缺的技術，雖然簡單，但結合現代的優化方法如動量、Adam等可以顯著提升神經網絡的性能。

---

這是「03-反傳遞算法」章節的初步框架，具體內容可以根據書的方向進行擴展。