以下是「04-卷積神經網路」章節的範例結構：

---

# 04-卷積神經網路（CNN）

## 4.1 卷積神經網路簡介
卷積神經網路（Convolutional Neural Network, CNN）是一種特殊的深度神經網路，主要應用於圖像處理和計算機視覺領域。CNN利用卷積層提取圖像中的空間結構特徵，通過共享參數的機制降低模型的複雜度。

### 4.1.1 CNN的典型應用
CNN廣泛應用於圖像分類、物體檢測、語音識別等領域，特別是自2012年AlexNet的誕生後，CNN在各類圖像競賽中取得了突破性的成果。

## 4.2 卷積神經網路的基本結構
典型的卷積神經網路由多個層次構成，主要包括：
- **卷積層（Convolutional Layer）**
- **池化層（Pooling Layer）**
- **全連接層（Fully Connected Layer）**

這些層次共同工作，實現圖像特徵的提取與分類。

### 4.2.1 卷積層
卷積層是CNN的核心組件。卷積層使用稱為「卷積核（Filter）」的小矩陣來掃描輸入數據，從而提取局部的特徵。卷積運算可以描述為：
\[
Z_{i,j} = \sum_{m=1}^{k}\sum_{n=1}^{k} X_{i+m-1, j+n-1} \cdot W_{m,n}
\]
其中，\(X\) 是輸入圖像，\(W\) 是卷積核，\(k\) 是卷積核的尺寸。

#### 卷積核的作用
卷積核能夠提取不同的圖像特徵，如邊緣、角點等。多個卷積核可以並行運行，生成不同的特徵圖（Feature Map）。

### 4.2.2 激活函數
在每一個卷積層之後，通常會應用非線性激活函數來增加網絡的表達能力。最常用的激活函數是ReLU（Rectified Linear Unit），其定義為：
\[
\text{ReLU}(x) = \max(0, x)
\]
ReLU有助於解決神經網絡中的梯度消失問題，並加速訓練過程。

### 4.2.3 池化層
池化層的主要作用是進行降維，減少特徵圖的空間尺寸，從而減少參數量和計算量。常見的池化方法有最大池化（Max Pooling）和平均池化（Average Pooling）。最大池化操作為：
\[
P_{i,j} = \max_{m,n} (Z_{i+m-1, j+n-1})
\]
其中，\(P\) 是池化後的結果。

### 4.2.4 全連接層
全連接層的作用是將提取的高層次特徵轉換為最終的輸出結果。通常會使用Softmax函數作為分類任務的最終輸出激活函數。Softmax函數將輸出轉換為概率分佈，公式為：
\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]

## 4.3 CNN的特性
### 4.3.1 局部連接
卷積層只對局部區域進行運算，這使得CNN能夠捕捉輸入數據中的局部特徵，同時大大減少了參數量。

### 4.3.2 參數共享
在同一特徵圖中，所有的神經元共享相同的卷積核參數，這進一步減少了需要訓練的參數，並防止過擬合。

### 4.3.3 平移不變性
卷積操作使得CNN對輸入圖像的小幅度平移具有不變性，這意味著即使圖像中的對象有少許位移，CNN仍能正確識別。

## 4.4 經典的CNN架構
### 4.4.1 LeNet-5
LeNet-5是最早的卷積神經網路之一，專門用於手寫數字識別。其結構包括兩個卷積層和兩個池化層，最後連接一個全連接層進行分類。

### 4.4.2 AlexNet
AlexNet是2012年ImageNet競賽的冠軍模型，它的誕生標誌著深度學習的崛起。相比LeNet-5，AlexNet使用了更多的卷積層和更大的卷積核，並引入了Dropout層來防止過擬合。

### 4.4.3 VGG
VGG模型通過堆疊較小的3x3卷積核進一步加深了網絡結構，從而獲得了更強大的特徵提取能力。

## 4.5 卷積神經網路的訓練
CNN的訓練與一般的神經網絡類似，通常使用反傳遞算法和梯度下降法來更新參數。具體步驟如下：

1. **前向傳播**：將輸入圖像通過網絡，計算每層的輸出。
2. **損失計算**：使用損失函數（如交叉熵損失）計算網絡預測與真實標籤的差異。
3. **反向傳播**：計算每個參數的梯度，使用梯度下降法更新權重。

## 4.6 Python實現卷積神經網路
以下是一個使用Keras構建簡單卷積神經網路的範例，該模型用於處理MNIST手寫數字識別：

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# 構建卷積神經網路模型
def create_cnn_model():
    model = models.Sequential()
    
    # 第一個卷積層
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
    model.add(layers.MaxPooling2D((2, 2)))
    
    # 第二個卷積層
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    
    # 第三個卷積層
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    
    # 展平並全連接層
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))
    
    return model

# 加載數據集並編譯模型
def train_cnn():
    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
    train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
    test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255

    model = create_cnn_model()
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))

# 訓練模型
train_cnn()
```

## 4.7 結論
卷積神經網路通過卷積層和池化層有效地提取圖像的特徵，並且在計算機視覺領域取得了卓越的成就。通過合理設計CNN架構，我們可以解決許多複雜的圖像處理問題。

---

這是「04-卷積神經網路」章節的初步框架，可以根據需要進行具體的修改和擴展。