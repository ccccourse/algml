

* https://unsloth.ai/
    * https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=idAEIeSQ3xdS
    * 其中有使用到 huggingface 的 SFT (Supervised Fine-tune)
    * https://huggingface.co/docs/trl/sft_trainer


微調的方法可以分為下列幾種 


在語言模型的微調過程中，並不是所有參數都會進行調整，具體的調整方式取決於微調的需求和方法。常見的有以下幾種方式：

## 全參數微調（Full Fine-tuning）：

在這種方法中，模型的所有參數都會參與微調。這意味著整個模型會根據新的數據進行訓練，這種方式會使模型更靈活地適應新的任務。
優點：可以達到最高的性能，特別是在新任務與原始預訓練任務有較大差異時。
缺點：這種方式需要大量的計算資源，且可能導致過擬合。

## 部分參數微調（Partial Fine-tuning）：

有時只會微調模型的部分參數，例如最後幾層的權重。這種方式假設模型的早期層已經學習了足夠的語言基礎知識，不需要再次調整。
優點：計算成本較低，且模型可以更快地收斂，降低過擬合風險。
缺點：在新任務與預訓練任務差異較大時，性能可能不如全參數微調。


## 增量學習或適配層（Adapter layers）：

這是一種更加靈活的微調方式，不直接微調原模型的參數，而是在模型中插入一些專門為新任務設計的「適配層」，這些層的參數會根據新任務進行訓練，而原模型的參數保持不變。
優點：大大減少了需要調整的參數數量，能夠在保留原模型知識的同時適應新任務。
缺點：需要設計合適的適配層，並且這種方法在某些情況下的性能可能不如全參數微調。

## 凍結特定層（Layer Freezing）：

通常，模型的某些層（尤其是靠近輸入端的早期層）會「凍結」，即它們的參數在微調過程中保持不變，只微調後面的幾層。這適合用於新任務與預訓練任務有較大相似度的情況。
優點：可以節省計算資源並減少過擬合風險，特別是當預訓練任務與微調任務有重疊時。
缺點：可能會限制模型的靈活性，導致在完全不同的任務上表現較差。
總之，具體是全參數微調還是只調整部分參數，取決於任務的性質、資源限制以及想要達到的效果。