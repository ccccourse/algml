# 微調

參考

* [微調大型語言模型LLM的技術LoRA及生成式AI-Stable diffusion LoRA](https://xiaosean5408.medium.com/%E5%BE%AE%E8%AA%BF%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8Bllm%E7%9A%84%E6%8A%80%E8%A1%93lora%E5%8F%8A%E7%94%9F%E6%88%90%E5%BC%8Fai-stable-diffusion-lora-61a41d636772)

概念：在模型補上一些微調參數（這些參數的初始值，在微調前影響為零）

![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aY3Rt2Xnkn0ifOXi.png)

但是這些參數數量很少，因為經過降維後重建

* [LLM最重要技術之一，一篇文章深入淺出Lora的方方面面](https://axk51013.medium.com/llm%E5%B0%88%E6%AC%84-all-about-lora-5bc7e447c234)

## 範例

* [Ollama  + Unsloth + Llama-3 + Alpaca.ipynb](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)

其中下列程式就是加入 LoRA 去修改模型，準備進行微調

```py
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)
```

這裡微調的指示資料使用 vicgalle/alpaca-gpt4 這個資料集

```py
from datasets import load_dataset
dataset = load_dataset("vicgalle/alpaca-gpt4", split = "train")
print(dataset.column_names)
```

該資料集位於下列網址

* https://huggingface.co/datasets/vicgalle/alpaca-gpt4

資料集有四個欄位 

1. instruction
2. input
3. output
4. text

例如

1. instruction：Explain why the following fraction is equivalent to 1/4
2. input： 4/16
3. output：The fraction 4/16 is equivalent to 1/4 because both fractions represent the same value. A fraction can be simplified by dividing both the numerator and the denominator by a common factor. In this case, 4 is a common factor of both the numerator and the denominator of 4/16. When we divide both by 4, we get 4/4 = 1 and 16/4 = 4, so the simplified fraction is 1/4. Alternatively, we can think of this in terms of multiplication. For example, if we multiply the numerator and denominator of the fraction 1/4 by 4, we get (1x4)/(4x4), or 4/16. Since both fractions can be derived from the other through multiplication or division by the same number, they represent the same value and are equivalent.
4. text：上面三欄合併後轉成文字的總和

然後指定好模板將 Instruction, input, output 轉為純文字後，就可以交給 Huggingface TRL's SFTTrainer 去微調了。

這要花上一些時間 ....

然後微調好，就可以拿來 inference 用了

unsloth 強調他的方法比 huggingface 的快很多

把微調後的模型存擋為 GGUF 格式

```py
if True: model.save_pretrained_gguf("model", tokenizer,)
```

微調好的檔案，也可以匯出給 ollama 使用

```sh
!ollama create unsloth_model -f ./model/Modelfile
```

接著用 

```sh
ollama run unsloth_model
```

就可以把該模型放在 ollama 上跑了



