以下是《語言模型背後的數學》中**7.2 前向傳播與反向傳播**的草稿：

### 7.2 前向傳播與反向傳播

前向傳播和反向傳播是神經網絡訓練的核心過程。前向傳播用於計算神經網絡的輸出，而反向傳播則用於更新權重和偏置，以最小化損失函數。本節將詳細介紹這兩個過程的工作原理及其數學基礎。

#### 1. 前向傳播

- **定義**：
  前向傳播是將輸入數據通過神經網絡逐層傳遞的過程，最終生成輸出結果。每一層的輸出作為下一層的輸入。

- **數學表達**：
  對於一個簡單的前饋神經網絡，假設有 \( L \) 層，其中每一層 \( l \) 的輸出可以表示為：
  \[
  a^{(l)} = f(z^{(l)}) = f(W^{(l)} a^{(l-1)} + b^{(l)})
  \]
  其中：
  - \( a^{(l)} \) 是第 \( l \) 層的激活輸出。
  - \( W^{(l)} \) 是第 \( l \) 層的權重矩陣。
  - \( b^{(l)} \) 是第 \( l \) 層的偏置向量。
  - \( z^{(l)} \) 是第 \( l \) 層的加權和。

- **示例**：
  假設一個包含兩層的神經網絡，第一層有 \( n_1 \) 個神經元，第二層有 \( n_2 \) 個神經元。輸入層的數據 \( x \) 先通過第一層，然後通過第二層，最終輸出為：
  \[
  \hat{y} = a^{(2)} = f(W^{(2)} f(W^{(1)} x + b^{(1)}) + b^{(2)})
  \]

#### 2. 反向傳播

- **定義**：
  反向傳播是利用損失函數的梯度來更新神經網絡權重的過程。該過程從輸出層開始，逐層向後計算梯度，並使用這些梯度來調整權重。

- **損失函數**：
  假設使用均方誤差（MSE）作為損失函數，其定義為：
  \[
  L(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2
  \]
  其中 \( y \) 是實際標籤，\( \hat{y} \) 是預測結果。

- **計算梯度**：
  反向傳播的目的是計算每一層權重和偏置的梯度。對於輸出層的權重，梯度可以表示為：
  \[
  \delta^{(2)} = \frac{\partial L}{\partial z^{(2)}} = (a^{(2)} - y) f'(z^{(2)})
  \]
  其中，\( \delta^{(2)} \) 是誤差項，\( f' \) 是激活函數的導數。

  對於隱藏層的梯度，可以使用鏈式法則進行計算：
  \[
  \delta^{(1)} = (W^{(2)T} \delta^{(2)}) f'(z^{(1)})
  \]

- **更新權重**：
  計算出每層的誤差項後，可以使用以下公式更新權重和偏置：
  \[
  W^{(l)} = W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}
  \]
  \[
  b^{(l)} = b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}
  \]
  其中，\( \eta \) 是學習率。

#### 3. 結論

前向傳播和反向傳播是神經網絡訓練的基礎。前向傳播計算網絡的輸出，反向傳播則通過梯度下降更新權重，從而使得模型能夠學習數據中的模式。理解這兩個過程的數學原理和運作方式，對於設計和訓練有效的神經網絡至關重要。

---

如果您有任何修改建議或希望擴展的內容，請隨時告訴我！