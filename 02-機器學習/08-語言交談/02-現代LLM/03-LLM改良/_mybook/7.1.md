以下是《語言模型背後的數學》中**7.1 感知器與激活函數**的草稿：

### 7.1 感知器與激活函數

#### 1. 感知器模型

- **定義**：
  感知器是最簡單的神經網絡結構，由Frank Rosenblatt在1958年提出。它是一種二分類模型，能夠根據給定的特徵將輸入數據分為兩類。感知器的基本組成包括輸入層、權重、偏置和激活函數。

- **數學表達**：
  對於一個單一感知器，其輸出可由以下公式計算：
  \[
  y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
  \]
  其中：
  - \( x_i \) 是輸入特徵。
  - \( w_i \) 是對應的權重。
  - \( b \) 是偏置項。
  - \( f \) 是激活函數，決定輸出的類別。

#### 2. 激活函數的作用

- **定義**：
  激活函數用於引入非線性，使得神經網絡可以擬合複雜的數據模式。感知器中的激活函數決定了輸出的結果，通常是二元的（例如0或1）。

- **常見的激活函數**：
  - **階躍函數（Step Function）**：
    \[
    f(x) = 
    \begin{cases} 
    1 & \text{if } x \geq 0 \\
    0 & \text{if } x < 0 
    \end{cases}
    \]
    階躍函數用於將輸出轉換為二元結果，但其不連續性使得模型難以訓練。

  - **Sigmoid函數**：
    \[
    f(x) = \frac{1}{1 + e^{-x}}
    \]
    Sigmoid函數的輸出範圍在(0, 1)之間，適合於二分類問題。然而，當輸入值很大或很小時，梯度會接近於零，導致梯度消失問題。

  - **Tanh函數**：
    \[
    f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    \]
    Tanh函數的輸出範圍在(-1, 1)之間，對於均值為零的數據表現較好，但同樣存在梯度消失問題。

  - **ReLU（Rectified Linear Unit）函數**：
    \[
    f(x) = \max(0, x)
    \]
    ReLU函數在正半軸上為線性，能夠有效地解決梯度消失問題，並促進模型的收斂速度。由於其計算簡單且能夠加速訓練，ReLU在深度學習中得到了廣泛應用。

#### 3. 感知器的訓練

- **訓練算法**：
  感知器的訓練過程主要依賴於感知器學習規則，該規則基於梯度下降法來更新權重和偏置，以最小化預測錯誤。

- **數學表達**：
  訓練過程可以描述為：
  \[
  w_i^{(t+1)} = w_i^{(t)} + \eta (y - \hat{y}) x_i
  \]
  \[
  b^{(t+1)} = b^{(t)} + \eta (y - \hat{y})
  \]
  其中，\( \eta \) 是學習率，\( y \) 是實際標籤，\( \hat{y} \) 是感知器的預測輸出。

#### 4. 結論

感知器是神經網絡的基本構建單元，通過激活函數的引入，使得模型具備了處理非線性問題的能力。雖然單層感知器的能力有限，但它們為後續多層神經網絡的發展奠定了基礎。理解感知器的運作原理和激活函數的選擇，對於設計更複雜的神經網絡至關重要。

---

如果您有任何修改建議或希望擴展的內容，請隨時告訴我！