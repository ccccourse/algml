## 1.2. 強化學習與其他機器學習算法的區別

強化學習和其他機器學習算法（如監督學習和非監督學習）有以下區別：

1. 學習方式

監督學習和非監督學習通常涉及從事先準備好的數據集中學習模型參數，強化學習則是通過與環境交互來學習，即代理在環境中進行試驗並學習如何通過行動最大化預期總體回報。因此，強化學習中的目標是學習一個策略/行動選擇，而不是單純的輸出預測。

2. 回饋方式

在監督學習中，每個訓練樣本都有對應的標籤或期望的輸出，模型的目標是盡可能地接近實際標籤。在非監督學習中，目標是發現數據集中的結構或模式，而不存在明確的期望輸出。然而，在強化學習中，給代理的唯一回饋是來自環境的獎勵或懲罰信號，目標是在最短的時間內最大化期望總體回報。在強化學習的過程中，代理不知道將來會得到的回饋，而必須從代理的行為中得到反饋來學習如何最大化未來的回饋。

3. 時間序列性

強化學習主要處理時間序列問題，涉及代理在各個時間步驟上的決策。代理的行為與環境互動，環境再根據代理的行為生成一個新的狀態，代理在新的環境中再次做出決策，迭代循環直到狀態達到終止條件。因此，強化學習需要考慮到現在和未來的回饋，不同於監督學習和非監督學習只考慮現在。

總之，強化學習區別於其他機器學習算法，依靠與環境交互的方式學習行動策略，並通過最大化長期回饋來達到目標。