## 5.2. PG 算法

PG 算法是 Policy Gradient（策略梯度）算法的簡稱，是一種常見的強化學習的算法。相比於 Q-learning 等基於值函數的方法，PG 算法可以比較好地處理連續性動作空間。具體地來說，PG 算法通過直接優化策略來學習最優策略。

在 PG 算法中，策略是一個函數，輸入是環境的狀態，輸出是選擇的動作碼。PG 算法訓練過程的目標是找到一個參數化的策略 $\pi_{\theta}(a|s)$，使得期望長期奖勵最大化：
$$\max_{\theta} \ \mathbb{E}_{s \sim p_{\theta}, a \sim \pi_{\theta}}[\sum_{t=0}^{T} \gamma^t r_t]$$

其中，$s \sim p_{\theta}$ 表示當前狀態由參數為 $\theta$ 的狀態轉移函數生成，而 $a \sim \pi_{\theta}$ 則表示當前動作由參數為 $\theta$ 的策略生成。

PG 算法通過最大化期望長期奖勵，來達到尋找最佳策略的目的。為了實現策略的無模型優化，該算法用梯度代替了值函數。通過計算梯度，PG 算法可以更新策略，最小化目標函數 $J(\theta)$。PG 算法的更新公式如下：
$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T_i} \nabla_{\theta} \log \pi_{\theta}(a_t^i | s_t^i) A_t^i$$

其中，$A_t^i$ 是 Advantage 函数，表示當前策略在 $t$ 時刻上下文 $s_t^i$ 中選擇動作 $a_t^i$ 的優勢。

在 PG 算法中，Advantage 函数有不同的計算方式，比如 GAE（Generalized Advantage Estimation）方法等。GAE 方法通過線性插值的方式計算 Advantage 函数，其公式如下：
$$A_t^i = \sum_{l=0}^{\infty} (\gamma \lambda)^l \Delta_{t+l}^i$$
$$\Delta_{t}^i = r_{t}^i + \gamma v(s_{t+1}^i) - v(s_t^i)$$

其中，$\Delta_{t}^i$ 表示在時刻 $t$ 狀態 $s_t^i$ 的優勢，$v(s)$ 表示狀態 $s$ 的經驗值。

上述公式中，$\lambda \in [0, 1]$ 為一個超參數，用於控制 Advantage 函数的偏置和方差。當 $\lambda = 1$ 時，GAE 方法等價於 N-step TD（Temporal Difference）方法。當 $\lambda = 0$ 時，GAE 方法等價於 Monte Carlo 方法。

PG 算法的優點在於可以比較好地處理連續性動作空間，並且也比較容易實現策略的彈性和鮮明度。但同時也存在着一些缺點，比如算法的收斂率比較慢，需要耗費大量的時間進行試驗，並且在梯度計算中，需要進行大量的樣本採樣和計算梯度，相對比較消耗計算資源。