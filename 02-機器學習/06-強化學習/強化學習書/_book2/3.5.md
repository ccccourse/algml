## 3.5. Q-Learning 算法在 Gym 中的應用

Q-Learning 是一種基於值的強化學習方法，主要用於解決 Markov 決策過程（MDP）。

下面介紹如何使用 Gym 套件實現 Q-Learning 算法，讓機器學會玩一個 Atari 遊戲 - Pong。

Gym 中內置了許多 Atari 遊戲的環境，其中包括了 Pong。Pong 是一個經典的二人對打遊戲，玩家需要使對手無法接到自己擊出的球，一旦對手未能接住球就得加分。

首先需要安裝 Gym，在命令行中輸入以下指令：

```python
!pip install gym
```

安裝完成後，進入 Python 編輯器，撰寫以下程序：

```python
import gym
import numpy as np

env = gym.make("Pong-v0")

print(env.action_space)  # 球拍上下移動的動作空間
print(env.observation_space)  # 遊戲畫面的維度
```

通過 `gym.make()` 函數可以創建出 Pong 的環境，`env.action_space` 表示球拍上下移動的可行動作，`env.observation_space` 則表示遊戲畫面的維度。

接著實現 Q-Learning 算法，以下為該算法的主要步驟：

1. 初始化 Q 值表。Q 值代表當處於一個狀態 s，並且執行了一個行動 a 後，能夠得到的期望獎勵值（也就是策略），所以 Q 值上起始來說需要初始化。

2. 選擇動作。使用 ε-greedy 演算法選擇動作，其中 ε 代表隨機動作選取的機率，通常會在一開始隨機選取較多，當學習進行到後面，隨機選取的機率就會逐漸降低。

3. 執行動作，進入下一個狀態。

4. 更新 Q 值。使用 Q-Learning 的學習規則：$Q(s, a) = (1 - \alpha) \times Q(s, a) + \alpha \times (r + \gamma \times \max_a' Q(s', a'))$，其中 $\alpha$ 為學習率，$\gamma$ 為折扣因子，$r$ 為當前的獎勵值，$a'$ 為下一步動作中，能夠得到最大獎勵的那個行動。

5. 重複上述步驟，直到完成整個遊戲。

以下為實現 Q-Learning 算法的完整程序：

```python
import gym
import numpy as np

def get_action(state, Q, n_actions, epsilon):
    if np.random.uniform() < epsilon:
        return np.random.choice(n_actions)
    else:
        return np.argmax(Q[state])

def state_to_bucket(state):
    bucket_state = [
        0 if state[0] < -0.4 else 1,
        0 if -0.4 <= state[0] < 0 else 2,
        0 if state[0] >= 0 else 3,
        0 if state[1] < -0.025 else 1,
        0 if state[1] >= -0.025 else 2,
        0 if state[2] < -0.5 else 1,
        0 if state[2] >= -0.5 else 2,
        0 if state[3] < -0.025 else 1,
        0 if state[3] >= -0.025 else 2
    ]
    return int("".join(map(str, bucket_state)))

def update_q_table(Q, state, next_state, action, reward, alpha, gamma):
    Q[state][action] = (1 - alpha) * Q[state][action] + \
        alpha * (reward + gamma * np.max(Q[next_state]))

def train(env, n_episodes, alpha, gamma, epsilon):
    n_actions = env.action_space.n
    n_states = 10 ** 4
    Q = np.zeros((n_states, n_actions))

    for i_episode in range(n_episodes):
        state = state_to_bucket(env.reset())
        eps_decay = 1 / (1 + np.exp(-i_episode / 1000))
        epsilon = epsilon * eps_decay if eps_decay >= 0.01 else 0.01

        done = False
        t = 0
        while not done:
            action = get_action(state, Q, n_actions, epsilon)
            next_state, reward, done, _ = env.step(action)
            next_state = state_to_bucket(next_state)

            update_q_table(Q, state, next_state, action, reward, alpha, gamma)
            state = next_state
            t += 1

        print(f"Episode {i_episode} finished after {t} timesteps.")

env = gym.make("Pong-v0")
train(env, 10000, 0.1, 0.9, 1.0)
```

在以上程序中，`get_action()` 函數用於根據 Q 值表和 ε-greedy 策略選取動作；`state_to_bucket()` 函數用於將連續的狀態離散化成有限數量的範圍內。因遊戲畫面實在太多，要是直接使用原始畫面像遷移學習那樣建立 Q 值表會過度拟合而導致过度学习。針對此狀況，我們可以將畫面的繪製方式轉換為向量（vector）。接著再針對這些向量進行離散化以得到有限的狀態值。

`update_q_table()` 函數用於根據原來的 Q 值表、當前狀態、下一個狀態、當前動作的獎勵、學習率和折扣因子，計算更新後的 Q 值表。

`train()` 函數是程序的主函數，訓練 Q 值表。

最後，執行程式以開始訓練模型，這項訓練所需時間會比較長，需要耐心等待。

透過以上步驟，我們利用 Q-Learning 算法訓練出的模型將會自學成才，掌握如何玩 Pong 遊戲！