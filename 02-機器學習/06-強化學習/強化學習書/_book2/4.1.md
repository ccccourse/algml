## 4.1. 深度神經網路

深度學習是近年來在許多領域中取得重大進展的研究分支，主要是基於深度神經網絡（Deep Neural Networks，DNNs）的學習算法。深度神經網絡是一種類比人類大腦，由多個神經元組成的嵌套層次結構，通常由多個隱藏層組成。每個神經元經過權重和偏置的計算綜合上一層的輸出產生一個輸出值，這個值會作為下一層的輸入值。

當深度神經網絡擁有更多的隱藏層時，它可以更好地學習複雜的高維度數據，這也是為什麼它可以被用來處理CV、NLP和遊戲等任務。

由於每個隱藏層都可以學習到越來越抽象的特徵，並利用前一層提供的特徵概念來形成更高層次的特徵，使得深度神經網絡具有更強大的特徵提取能力。

搭建深度神經網絡的過程類似於搭建標準神經網路，首先決定模型的拓撲結構，即：輸入層、隱藏層、輸出層的個數以及層與層之間的連接方式，然後給每一個連接線附上權重。當結構搭建好後，通常可以使用適應性優化算法，如反向傳播（Back Propagation）來更新連接線上的權重值，使其逐步地去最小化損失函數的值，在不斷的迭代過程中，模型不斷地自適應提高。

在機器學習中，激活函數是在深度神經網路的每個神經元中進行非線性轉換的核心操作，目的是讓神經元的輸出值映射為一定範圍內的值，可以增加模型的表達能力。 常用的激活函數有 Sigmoid、ReLU、Tanh、LeakyReLU 等。

 - Sigmoid function：由於其值域在(0,1)之間而具有良好的連續性和光滑性，常被用於二分類任務中。

 - ReLU function：具有拋棄次要特徵和收斂迅速等優點，近年來成為最常使用的激活函數之一，一般應用於分類和回歸等任務中。 

 - Tanh function：在於 Sigmoid 類似，但是其值域在(-1,1),相比 Sigmoid,它對於負數的響應更強，但同樣會面臨梯度消失的問題。

 - LeakyReLU function：將 ReLU 中負的部分乘以一個定值，既保持 ReLU 非線性的特性，又克服了 Dead ReLU 的缺點，即使神經元的權重保持負值，也能有微小的輸出，使訓練更穩定。

深度學習中常用的算法有反向傳播算法（Backpropagation，BP）和卷積神經網路（Convolutional Neural Networks，CNNs）。反向傳播算法是一種基於梯度下降的算法，透過不斷調整每層間神經元的權重，最小化損失函數。而卷積神經網路是一種特殊的深度學習神經網路，用於處理圖像識別等視覺任務，可以將輸入的圖像作為權重的一部分來處理，更好地進行圖像的特徵提取，通常包括捲積層、池化層和全連接層三個主要模塊。

總體而言，深度神經網絡通過多個隱藏層以及激活函數可以表達非常復雜的函數關係，在大量訓練數據下可以有效提高模型的性能，但也存在著需要大量計算和訓練時間長的問題。