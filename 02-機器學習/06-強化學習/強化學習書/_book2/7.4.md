## 7.4. MADDPG 算法及實現

MADDPG 是 Multi-Agent Deep Deterministic Policy Gradients 的簡稱，是一種深度強化學習算法，用於解決多智能體協作的問題。在 MADDPG 中，每個智能體都有自己的腦筋（即一個神經網絡），並且這些智能體可以觀察到它們的狀態和環境。它們需要通過協調和合作來實現全局的最大化獎勵。

MADDPG 算法基於 DDPG 算法進行擴展，因此首先了解 DDPG 算法。DDPG 是 Deep Deterministic Policy Gradient 的簡稱，是一種強化學習中的 Actor-Critic 算法。該算法使用了深度神經網絡來估計策略和值函數，並使用 experience replay（經驗回放）來增強學習效果。

在 DDPG 算法中，有兩個網絡：一個策略網絡（Actor）和一個值網絡（Critic）：

- 策略網絡是一個近似最優策略的模型，它輸入狀態並輸出動作，以此來決定智能體應該採取的行動。

- 值網絡是一個估計函數，它輸入狀態和動作並輸出相應的值，以此來評估策略的優劣。

每個智能體在 MADDPG 中也有這兩個網絡。在確定自己應該採取的動作時，它需要考慮其他智能體的策略。為了實現這點，MADDPG 引入了兩個新的定義：

1. 全局狀態（Global State）：對於全部智能體來說，它們在某個時間刻的所有狀態被合併成一個矢量，形成一個全局狀態。

2. 個體觀察（Individual Observation）：每個智能體觀察到了全局狀態的一部分，就像在真實世界中人類可以感知某些事物，但對於其他事物卻是盲點一樣。每個智能體只能看到對自己有意義的狀態，而無法看到其他智能體的私有狀態。

在 MADDPG 中，每個智能體都有自己的個體觀察和動作，但是所有智能體的策略網絡和值網絡是一樣的，這種設置方式稱為共享網絡。在實現 MADDPG 算法時，需要考慮以下幾點：

1. 建立智能體網絡：MADDPG 算法是基於 DDPG 的擴展，因此需要創建每個智能體的策略網絡和值網絡。

2. 收集環境數據：使用肌耐的 experience replay 作法。

3. 計算梯度和更新參數：使用梯度下降法更新神經網絡的權重。

以下是 MADDPG 算法的具體實現步驟：

1. 初始設置：初始化神經網絡參數，環境參數，超參數和經驗池。

```
# 初始化神經網絡
actor_net = ActorNetwork()
critic_net = CriticNetwork()
actor_target_net = ActorNetwork()
critic_target_net = CriticNetwork()

# 初始化環境和超參數
env = gym.make('multi_agent_env')
num_agents = env.n_agents
observation_space = env.observation_space
action_space = env.action_space
tau = ...
gamma = ...
memory_size = ...

# 初始化經驗池
memory = ReplayMemory(memory_size)
```

2. 定義行動策略（Action Policy）：輸入觀察，輸出行動。

```
# 定義行動策略
class ActionPolicy:
    def __init__(self, actor):
        self.actor = actor
    
    def act(self, states):
        actions = []
        for i in range(num_agents):
            # 接收輸入狀態獲取動作
            action = actor(states[i])
            actions.append(action)
        return actions
```

3. 定義智能體的策略網絡（Actor）和值網絡（Critic）。

```
# 自适應雙端策略梯度(MADDPG)
class ActorNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=64, init_w=3e-3):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.init_weights(init_w)
    
    def init_weights(self, init_w):
        self.fc3.weight.data.uniform_(-init_w, init_w)
        self.fc3.bias.data.uniform_(-init_w, init_w)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.tanh(self.fc3(x))
        return x
    

class CriticNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=64, init_w=3e-3):
        super(CriticNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.init_weights(init_w)
    
    def init_weights(self, init_w):
        self.fc3.weight.data.uniform_(-init_w, init_w)
        self.fc3.bias.data.uniform_(-init_w, init_w)
    
    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

4. 定義共享網絡的目標策略網絡（Target Actor）和目標值網絡（Target Critic）。

```
# 定義共享網絡的目標策略網絡和值網絡
class TargetActorNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=64, init_w=3e-3):
        super(TargetActorNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.init_weights(init_w)
    
    def init_weights(self, init_w):
        self.fc3.weight.data.uniform_(-init_w, init_w)
        self.fc3.bias.data.uniform_(-init_w, init_w)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.tanh(self.fc3(x))
        return x
    

class TargetCriticNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=64, init_w=3e-3):
        super(TargetCriticNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.init_weights(init_w)
    
    def init_weights(self, init_w):
        self.fc3.weight.data.uniform_(-init_w, init_w)
        self.fc3.bias.data.uniform_(-init_w, init_w)
    
    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

5. 定義 Agent 類別。

```
# 定義 Agent 類別
class Agent:
    def __init__(self, actor, critic, actor_target, critic_target, memory, batch_size, action_policy):
        self.actor = actor
        self.critic = critic
        self.actor_target = actor_target
        self.critic_target = critic_target
        self.memory = memory
        self.batch_size = batch_size
        self.action_policy = action_policy
    
    def act(self, states):
        return self.action_policy.act(states)
    
    def update(self, tau, gamma):
        if len(self.memory) < self.batch_size:
            return
        
        transitions = self.memory.sample(self.batch_size)
        batch = Transition(*zip(*transitions))

        next_state = torch.cat(batch.next_state).view(self.batch_size, -1)
        reward = torch.cat(batch.reward).view(self.batch_size, -1)
        done = torch.cat(batch.done).view(self.batch_size, -1)
        state = torch.cat(batch.state).view(self.batch_size, -1)
        action = torch.cat(batch.action).view(self.batch_size, -1)

        # 計算 critic 損失函數
        target_next_action = self.actor_target(next_state)
        target_next_value = self.critic_target(next_state, target_next_action)
        q_target = reward + gamma * target_next_value * (1 - done)
        q_current = self.critic(state, action)
        critic_loss = F.smooth_l1_loss(q_current, q_target.detach())

        # 更新 critic 網絡
        self.critic.zero_grad()
        critic_loss.backward()
        nn.utils.clip_grad_norm_(self.critic.parameters(), grad_clip)
        self.critic_optim.step()

        # 計算 actor 損失函數
        policy_loss = -self.critic(state, self.actor(state)).mean()

        # 更新 actor 網絡
        self.actor.zero_grad()
        policy_loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), grad_clip)
        self.actor_optim.step()

        # 更新目標網絡
        self._update_target_networks(tau)

    def _update_target_networks(self, tau):
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

```

6. 在主循環中運行代理。

```
# 定義主循環
actor_optim = torch.optim.Adam(actor_net.parameters(), lr=...)
critic_optim = torch.optim.Adam(critic_net.parameters(), lr=...)
agent_1 = Agent(actor_net, critic_net, actor_target_net, critic_target_net, memory, batch_size, action_policy)
agent_2 = Agent(actor_net, critic_net, actor_target_net, critic_target_net, memory, batch_size, action_policy)

for i_episode in range(max_episodes):
    # 初始化環境
    obs = env.reset()
    episode_reward = 0
    done = [False for _ in range(num_agents)]
    
    for t in range(max_steps):
        # 根據策略網絡選擇行動
        actions = [agent_1.act([obs[0]]), agent_2.act([obs[1]])]

        # 執行行動得到環境反饋
        obs_next, rewards, done, _ = env.step(actions)

        # 保存經驗到經驗池中
        for i in range(num_agents):
            memory.push(obs[i], actions[i], rewards[i], obs_next[i], done[i])
        
        # 更新智能體策略和值函數
        agent_1.update(tau, gamma)
        agent_2.update(tau, gamma)

        # 更新狀態
        obs = obs_next
        episode_reward += np.sum(rewards)
        
        if all(done):
            break

```