## 6.2. A2C 算法

## A2C 簡介

* A2C（Advantage Actor-Critic）是將 Policy Gradient 和 Value Critic 結合的算法，它融合了兩者的優點，Policy Gradient 可以動態地調整策略，不需要使用價值函數，比較適合離散控制問題；而 Value Critic 不需要根據每一步的動作得到一個概率分佈，可以更好地處理連續控制問題。

* 先做出某個動作，再計算該動作對獎勵的貢獻，得到該動作的優勢值，在步長內對優勢值和其他狀態、動作相關的長期獎勵進行迭代更新，最終得到每個狀態和動作的價值。

    * 優勢值：一個動作對其他動作的榜樣的差距。




以CartPole問題為例，Actor-Critic的模型可以建立在Policy Network與Value Function的基礎上，其中

* Policy Network : $$
\pi(a_t|s_t; \theta)
$$

* Value Function $$
q_{\phi}(s_t, a_t) = V_{\phi}(s_t) + A_{\phi}(s_t, a_t)
$$

其中，

* $\theta$: Policy Network 的參數。

* $\phi$: Advantage Function 的參數。

* $A_{\phi}(s_t, a_t)$: 動作優勢值（Advantage Function）是由預測狀態值 $V_{\phi}(s_t)$ 的方式計算得到的。 

相較於傳統的Actor-Critic模型， A2C模型大大減少了係數的數量，從而降低了算法的難度與模型的方差。


### A2C 算法步驟：

1. 初始化Actor Network和Critic Network的權重。

2. 從當前狀態開始，使用Actor Network生成動作的概率分布，從而選擇一個動作；根據該動作和狀態，使用Critic Network估計V的值。

3. 將狀態、動作、獎勵、下一個狀態、是否終止的信息加到記憶體中。

4. 每當n步，就從記憶體中隨機選取n個樣本，用於Policy Gradient的更新。
    * 對於COntinuous Action Space, 用TD error更新Actor和Critic。
    * 對於Discrete Action Space, 用Crossentropy更新Actor，用TD error更新Critic。

### A2C 更新公式

* 狀態狀值更新（Value function update）:

$$ V(s_{t}) =  r_{t+1} + \gamma V(s_{t+1}) $$

更新頻率是 $N$ 步(N-step), 表示當當前狀態 $s$ 每走 $N$ 步時，根據 $N$ 步長的回報來估計狀態價值 $V$，公式如下：

$$ V(s_{t}) =  \sum_{i=0}^{N-1} \gamma^i r_{t+i+1} + \gamma^N V(s_{t+N}) $$

下面是A2C的Pseudo Code：

### A2C Pseudo Code

```
  Initialize critic network V(s; w) with random weights w
  Initialize actor network pi(a | s; Θ) with random weights Θ
  Initialize empty replay buffer D
  for episode = 1, M do:
      S1 ← initial state
      for t = 1, T do:
          A t ← sample action from π(.|s t ; Θ)
          S t+1 , R t+1 ← environment step(A t )
          D ← {(s t , A t , R t+1 , s t+1 )}
          if terminal(s t+1 ) then
              break
          end if
          if mod(t , t max ) = 0 or terminal(s t+1 ) then
              Compute V (s t ; w) for all t = T, T − 1, ..., t + 1
              Discounted return R ̄ t = [R t+1 + γV (s t+ 1 ; w)] n-step return
              Θ ← Θ + αθDC ∇Θ log π(a t |s t ; Θ)(R ̄ t − V (s t ; w))
              w ← w + αwDV [R ̄ t − V (s t ; w)] ∂V (s t ; w)/∂w
          end if
      
      end for
  end for
```


## 執行 A2C 算法

### 引入相關套件庫

首先我們需要先引入相關套件庫，在此我們將使用Openai Gym。

```python
!pip install gym[atari]
```

```python
import os
import argparse
import gym
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K

print("Tensorflow version:", tf.__version__)
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
```


### 建立A2C模型

```python
class A2C_Agent:
    def __init__(self, obs_dim, act_dim):
        self.gamma = 0.99
        self.alpha = 5e-4
        self.beta_entropy = 5e-3
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        
        self.actor, self.critic, self.policy = self.create_models()
        self.opt_actor = Adam(learning_rate=self.alpha)
        self.opt_critic = Adam(learning_rate=self.alpha)
    
        
    # 創建 Actor 和 Critic Network
    def create_models(self):
        inputs = Input(shape=(self.obs_dim,))
        common = Dense(64, activation='relu')(inputs)
        common = Dense(32, activation='relu')(common)

        actor = Dense(self.act_dim, activation='softmax')(common)
        critic = Dense(1)(common)

        policy = Model(inputs=inputs, outputs=[actor, critic])
        actor_network = Model(inputs=inputs, outputs=actor)

        actor_probs = tf.keras.layers.Input(shape=(self.act_dim, ))
        advantages = tf.keras.layers.Input(shape=(1, ))
        critic_value = tf.keras.layers.Input(shape=(1, ))

        policy_loss = -tf.reduce_mean(tf.math.log(tf.reduce_sum(actor_probs * actor, axis=1, keepdims=True)) * advantages)
        entropy_loss = -tf.reduce_mean(tf.reduce_sum(actor * tf.math.log(actor), axis=1))
        critic_loss = tf.reduce_mean((critic - critic_value) ** 2)

        weighted_loss = policy_loss + self.beta_entropy * entropy_loss
        policy.add_loss(weighted_loss)

        policy.compile(optimizer=Adam(self.alpha))
        
        return actor_network, critic, policy
    
    # 狀態估計值
    def critic_prediction(self, obs):
        return np.squeeze(self.critic.predict(obs), axis=-1)
    
    # 選擇動作
    def choose_action(self, obs):
        policy, _ = self.policy.predict(obs)
        action = np.random.choice(range(policy.shape[1]), p=policy[0])
        return action, policy[0]
    
    # 透過A2C作為更新 Policy 和 Value Networks 的訓練方法
    def train(self, obs, actions, rewards, next_obs, dones):
        # 動作轉換
        actions = tf.keras.utils.to_categorical(actions, num_classes=self.act_dim)
        
        # Critic Network 訓練
        value_next = self.critic_prediction(next_obs)
        y = rewards + (1 - dones) * self.gamma * value_next
        y = y.reshape(-1, 1)

        critic_history = self.critic.fit(x=obs, y=y, verbose=0)
        
        # Actor Network 訓練
        with tf.GradientTape() as tape:
            policy, value = self.policy(obs)
            advantages = y - value
            policy_probs = tf.reduce_sum(policy * actions, axis=1, keepdims=True)
            log_policy = tf.math.log(policy_probs)
            policy_loss = tf.reduce_mean(-log_policy * advantages)
            entropy_loss = tf.reduce_mean(-tf.reduce_sum(policy * tf.math.log(policy), axis=1))
            loss = policy_loss + self.beta_entropy * entropy_loss

        grads = tape.gradient(loss, self.policy.trainable_weights)
        self.opt_actor.apply_gradients(zip(grads, self.policy.trainable_weights))
```


### 訓練 A2C 模型

```python
def train(env_name: str, save_path: str):
    env = gym.make(env_name)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n
    agent = A2C_Agent(obs_dim, act_dim)
    
    epochs = 2000
    rewards_history = []
    for epoch in range(epochs):
        obs = env.reset()
        done = False
        epoch_reward = 0

        buffer_obs = []
        buffer_act = []
        buffer_rew = []
        buffer_next_obs = []
        buffer_done = []

        while not done:
            action, policy = agent.choose_action(np.array([obs]))

            next_obs, reward, done, _ = env.step(action)

            buffer_obs.append(obs)
            buffer_act.append(action)
            buffer_rew.append(reward)
            buffer_next_obs.append(next_obs)
            buffer_done.append(done == False)

            obs = next_obs
            epoch_reward += reward

        agent.train(
            np.array(buffer_obs),
            np.array(buffer_act),
            np.array(buffer_rew),
            np.array(buffer_next_obs),
            np.array(buffer_done)
        )

        rewards_history.append(epoch_reward)
        print("[Episode {:03d}] Reward: {:.3f}".format(epoch, epoch_reward))
        
    agent.policy.save_weights(save_path)
    return rewards_history
```


### 執行訓練

```python
 # 訓練並儲存模型
env_name = 'CartPole-v1'
save_path = 'A2C_{}_weights.h5'.format(env_name)

rewards_history = train(env_name, save_path)
```


### 訓練過程與訓練效果展示

```python
import matplotlib.pyplot as plt

def plot_rewards(rewards: list):
    plt.plot(rewards)
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Training History')
    plt.show()
    
plot_rewards(rewards_history)
```


### 加載前一模型

```python
def load_model(env_name: str, load_path: str):
    env = gym.make(env_name)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n
    
    agent = A2C_Agent(obs_dim, act_dim)
    agent.policy.load_weights(load_path)
    agent.actor.summary()
    return agent
```


### 評估模型

```python
def test(agent, env_name: str, render=False):
    env = gym.make(env_name)
    env.seed(0)
    obs = env.reset()
    done = False
    total_reward = 0
    while not done:
        if render:
            env.render()
        action, _ = agent.choose_action(obs.reshape(1, -1))
        obs, reward, done, info = env.step(action)
        total_reward += reward
    return total_reward
```


### 測試模型

```python
load_path = './A2C_CartPole-v1_weights.h5'
env_name = 'CartPole-v1'

agent = load_model(env_name, load_path)
reward = test(agent, env_name, True)
print("[Test] Reward:", reward)
```