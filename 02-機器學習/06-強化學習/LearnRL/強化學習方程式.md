## 強化學習方程式

1. Q 學習的更新公式：$$Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$$
3. Sarsa 的更新公式：$$Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)$$
4. 蒙特卡羅树搜索的更新公式：$$Q(s, a) = (1 - \alpha) \cdot Q(s, a) + \alpha \cdot (R + \gamma \cdot V(s'))$$
5. 动态规划的递推式：$$V_t(s) = \max_{a \in A} { r_t + \gamma V_{t+1}(s')}$$
6. 贝尔曼最优策略的方程：$$\pi^s = \arg \max_{a \in A} Q(s, a)$$
7. 期望 Sarsa 的更新公式：$$Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma \sum_{a'} \pi(a' | s') Q(s', a') - Q(s, a) \right)$$
8. 行动价值函数的贝尔曼方程：$$Q(s, a) = \sum_{s'} p(s' | s, a) \left[ r(s, a, s') + \gamma \sum_{a'} \pi^(a' | s') Q^*(s', a') \right]$$


