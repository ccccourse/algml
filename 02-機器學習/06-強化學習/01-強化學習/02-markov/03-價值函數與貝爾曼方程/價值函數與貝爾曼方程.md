## 2.3 貝爾曼方程

貝爾曼方程是解決 MDP 問題的關鍵公式，它描述了狀態值函數$V(s)$和動作值函數 $Q(s,a)$ 之間的關係，並通過递归地迭代求解值函數。貝爾曼方程分為兩種，分別是狀態值函數的貝爾曼方程和動作值函數的貝爾曼方程。

1. 狀態值函數的貝爾曼方程： 

$$V(s) = \sum_{a} \pi(a|s)\sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V(s')]$$ 

上式表示，狀態值函數 $V(s)$ 等於對所有可能的行動 $a$ ，根據策略 $\pi$ 進行加權後的結果，加權因子為 $\pi(a|s)$ ，再加上對所有可能的下一個狀態 $s'$ ，在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 的機率 $P(s'|s,a)$ ，再加上在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 並得到即時獎勵 $R(s,a,s')$ ，再加上折扣因子 $\gamma$ 乘上下一個狀態 $s'$ 的值函數 $V(s')$ 。

2. 動作值函數的貝爾曼方程：

$$
Q(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma \max_{a'} Q(s',a')]
$$

上式表示，動作值函數 $Q(s,a)$ 等於對所有可能的下一個狀態 $s'$ ，在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 的機率 $P(s'|s,a)$ ，再加上在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 並得到即時獎勵 $R(s,a,s')$ ，再加上折扣因子 $\gamma$ 乘上下一個狀態 $s'$ 下最大動作值函數 $Q(s',a')$。

貝爾曼方程是值迭代、策略評估和策略改善等強化學習算法的基礎。通過不斷迭代更新值函數，智能體可以逐步改進策略，達到最優策略。
