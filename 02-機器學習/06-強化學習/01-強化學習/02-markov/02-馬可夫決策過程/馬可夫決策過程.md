# 馬可夫決策過程

* from -- https://ithelp.ithome.com.tw/articles/10201186

和馬可夫鏈比起來，馬可夫決策過程有一個很明顯的差異，轉移的過程不再是依照轉移矩陣，有一個固定的轉移機率，而是按照處於的狀態與動作決定。數學家是這麼定義的：

1. 狀態集 (Set of states)：所有可能的狀態 S
2. 初始狀態 (Initial state)： s0
3. 動作集 (Set of actions)：所有可能的動作，記為 A
4. 轉移模型 (Transition model)：轉移到下一個狀態的模型，記為 T(s,a,s')
5. 獎勵函數 (Reward function)：評價「決策的結果是好、或不好」的函數，記為 r(s,a,s')

## 2.2 馬可夫決策過程

馬可夫決策過程（Markov Decision Process，MDP）是一種描述強化學習問題的數學框架。MDP由五元組$(S,A,P,R,\gamma)$構成，其中：

1. $S$ 是狀態集合。
2. $A$ 是行動集合。
3. $P$ 是狀態轉移機率函數，表示在狀態 $s$ 採取行動 $a$ 之後，智能體轉移到狀態 $s'$ 的概率，即 $P(s'|s,a)$ 。
4. $R$ 是獎勵函數，表示在狀態$s$採取行動$a$之後，智能體獲得的即時獎勵，即 $R(s,a)$ 。
5. $\gamma$ 是折扣因子，表示將來的獎勵價值對當前獎勵價值的影響程度。

在MDP中，智能體需要學習一個策略 $\pi:S\rightarrow A$ ，該策略表示在每個狀態下應該採取哪個行動。學習策略的過程稱為策略評估和策略改善。策略評估的目的是估計一個策略的值函數 $V_{\pi}(s)$ 或動作值函數 $Q_{\pi}(s,a)$ ，其中 $V_{\pi}(s)$ 表示在狀態$s$下採取策略$\pi$的期望回報， $Q_{\pi}(s,a)$ 表示在狀態 $s$ 下採取行動 $a$ 後，再採取策略 $\pi$ 的期望回報。策略改善的目的是通過更新策略來提高值函數。

在強化學習中，我們通常使用基於值函數的方法來解決 MDP 問題，包括Q-learning和Deep Q-Network等。在後續章節中，我們將介紹這些方法的原理和實現方法。

