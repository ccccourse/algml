## Q-Learning

Q-Learning是一种典型的无模型（model-free）的强化学习算法，它通过迭代地更新动作值函数来逐渐逼近最优动作值函数，从而实现最优的决策。Q-Learning常常表达为以下的形式：

$$Q(s_t, a_t)←Q(s_t, a_t)+\alpha(reward+\gamma \max_{a} Q(s_{t+1},a)-Q(s_t,a_t))$$

其中，$Q(s_t,a_t)$ 表示在状态 $s_t$ 时选择动作 $a_t$ 所得的收益值，$\alpha$ 是学习率（learning rate），$reward$ 是奖赏（reward），$\gamma$ 是折扣因子。$\max_{a}Q(s_{t+1},a)$ 表示选择在下一个状态 $s_{t+1}$ 时能够得到的最大收益值。

Q-Learning的关键在于更新动作值函数，通过逐渐迭代动作值函数来逼近最优动作值函数，最终学习到最优的策略。可能的动作数量是有限的，因此可以维护一个Q表的数据结构，它在每个状态下保存每个可能动作的预期回報值。随着学习的进行，Q表的值将被不断更新，最终得到最优的预期回报值，从而使智能体通过动作值函数得到最优的动作并获得最大的实际奖励。


## Q-Learning 與 SARSA

* 參考 [3.4.2 Q学习：异策略时序差分控制](https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3?id=_342-q%e5%ad%a6%e4%b9%a0%ef%bc%9a%e5%bc%82%e7%ad%96%e7%95%a5%e6%97%b6%e5%ba%8f%e5%b7%ae%e5%88%86%e6%8e%a7%e5%88%b6)

如图 3.34a 所示，Sarsa 用自己的策略产生了 $S,A,R,S',A'$ 这条轨迹，然后用 $Q(S_{t+1}, a_{t+1})$ 去更新原本的 Q 值 
 )。 但是 Q学习 并不需要知道我们实际上选择哪一个动作 ，它默认下一个动作就是 Q 值最大的那个动作。

![](https://datawhalechina.github.io/easy-rl/img/ch3/3.18.png)

補充： ε-贪心探索: Agent 在探索時，有一個很小的機率 ε 選擇未知的動作，剩下 1-ε 的機率選擇已知的動作。


* Sarsa 是一个典型的同策略算法，它只用了一个策略
π，它不仅使用策略 π 学习，还使用策略 π 与环境交互产生经验。 如果策略采用 ε-贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 ε-贪心 算法，策略会不断改变（ε 值会不断变小），所以策略不稳定。

* Q学习是一个典型的异策略算法，它有两种策略————目标策略和行为策略，它分离了目标策略与行为策略。Q学习可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 ε-贪心 算法，但目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以 Q学习 不需要兼顾探索。

* 我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。