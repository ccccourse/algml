# 強化學習整體架構

1. 有模型： 已知 P(s'|s,a)
    * 可以用策略改良，或者動態規劃找到最佳策略
2. 無模型： 未知 P(s'|s,a)
    * 增量法 (基於隨機近似理論中的 Robbins Monro 算法)
    * 問題是可能一直死掉，到不了終點，所以後面接近終點那些都測試不到。
    * 1. 此時可採用『起始探索』，讓任何 s0,a0 都能成為起始
    * 2. 無法採用『起始探索』的，可以用『柔性策略』（epsilon 柔性策略）（例如 epsilon 貪心就是給最好的 (1-epsilon), 剩下的 epsilon 平均分配給其他狀態。

* [強化學習有模型和無模型，差異在哪？](https://chatgpt.com/c/6729a750-40bc-8012-afdf-004be2b6fc78)

## 無模型方法

1. Q-Learning: 

    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    $$

    Q[s,a] += alpha*(reward + gamma*np.max(Q[s1,:]) - Q[s,a])

2. SARSA: 

    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
    $$

    a1 = np.argmax(Q[s1, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))  # 選擇下一步行動 a'
    Q[s, a] += alpha * (reward + gamma * Q[s1, a1] - Q[s, a])  # 使用 SARSA 更新公式


* https://github.com/datawhalechina/easy-rl/blob/master/notebooks/Sarsa.ipynb

Sarsa算法跟Q learning算法基本模式相同，但是根本的区别在于，Sarsa是先做出动作然后拿这个做的动作去更新，而Q learning是假定下一步最大奖励对应的动作拿去更新，然后再使用
-greedy策略，也就是说Sarsa是on-policy的，而Q learning是off-policy的。

3. TD(lambda):


    a1 = np.argmax(Q[s1, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))
    delta = reward + gamma * Q[s1, a1] - Q[s, a]  # TD誤差計算

    # 更新資格跡 E[s,a]

    Q[s, a] += alpha * delta * E[s, a]

