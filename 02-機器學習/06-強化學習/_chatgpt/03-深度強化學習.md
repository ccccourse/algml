
* 策略梯度方法
* Q-Learning
* SARSA

## 策略梯度定理

$$
J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
$$

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \nabla_\theta \log \pi_{\theta}(a|s) Q^\pi(s, a) \right]
$$

## 連續動作空間的策略梯度定理

$$
\nabla E_{\pi(\theta)}[G_0] = E \left[ \sum_{t=0}^{+\infty} \gamma^t \nabla \pi(S_t;\theta)  [\nabla_a q_{\pi(\theta)} (S_t,a)]_{a=\pi(S_t; \theta)} \right]
$$

## 從 Baseline 到 Critic

ChatGPT: https://chatgpt.com/c/672dd2ce-a47c-8012-a56b-341adef952f2

* ccc: 強化學習 VPG (vanilla Policy Gradient) 簡單策略梯度算法，為何要擴充為 有 Baseline 的版本， Baseline 有何用途？

結論摘要: Baseline 的選擇很重要。最常見的做法是使用 狀態價值函數 V(s_t)，因為它估計了該狀態下後續的期望回報，可以有效地平衡不同狀態之間的回報。

用 V(s) 輔助 q(s,a) 的計算，沒有 a 會更穩定，所以能幫助 q(s,a) 估計。

* [ccc: actor critic 中的 critic 算是一種 baseline 方法嗎？](https://chatgpt.com/c/673165e6-6a1c-8012-a41e-411f8022fce8)




## ccc: 何謂策略梯度定理？

策略梯度定理（Policy Gradient Theorem）是強化學習中的一個重要理論，主要用於優化策略函數。在強化學習中，策略（Policy）決定了智能體在給定環境狀態下該如何行動，而策略梯度方法則通過直接對策略進行微調來尋求最優策略。

**策略梯度定理的基本概念：**

假設有一個強化學習的環境，並且智能體的策略用 \( \pi_{\theta}(a|s) \) 來表示，其中 \( \theta \) 是策略的參數，\( a \) 是行動，\( s \) 是狀態。目標是最大化期望回報（或獎勳），這可以表示為：

\[
J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
\]

其中：
- \( J(\theta) \) 是期望回報函數；
- \( r_t \) 是在時間步 \( t \) 的獎勳；
- \( \gamma \) 是折扣因子；
- \( \mathbb{E}_{\pi_{\theta}} \) 表示根據策略 \( \pi_{\theta} \) 進行的期望。

**策略梯度定理** 描述了如何計算這個期望回報對策略參數 \( \theta \) 的梯度。具體來說，策略梯度定理告訴我們，策略參數的梯度可以表示為：

\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \nabla_\theta \log \pi_{\theta}(a|s) Q^\pi(s, a) \right]
\]

其中：
- \( \nabla_\theta J(\theta) \) 是期望回報對策略參數 \( \theta \) 的梯度；
- \( \pi_{\theta}(a|s) \) 是在狀態 \( s \) 下採取行動 \( a \) 的概率；
- \( Q^\pi(s, a) \) 是在狀態 \( s \) 和行動 \( a \) 下的 Q 函數，表示從該狀態行動開始，後續的期望回報。

這個公式的意思是，策略的參數應該朝著增加 \( \log \pi_{\theta}(a|s) \) 和 \( Q^\pi(s, a) \) 的乘積的方向進行調整。簡單來說，這是一種基於策略的優化方法，其中智能體通過調整自己的策略參數來最大化期望回報。

**如何使用策略梯度：**

1. **採樣數據：** 透過與環境互動來收集狀態、行動和回報的序列。
2. **計算梯度：** 根據策略梯度定理計算梯度，這需要使用 \( \log \pi_{\theta}(a|s) \) 和 \( Q^\pi(s, a) \)。
3. **更新策略：** 使用梯度下降方法更新策略參數 \( \theta \)。

策略梯度方法適用於具有高維或連續動作空間的情況，特別是在難以直接建構值函數的情形下（例如策略基方法比值基方法更合適的場合）。

## 連續動作空間的梯度方法

