### 範例與案例分析

---

### **1. 交叉熵的基礎範例**

#### **1.1. 手工計算交叉熵損失**

考慮一個二分類問題，真實標籤和模型預測的概率如下：

- 真實標籤 \(y = [1, 0]\) （對應於第 1 類）
- 預測分布 \(Q = [0.9, 0.1]\)

交叉熵損失的計算公式：
\[
\mathcal{L} = - \sum_{i=1}^{C} y_i \log Q_i
\]

計算：
\[
\mathcal{L} = - (1 \cdot \log 0.9 + 0 \cdot \log 0.1) = - \log 0.9 \approx 0.105
\]

#### **1.2. 多分類問題**

假設真實標籤為第 2 類 \(y = [0, 1, 0]\)，模型的預測分布為 \(Q = [0.2, 0.7, 0.1]\)。交叉熵損失為：
\[
\mathcal{L} = - (0 \cdot \log 0.2 + 1 \cdot \log 0.7 + 0 \cdot \log 0.1) = - \log 0.7 \approx 0.357
\]

#### **1.3. 使用 PyTorch 計算**

```python
import torch
import torch.nn as nn

# 模型輸出的 logits 和真實標籤
logits = torch.tensor([[2.0, 0.5, 0.3], [0.1, 1.0, 0.2]])  # (N, C)
labels = torch.tensor([0, 1])  # 真實類別索引

# 定義交叉熵損失
criterion = nn.CrossEntropyLoss()

# 計算損失
loss = criterion(logits, labels)
print(f"交叉熵損失: {loss.item()}")
```

在這個範例中，模型輸出的是 logits 而非概率，`CrossEntropyLoss` 內部會自動計算 Softmax，然後再計算交叉熵。

---

### **2. 案例分析：交叉熵在具體應用中的表現**

#### **2.1. 圖像分類**

使用 MNIST 數據集進行手寫數字分類：

- **任務**：預測輸入圖像的數字類別（0–9）。
- **模型**：一個簡單的 CNN，最後一層輸出 10 個類別的 logits。
- **損失函數**：交叉熵損失。

**代碼範例：**

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# 加載數據集
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

# 定義簡單的 CNN
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.fc1 = nn.Linear(32 * 26 * 26, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x

# 初始化模型、損失函數和優化器
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 訓練過程
for epoch in range(3):  # 訓練 3 個 epoch
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")
```

#### **分析結果**：

1. **損失下降**：隨著訓練進行，模型逐漸學會正確分類，交叉熵損失減小。
2. **模型泛化**：在測試數據上，交叉熵損失越小，模型的準確率通常越高。

---

#### **2.2. 自然語言處理**

使用 IMDb 數據集進行情感分析：

- **任務**：預測文本的情感（正面或負面）。
- **模型**：LSTM 模型，最後一層輸出二元分類的 logits。
- **損失函數**：交叉熵損失。

**代碼範例：**

```python
import torch.nn as nn

class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, output_size):
        super(SentimentLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        x = self.fc(hidden[-1])
        return x

# 模型初始化
model = SentimentLSTM(vocab_size=5000, embed_size=128, hidden_size=64, output_size=2)
criterion = nn.CrossEntropyLoss()
```

---

#### **2.3. 多標籤分類**

在多標籤分類中（如目標檢測），每個輸出節點代表一個類別的概率，使用 Binary Cross-Entropy 而非多分類交叉熵。

---

### **3. 常見問題與解決方案**

1. **模型輸出為 logits 還是概率？**
   - 交叉熵通常直接接受 logits，無需手動計算 Softmax。

2. **類別不平衡時的挑戰**
   - 使用加權交叉熵損失，平衡不同類別的影響。

3. **標籤的表示**
   - 使用 one-hot 標籤時，需確保損失函數支持該格式，否則可能需要轉化。

---

### **4. 總結**

交叉熵損失在圖像分類、語音識別、文本分析等領域中廣泛應用，具有簡單、穩定且解釋性強的特點。通過理解其數學基礎與應用場景，研究者可以靈活調整損失函數以解決實際問題。