### 相對熵（Kullback-Leibler 散度）

---

### **1. 相對熵的定義**

相對熵，又稱 **Kullback-Leibler (KL) 散度**，是用來衡量兩個概率分布 \(P\) 和 \(Q\) 之間差異的度量。它描述了從分布 \(Q\) "預測" \(P\) 的結果所需的額外資訊量。

#### 數學定義：
對於離散隨機變數 \(X\)：
\[
D_{\text{KL}}(P \| Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}
\]
對於連續隨機變數：
\[
D_{\text{KL}}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx
\]

- \(P(x)\)：實際分布（真實分布）。
- \(Q(x)\)：近似分布（假設分布）。
- 單位由對數的底數決定（通常為比特或奈特）。

---

### **2. 相對熵的直觀意義**

1. **衡量差異**：
   - \(D_{\text{KL}}(P \| Q)\) 表示從 \(Q\) 分布編碼觀測值但實際分布為 \(P\) 時，平均每個樣本額外需要的資訊量。

2. **非對稱性**：
   - \(D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)\)，因此它不是一種"距離"。
   - 從 \(P\) 到 \(Q\) 的"方向"和從 \(Q\) 到 \(P\) 的"方向"可能有不同的結果。

---

### **3. 相對熵的特性**

1. **非負性**：
   \[
   D_{\text{KL}}(P \| Q) \geq 0
   \]
   當且僅當 \(P = Q\) 時，\(D_{\text{KL}}(P \| Q) = 0\)。

2. **非對稱性**：
   \[
   D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)
   \]

3. **無法直接解釋為距離**：
   - 它不滿足三角不等式，且具有方向性。

---

### **4. 相對熵的應用**

1. **模型評估**：
   - 用於衡量模型分布 \(Q\) 與真實分布 \(P\) 的差異，特別是在機器學習中。

2. **最大似然估計**：
   - 在許多情況下，最大化似然函數等價於最小化 \(D_{\text{KL}}(P \| Q)\)。

3. **信息壓縮**：
   - 衡量使用錯誤模型 \(Q\) 壓縮基於分布 \(P\) 生成的數據時的效率損失。

4. **變分推斷**：
   - 在貝葉斯推斷中，用於衡量後驗分布與近似分布的差異。

5. **自然語言處理與圖像處理**：
   - 用於生成模型（如變分自編碼器，VAE）和語義相似性分析。

---

### **5. 相對熵的計算範例**

#### 離散分布：
假設 \(P\) 和 \(Q\) 定義為：
\[
P(X) = \{0.8, 0.2\}, \quad Q(X) = \{0.7, 0.3\}
\]
則相對熵：
\[
D_{\text{KL}}(P \| Q) = 0.8 \log \frac{0.8}{0.7} + 0.2 \log \frac{0.2}{0.3}
\]
計算得：
\[
D_{\text{KL}}(P \| Q) \approx 0.0614 \, \text{比特}
\]

#### 連續分布：
假設 \(P(x)\) 和 \(Q(x)\) 為高斯分布：
- \(P(x) = \mathcal{N}(\mu_1, \sigma_1^2)\)
- \(Q(x) = \mathcal{N}(\mu_2, \sigma_2^2)\)

相對熵的閉式解：
\[
D_{\text{KL}}(P \| Q) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
\]

---

### **6. 相對熵與交叉熵的關係**

交叉熵 \(H(P, Q)\) 定義為：
\[
H(P, Q) = H(P) + D_{\text{KL}}(P \| Q)
\]
- \(H(P)\)：真實分布的熵。
- \(H(P, Q)\)：用 \(Q\) 分布編碼 \(P\) 時的總編碼長度。

這表明，交叉熵等於真實分布熵加上模型與真實分布的差異。

---

### **7. 相對熵的局限性**

1. **非對稱性問題**：
   - 不對稱性可能導致在一些應用中解釋困難。

2. **對零概率的敏感性**：
   - 如果 \(Q(x) = 0\) 且 \(P(x) > 0\)，則 \(D_{\text{KL}}(P \| Q)\) 無窮大。

3. **無法直接比較模型好壞**：
   - 單獨的 \(D_{\text{KL}}(P \| Q)\) 無法說明模型是否合適，需結合其他指標。

---

### **8. 延伸與改進**

1. **對稱化相對熵**：
   - 使用 Jensen-Shannon 散度（JSD）將 KL 散度對稱化：
     \[
     D_{\text{JS}}(P \| Q) = \frac{1}{2} D_{\text{KL}}(P \| M) + \frac{1}{2} D_{\text{KL}}(Q \| M)
     \]
     其中 \(M = \frac{1}{2}(P + Q)\)。

2. **KL 散度正則化**：
   - 在機器學習中，加入 \(D_{\text{KL}}(P \| Q)\) 作為正則項，限制模型的複雜度。

相對熵作為資訊理論中的核心工具，不僅揭示了概率分布之間的差異性，還為數據建模與學習提供了重要的數學基礎。