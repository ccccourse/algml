## 4.2 正則化

在機器學習中，正則化是用來控制模型過度擬合（overfitting）的一種方法，通常會在損失函數（loss function）中加入正則化項，這個項會隨著模型參數增大而增大，從而逼使模型參數趨近於 0，進而達到防止過度擬合的效果。

監督式學習中會用到的正則化方法有：
- L1 正則化：L1 正則化會讓參數向量變得稠密，也就是只有部分的參數會被使用，因此可以達到特徵選取（feature selection）的效果。在 L1 正則化下，損失函數會加上參數向量中各個元素的絕對值之和（稱為 L1 范數或 Lasso 懲罰項），數學公式如下：

    J(w) = MSE(y, y_hat) + α||w||1

    其中，MSE 是均方誤差，y 是實際值，y_hat 是預測值，α 是正則化參數，w 是模型參數向量。

    在 Scikit-Learn 中，L1 正則化用的是 Lasso 類別，可以使用的參數有 alpha（正則化力度）和 max_iter（最大迭代次數），使用方法如下：

    ```
    from sklearn.linear_model import Lasso

    lasso = Lasso(alpha=0.1, max_iter=10000)
    lasso.fit(X_train, y_train)
    ```

- L2 正則化：L2 正則化會讓參數向量變得稠密，但是不像 L1 正則化一樣會使得某些參數變為 0，而是讓各個參數都趨近於 0。在 L2 正則化下，損失函數會加上參數向量中各個元素的平方和的一半（稱為 L2 范數或 Ridge 懲罰項），數學公式如下：

    J(w) = MSE(y, y_hat) + α||w||2^2

    其中，MSE 是均方誤差，y 是實際值，y_hat 是預測值，α 是正則化參數，w 是模型參數向量。

    在 Scikit-Learn 中，L2 正則化用的是 Ridge 類別，可以使用的參數有 alpha（正則化力度）和 solver（求解方法），使用方法如下：

    ```
    from sklearn.linear_model import Ridge

    ridge = Ridge(alpha=0.1, solver="cholesky")
    ridge.fit(X_train, y_train)
    ```

- Elastic Net：Elastic Net 是 L1 和 L2 正則化的結合體，可以同時達到特徵選取和避免過度擬合的效果。在 Elastic Net 正則化下，損失函數會加上 L1 范數和 L2 范數的線性組合，數學公式如下：

    J(w) = MSE(y, y_hat) + αρ||w||1 + α(1-ρ)||w||2^2

    其中，MSE 是均方誤差，y 是實際值，y_hat 是預測值，α 是正則化參數，ρ 是 L1 和 L2 的比重參數，w 是模型參數向量。

    在 Scikit-Learn 中，Elastic Net 正則化用的是 ElasticNet 類別，可以使用的參數有 alpha（正則化力度）、l1_ratio（L1 范數比重）、max_iter（最大迭代次數），使用方法如下：

    ```
    from sklearn.linear_model import ElasticNet

    elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)
    elastic_net.fit(X_train, y_train)
    ```

相較於不使用正則化的模型，正則化方法可以增加模型的泛化能力，讓模型適用於更多的數據集。在使用正則化時，需要調整相應的參數，並通過交叉驗證（cross-validation）等方法進行模型的選擇。