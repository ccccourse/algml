## 第三章 非監督式學習

## 非監督式學習介紹
監督式學習通常需要有標籤的數據集，目標是建立一個模型來預測結果或分類。相比之下，非監督式學習並不需要有標籤的數據，其目的是探索數據中的結構和模式。

非監督式學習有很多應用場景，比如： 文本分類、推薦系統、影像分類、自然語言處理等等。本文介紹四個非監督式學習的模型：K-means、層次聚類、DBSCAN、主成分分析(PCA)。

## K-means

K-means 是最常用的聚類算法之一，其基本思想是將數據集劃分為 *k* 個不同的集群，每個集群都代表一個獨立的群體。與其他聚類算法相比，K-means 不需要事先確定群體數量，而是經過多次迭代學習到最佳的群體數量和中心點。

以下是使用 scikit-learn 運行 K-means 的一個示例。首先，我們從 scikit-learn 加載 Iris 數據集。

```python
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:, :4]
y = iris.target
```

接下來，我們使用 KMeans 模型進行聚類。KMeans 的重要的超參數是 k 值（群體數量），通過遍歷不同的 k 值，我們可以尋找最優的聚類狀態。

```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X)
```

這段代碼中，我們指定了 k 值為 3，並隨機選擇中心點。通過 fit() 方法訓練模型，然後 predict() 方法將已知數據集分配到不同的集群中。

```python
y_pred = kmeans.predict(X)
```

此外，可以通過分群效果圖快速瞭解 KMeans 的分群效果：

```python
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.title("K-means clustering of iris data")
plt.show()
```
![image.png](attachment:image.png)


## 層次聚類

層次聚類方法是一種分配數據點到不同層次的方法，每一層次都對應一個不同的聚類形態。在層次聚類中，我們需要選擇距離度量標準和鏈接策略。距離度量標準可以是歐氏距離、曼哈頓距離或者其他相似的測度。常用的鏈接策略包括最短距離標準、最長距離標準和平均距離標準。

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram

X, y = make_blobs(n_samples=10, centers=3, random_state=42)

agg = AgglomerativeClustering(n_clusters=3)
labels = agg.fit_predict(X)

linkage_matrix = ward(X)
dendrogram(linkage_matrix, p=30, truncate_mode='level')
plt.xlabel("Sample index")
plt.ylabel("Cluster distance")
plt.title("Hierarchical clustering on toy example")
plt.show()
```

上面的代碼生成一個玩具數據集，該數據集包含三個集群。
我們創建 AgglomerativeClustering 對象，並指定群體數量為 3。
fit_predict() 方法執行分群。通過 Ward 鏈接策略和歐氏距離度量，我們生成階層聚類的樹形圖。

## DBSCAN

DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一種基於密度的聚類算法，將相鄰的數據點聚類在一起。相對於 K-means 和層次聚類，DBSCAN 不需要事先指定群體數量。

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(X)

plt.scatter(X[:, 0], X[:, 1], c=clusters)
plt.title("DBSCAN clustering on moons data")
plt.show()
```

在上面的示例中，我們使用 DBSCAN 將 Moon 數據集進行了分群。其中，eps 參數和 min_samples 參數是 DBSCAN 的關鍵超參數。eps 給出相鄰點的最大距離，大於此距離的兩個點之間視為不相鄰。min_samples 是指在 eps 銜接的數據點數最小值，這是將數據點聚類在一起的關鍵標準。

## PCA

主成分分析（PCA）是一種數據降維算法，用於發現數據中的模式。PCA 將高維數據投影到低維空間中，從而可以可視化和分析數據的特徵。

PCA 的本質是找到一些新的維度或主成分，這些成分是原始數據線性組合的結果。這些新的主成分是不相關的，即它們之間沒有任何相關性。因此，PCA 可以將元數據轉換為一組不相關的變量（主成分），這些變量論證了數據的內在結構。

讓我們看一個使用 PCA 可視化數據分佈的例子:

```python
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

pca = PCA(n_components=2)
X_r = pca.fit_transform(X)

plt.figure()
colors = ['navy', 'turquoise', 'darkorange']
lw = 2

for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name)

plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of IRIS dataset')
plt.show()
```

這段代碼中，我們首先使用 PCA 將 iris 數據集從 4 維降到 2 維。然後，我們使用散點圖可視化 iris 數據集中的三個點。三個顏色分別代表 iris 的三個品種。

## 總結

本文介紹了四種非監督式學習方法：K-means、層次聚類、DBSCAN、PCA。這些技術可以幫助我們更好地探索和理解數據中的結構和模式。

在 scikit-learn 中使用非監督式學習非常容易，只需要幾行代碼就可以完成一項任務。熟練掌握這些技術可以讓我們在數據探索中更加游刃有余。