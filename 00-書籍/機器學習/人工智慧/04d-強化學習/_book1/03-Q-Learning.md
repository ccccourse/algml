# 第三章： Q-Learning

## 3.1 Q-Learning基本概念

Q-Learning是一種基於模型的、無模型的強化學習算法，旨在學習動作值函數$Q(s,a)$，並從中獲得最佳策略。Q-Learning是一種基於監督學習的方法，通過觀察到的即時獎勵，來調整已知的狀態和動作的價值。

Q-Learning算法的基本流程如下：

1. 初始化動作值函數 $Q(s,a)$ ；
2. 選擇一個初始狀態 $s$ ；
3. 在當前狀態 $s$ 下選擇一個行動 $a$ ；
4. 執行行動 $a$ ，觀察到環境的新狀態 $s'$ 和即時獎勵 $R(s,a,s')$ ；
5. 用貝爾曼方程更新動作值函數 $Q(s,a)$ ：

    $$
    Q(s,a) \leftarrow Q(s,a) + \alpha \left(R(s,a,s') + \gamma \max_{a'} Q(s',a') - Q(s,a)\right)
    $$
    
    其中，$\alpha$ 是學習率， $\gamma$ 是折扣因子，用於調整未來獎勵的價值，$\max_{a'} Q(s',a')$ 表示下一個狀態 $s'$ 下最大的動作值函數值；
    
6. 重複執行步驟3至5，直到達到終止狀態。

在實際應用中，Q-Learning通常與策略控制方法（如$\epsilon$-貪婪策略）相結合，以實現更好的性能。

## 3.2 Q-Learning算法
## 3.3 Q-Learning的實現
## 3.4 Q-Learning的應用