## 6. Actor-Critic 算法

Actor-Critic (AC) 是一種結合了 policy-based 和 value-based 方法的強化學習算法。Actor-Critic 方法中，一個 agent 來進行相互作用，agent 包含了兩種類型的 network，一個是直接進行 policy 學習的 actor，另一個是評估 state-value 的 critic。

<!-- more -->

在這個設置下，actor 傳遞 state 作為輸入，輸出結果是它應該采取的行動；critic 則使用 state 作為輸入，輸出結果是對 state 的評估價值。

Actor-Critic 通常被認為是比單純的 policy-based 方法更穩定，而且比單純的 value-based 方法能夠更優雅地處理連續的行動空間。

與簡單的 SARSA 或 Q-learning 不同，在 AC 中，現在的狀態 s 與下一個狀態 s' 都會被考慮用於更新策略，其公式如下：

$$
\theta \leftarrow \theta + \alpha(r + \gamma V(s') - V(s)) \nabla_\theta \log \pi(a|s,\theta)
$$

$$
V(s) \leftarrow V(s) + \beta (r + \gamma V(s') - V(s))
$$

其中 $\theta$ 是 actor 在 policy 搜尋空間中的權重，$V$ 是 state 值函數，$\log \pi$ 是哈慈積分，$\beta$ 是 critic 的步長，$s$，$a$，$s'$ 和 $r$ 分別表示當前狀態，當前行動，下一個狀態和參與的獎勵。

AC 算法的過程是兩個相互流程，actor 給出動作，然後被 critic 的回饋所學習的 actor 會提升原始策略，最終增強學習的績效。