## 3.3. Q-Learning 算法實現

接下來讓我們來實現 Q-learning 算法。在此之前，讓我們回憶一下 Q-learning 完整的更新規則：

$$Q(s,a)\leftarrow (1-\alpha)Q(s,a) + \alpha(r + \gamma \max_{a'} Q(s',a'))$$

其中 $\alpha$ 是學習速率，$0 \leq \gamma \leq 1$ 是折扣因子。

在 coding 開始前，我們需要先將環境初始化。此範例採用的是 OpenAI Gym 的 FrozenLake 環境。FrozenLake 是一個模擬人走冰面的環境，代表人隻要能避開冰面的洞和結冰不良的區塊，千里冰封也能走過。

下面是初始化環境的程式碼：


```python
import gym

env = gym.make('FrozenLake-v0')
```

接下來我們定義學習參數以及 Q-table：


```python
# 超參數
alpha = 0.8          # 學習速率
gamma = 0.95         # 折扣因子
num_episodes = 2000  # 迭代次數

# Q-table
Q = np.zeros([env.observation_space.n,env.action_space.n])
```

現在我們開始實現 Q-learning 算法。過程如下：


```python
# 執行所有迭代
for i in range(num_episodes):
    # 初始化環境
    s = env.reset()
    rAll = 0
    d = False
    j = 0
    # Q-learning 更新規則
    while j < 99:
        j += 1
        # 選擇行動
        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))
        # 取得新的狀態和報酬
        s1,r,d,_ = env.step(a)
        # 將新的知識累積到 Q-table 中
        Q[s,a] = Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:]) - Q[s,a])
        rAll += r
        s = s1
        if d == True:
            break
print("完成迭代")
```


在每次迭代中，我們基於現有的 Q-table ，按照某個策略（比如 $\epsilon$-greedy 或隨機策略）選擇一個行動並進行。此處採用門檻式的隨機策略，而 $\epsilon$值按 $\dfrac{1}{i+1}$的間隔遞減。

取得新的狀態和報酬後，便可進行 Q-learning 更新：以新的策略更新現有知識，將更新後的值存入 Q-table 中。而迭代次數則是預設值 $2000$。

接下來我們來看一下，訓練完成後的 Q-table 長怎樣：


```python
print(Q)
```

輸出結果：


```python
array([[6.17281296e-02, 2.28834619e-02, 2.27206213e-02, 2.26468236e-02],
       [2.61820153e-03, 2.03141954e-02, 2.24876061e-03, 2.00388865e-02],
       [1.23676879e-02, 1.22384108e-02, 1.63765495e-02, 1.79192705e-02],
       [6.86123056e-03, 8.31540426e-04, 2.76343018e-03, 6.59019414e-03],
       [7.52726172e-02, 2.76028723e-03, 2.34946582e-02, 8.65183959e-03],
       [0.00000000e+00, 7.04910020e-03, 0.00000000e+00, 7.92835458e-03],
       [2.38527445e-04, 1.29474960e-04, 7.21190945e-02, 6.93604704e-06],
       [0.00000000e+00, 2.01372972e-02, 0.00000000e+00, 0.00000000e+00],
       [3.97320454e-03, 1.33972151e-02, 2.86650437e-03, 6.36635441e-02],
       [1.27782290e-02, 7.74615374e-02, 1.07973668e-02, 3.59735970e-02],
       [1.45990386e-01, 2.08787951e-03, 2.08148311e-03, 1.03971659e-03],
       [0.00000000e+00, 8.68228256e-07, 9.95982654e-04, 0.00000000e+00],
       [0.00000000e+00, 4.77527694e-03, 1.90278164e-01, 1.65216732e-03],
       [0.00000000e+00, 0.00000000e+00, 2.74267200e-02, 0.00000000e+00],
       [1.89590584e-02, 2.25401205e-01, 2.36229076e-01, 4.82782210e-02],
       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])
```

在更新完 Q-table 後，我們可以開始使用現有知識行走，看看訓練的成果如何。接下來，我們定義測試環境的部分。每走一步，都會印出現在所處位置和可供選擇的行動，以及 Q-table 中每個行動的價值：


```python
# 測試
s = env.reset()
for i in range(100):
    env.render()
    a = np.argmax(Q[s,:])
    s,r,d,_ = env.step(a)
    if d == True:
        break
```

執行上述程式碼，會開啟一個遊戲模擬視窗，我們能看到起點 S 和終點 G 以及洞 H。玩家將以 S 為起點，走到終點 G 即代表過關；但若掉進 H 則輸掉遊戲。正在遊玩的樣子如下：


```
SFFF
FHFH
FFFH
HFFG
  (Down)
SFFF
FHFH
FFFH
HFFG
  (Down)
SFFF
FHFH
FFFH
HFFG
  (Down)
SFFF
FHFH
FFFH
HFFG
  (Down)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
  (Right)
SFFF
FHFH
FFFH
HFFG
```


我們能夠發現，訓練後的智能體能夠成功抵達終點 G ，並且良好地避開了洞 H。這說明了 Q-learning 通過反覆更新 Q-table，能夠讓智能體對規則有一定程度的瞭解，使得智能體能夠順利地從起點走到終點並制勝。

此處只介紹了簡單的 Q-learning 範例，實務上 Q-learning 能夠處理更大規模的問題。但是，在處理更複雜的問題時，Q-learning 沒有克服所有問題。例如，當狀態具有高度扭曲性時，Q-learning 可能會從一個未經過相關狀態的狀態開始遨遊並永遠不會找到更好的路徑。此時就需要更為先進的演算法來幫助機器學習更複雜的任務，包括深度 Q-learning 等。