## 6.1. Actor-Critic 算法簡介

Actor-Critic 則是一種介紹中較為流行的算法，它是 Policy Gradient 的一種改進，也可以用於連續狀態的問題。Actor-Critic 的核心思想是將 policy 和 value 的訓練同時進行。

具體來講，模型包含兩部分：Actor 和 Critic。Actor 將對應狀態輸出策略分佈，控制系統在此狀態下的行為；Critic 則是評價預測狀態的質量，促進智能體學習到最佳策略。而 Actor 則使用 Policy Gradient 原理進行策略的更新，Critic 則使用 Temporal Difference (TD) 策略評估，訓練中對每個 state-action pair 給出一個對值函數的估計，稱為 TD error，根據此調整 Critic 的參數。同時，Actor 的參數也隨著 TD error 進行隨機梯度下降更新。

Actor-Critic 算法又可再細分為多種類型，如 Advantage-Actor-Critic (A2C) 算法、Asynchronous Advantage-Actor-Critic(A3C) 算法等等，這些算法均屬於 Actor-Critic 的改進版，通過不斷開發創新，逐漸提高 Agent 的學習效率和性能。