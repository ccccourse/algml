# 第二章： 馬可夫鏈與決策過程

## 2.1 馬可夫鏈

馬可夫鏈是一種隨機過程，它具有馬可夫性質，即當前狀態的未來發展只與當前狀態有關，與過去狀態無關。一個馬可夫鏈可以用一個有限或無限的狀態集合來表示，並且每個狀態都有一個轉移概率矩陣，表示在當前狀態下轉移到下一個狀態的概率。

在強化學習中，馬可夫鏈被廣泛用於描述智能體在不同環境狀態之間的轉移。為了方便，我們將在本書中使用有限馬可夫鏈來描述環境狀態之間的轉移。

有限馬可夫鏈由以下三個要素組成：

1. 狀態集合

有限馬可夫鏈的狀態集合表示環境中可能出現的不同狀態，通常用$S$表示。狀態集合可以是有限的，也可以是無限的。

2. 轉移概率矩陣

有限馬可夫鏈的轉移概率矩陣 $P$ 表示從一個狀態轉移到另一個狀態的概率。矩陣中的每個元素 $P_{s,s'}$ 表示當前狀態為 $s$ 時，下一個狀態為 $s'$的概率。轉移概率矩陣可以表示為：

$$P[N,N]$$

其中， $N$ 表示狀態集合中的狀態數量。

3. 初始概率分布

有限馬可夫鏈的初始概率分布表示在起始狀態時，每個狀態出現的概率。通常，初始概率分布可以表示為一個概率向量 $\pi$ ，其中 $\pi_s$ 表示初始狀態為 $s$ 的概率。

有限馬可夫鏈的動態可以表示為一個序列，其中每個狀態是從上一個狀態轉移而來的。給定一個初始狀態，我們可以根據轉移概率矩陣生成一個狀態序列。具體地，給定初始狀態 $s_0$ ，我們可以根據轉移概率矩陣 $P$ 生成狀態序列 $s_0, s_1, s_2, \cdots$ ，其中 $s_{t+1}$ 的概率為 $P_{s_t,s_{t+1}}$ 。

在強化學習中，智能體在不同狀態之間的轉移可以用馬可夫鏈來描述。智能體所處的環境可以被看作一個馬可夫鏈，智能體所採取的行動可以影響環境狀態的轉移，而環境狀態的轉移又可以影響智能體所處的環境狀態。因此，強化學習中的環境通常被描述為一個馬可夫決策過程。

在後續章節中，我們將介紹如何使用強化學習算法來解決馬可夫決策過程中的問題。

## 2.2 馬可夫決策過程

馬可夫決策過程（Markov Decision Process，MDP）是一種描述強化學習問題的數學框架。MDP由五元組$(S,A,P,R,\gamma)$構成，其中：

1. $S$ 是狀態集合。
2. $A$ 是行動集合。
3. $P$ 是狀態轉移機率函數，表示在狀態 $s$ 採取行動 $a$ 之後，智能體轉移到狀態 $s'$ 的概率，即 $P(s'|s,a)$ 。
4. $R$ 是獎勵函數，表示在狀態$s$採取行動$a$之後，智能體獲得的即時獎勵，即 $R(s,a)$ 。
5. $\gamma$ 是折扣因子，表示將來的獎勵價值對當前獎勵價值的影響程度。

在MDP中，智能體需要學習一個策略 $\pi:S\rightarrow A$ ，該策略表示在每個狀態下應該採取哪個行動。學習策略的過程稱為策略評估和策略改善。策略評估的目的是估計一個策略的值函數 $V_{\pi}(s)$ 或動作值函數 $Q_{\pi}(s,a)$ ，其中 $V_{\pi}(s)$ 表示在狀態$s$下採取策略$\pi$的期望回報， $Q_{\pi}(s,a)$ 表示在狀態 $s$ 下採取行動 $a$ 後，再採取策略 $\pi$ 的期望回報。策略改善的目的是通過更新策略來提高值函數。

在強化學習中，我們通常使用基於值函數的方法來解決 MDP 問題，包括Q-learning和Deep Q-Network等。在後續章節中，我們將介紹這些方法的原理和實現方法。


## 2.3 貝爾曼方程

貝爾曼方程是解決 MDP 問題的關鍵公式，它描述了狀態值函數$V(s)$和動作值函數 $Q(s,a)$ 之間的關係，並通過递归地迭代求解值函數。貝爾曼方程分為兩種，分別是狀態值函數的貝爾曼方程和動作值函數的貝爾曼方程。

1. 狀態值函數的貝爾曼方程： 

$$V(s) = \sum_{a} \pi(a|s)\sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V(s')]$$ 

上式表示，狀態值函數 $V(s)$ 等於對所有可能的行動 $a$ ，根據策略 $\pi$ 進行加權後的結果，加權因子為 $\pi(a|s)$ ，再加上對所有可能的下一個狀態 $s'$ ，在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 的機率 $P(s'|s,a)$ ，再加上在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 並得到即時獎勵 $R(s,a,s')$ ，再加上折扣因子 $\gamma$ 乘上下一個狀態 $s'$ 的值函數 $V(s')$ 。

2. 動作值函數的貝爾曼方程：

$$
Q(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma \max_{a'} Q(s',a')]
$$

上式表示，動作值函數 $Q(s,a)$ 等於對所有可能的下一個狀態 $s'$ ，在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 的機率 $P(s'|s,a)$ ，再加上在狀態 $s$ 下採取行動 $a$ 轉移到 $s'$ 並得到即時獎勵 $R(s,a,s')$ ，再加上折扣因子 $\gamma$ 乘上下一個狀態 $s'$ 下最大動作值函數 $Q(s',a')$。

貝爾曼方程是值迭代、策略評估和策略改善等強化學習算法的基礎。通過不斷迭代更新值函數，智能體可以逐步改進策略，達到最優策略。