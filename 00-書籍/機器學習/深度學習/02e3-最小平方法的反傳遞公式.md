## Regression 問題最小平方法的反傳遞公式

最小平方法（Least Squares Method）是迴歸分析中常見的一種方法，用來求解線性回歸模型的參數。在求解參數時，通常使用的是最小二乘法（Ordinary Least Squares, OLS）。

最小二乘法的基本思想是：將所有觀測點到回歸直線的距離平方和最小化，即：

$$\min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$$

其中 $y_i$ 是因變量的觀測值，$x_i$ 是自變量的觀測值，$\beta_0$ 和 $\beta_1$ 是回歸係數（截距和斜率）。

最小二乘法中的模型參數可以通過求解以下方程組得到：

$$\begin{bmatrix}
n & \sum_{i=1}^n x_i \\
\sum_{i=1}^n x_i & \sum_{i=1}^n x_i^2 \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\end{bmatrix}
=
\begin{bmatrix}
\sum_{i=1}^n y_i \\
\sum_{i=1}^n x_i y_i \\
\end{bmatrix}$$

其中 $n$ 是樣本數。

反傳遞（backpropagation）是一種用於多層神經網絡中的參數更新算法，通常用於解決回歸和分類問題。在反傳遞算法中，通過計算損失函數對欲求解參數的偏導數進行階段性反向傳播，以獲得損失函數對於每個欲求解參數的梯度信息，進而使用梯度下降等優化算法對參數進行更新。

對於線性回歸問題，我們可以將最小二乘法的求解過程看作是一種反傳遞算法，其中回歸係數就是神經網絡中的權重參數。

最小二乘法的參數更新規則可以表示為：

$$\begin{aligned}
\beta_0 &= \beta_0 - \alpha \frac{\partial J}{\partial \beta_0} \\
\beta_1 &= \beta_1 - \alpha \frac{\partial J}{\partial \beta_1}
\end{aligned}$$

其中 $\alpha$ 是學習率，$J$ 是損失函數。

根據最小二乘法的定義，損失函數為：

$$J = \frac{1}{2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$$

對 $\beta_0$ 和 $\beta_1$ 求偏導數得到：

$$\begin{aligned}
\frac{\partial J}{\partial \beta_0} &= \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)(-1) \\
&= -\sum_{i=1}^n (y_i - \hat{y_i})
\end{aligned}$$

$$\begin{aligned}
\frac{\partial J}{\partial \beta_1} &= \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)(-x_i) \\
&= -\sum_{i=1}^n (y_i - \hat{y_i}) x_i
\end{aligned}$$

其中 $\hat{y_i}$ 是模型對於第 $i$ 條樣本的預測值。

因此，最小二乘法的反傳遞公式（參數更新規則）可以表示為：

$$\begin{aligned}
\beta_0 &= \beta_0 + \alpha \sum_{i=1}^n (y_i - \hat{y_i}) \\
\beta_1 &= \beta_1 + \alpha \sum_{i=1}^n (y_i - \hat{y_i}) x_i
\end{aligned}$$

其中 $\alpha$ 是學習率。

參考文獻：
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112). New York: springer.
- 李潤鵬. 機器學習中的優化方法——梯度下降, 反傳遞算法與牛頓法[J]. 網址：https://zhuanlan.zhihu.com/p/93640256