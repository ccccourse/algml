## 4.2. 深度 Q 網路

深度 Q 網路 (Deep Q-Network, DQN) 是針對 Q 學習的改進，透過使用深度學習模型，來實現更高維度和複雜度的狀態空間下的 Q 值估計。

DQN 是由 DeepMind 於 2015 年所提出的一種深度強化學習算法，並在 Atari 遊戲上取得了超越人類的表現。DQN 的特點在於其結合了卷積神經網路 (Convolutional Neural Network, CNN) 和 Q 學習的思想，透過 CNN 學習圖像的特徵，並結合 Q 學習對於圖像中的處理結果進行評估。

DQN 的主要思路是基於 Q 學習步驟，在此之上將 Q 函數全局逼近，避免了對於手動提取特徵的需要，例如上述的 Sarsa 和 Q 學習算法都需要人工提取的特徵來表示狀態和行動，而 DQN 可以直接接受原始的狀態圖像來進行學習。

DQN 在學習過程中，由於遊戲場景的規模太大，學習起來非常困難，主要有以下幾個關鍵技術：

1. Experience Replay
2. Fixed Q-Target
3. Dueling Network

將 DQN 中這些技巧結合起來形成一個完整的深度 Q 網路算法模型。

1. Experience Replay

當我們與環境進行交互時，產生了各種不同的狀態，以及相應的行動和獎勵。在 Q 學習中，我們要透過這些狀態來不斷更新 Q 函數，而我們使用 batch 次狀態，來一起進行目標值（TD target）的更新。

因此，為了避免使用這些時序相關聯的數據對目標值的更新產生影響，即防止數據間相關性對學習效果產生干擾，DQN 引入了 Experience Replay 技術。

簡單來說，Experience Replay 技術即是建立一個存儲過去經歷的記憶庫，將 agent 與環境交互得到的數據存儲下來，並在之後的訓練過程中，從這個數據庫中隨機抽樣（一個 batch 大小）用來進行訓練。這樣一方面可以減少狀態間相關性對訓練影響，另一方面也具備了更充分的利用數據。

2. Fixed Q-Target

在更新 Q 函數的過程中，我們需要估計目標值，常用的方法是設計一個辦法來得到逼近的目標值，目標值會受到現有估計策略的影響，很容易引起目標值的震盪和發散。

為了解決這個問題，DQN 使用 Fixed Q-Target 技術，即使用一個不同於當前的 Q 網路，來估計和更新 Q 函數。

具體而言，在 DQN 算法中，我們使用 $\hat{Q}$ 網路作為 Fixed Q-Target，當更新網路 Q 的參數時，我們不計算 Q 的目標值，而是將 $\hat{Q}$  網路當作固定的目標來計算 $Q(s,a,\theta)$ 的目標值，也就是輸出 $y_j=r_j+\gamma \max_a \hat{Q}(s',a,\theta^-)$，其中 $\theta^-$ 表示固定的 $\hat{Q}$ 網路的參數，使用這個技術，可以有效避免目標值的震盪和發散問題。

3. Dueling Network

Dueling Network 是 DQN 的一種改進版本，特別針對值函數估計。傳統的 Q 學習中的 Q 函數是由狀態和行動共同決定的，而 Dueling Network 想要區分開負責估計狀態依賴性較高的網路（Value Network），和負責估計行動優勢的網路（Advantage Network）兩部分，進而提高估值的精度和訓練的穩定性。

Dueling Network 的核心思想是，在神經網路的輸出端，同時學習兩個子網路，一個是狀態價值函數，另一個是行動價值函數，然後再將這兩個部分的值相加，得到最終的 Q 值估計。

具體來說，在 Dueling Network 中，Q 函數的估計可以表示為：

$$Q(s,a; \theta,\alpha,\beta) = V(s;\theta,\beta) + A(s,a;\theta,\alpha) - \frac{1}{|A|} \sum\nolimits_{a'} A(s,a';\theta,\alpha)$$

其中，V(s) 是狀態價值函數 (State-Value Function)，A(s,a) 是行動優勢函數 (Advantage function)，$\alpha$ 和 $\beta$ 分別是 A 和 V 的參數組。因此在 Dueling Network 中，A 和 V 分別組成不同的子網路，如此一來，不需要再像傳統的 Q 函數計算中一樣將狀態和行動的所有特徵全部送入神經網路。

參考文獻：

1. Minh V, Kavukcuoglu K, Silver D, et al. Playing Atari with deep reinforcement learning[J]. arXiv preprint arXiv:1312.5602, 2013.
2. Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning[J]. Nature, 2015, 518(7540): 529-533.
3. Chen R, Li J, Ye Z, et al. Dueling Network Architectures for Deep Reinforcement Learning[J]. Proceedings of the 32nd International Conference on Machine Learning, 2015: 1995-2003.