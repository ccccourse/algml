## 7.5. MADDPG 算法在 Gym 中的應用

Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 是強化學習中的一種算法，用於多智能體系統中。與 DDPG 算法有相似之處，MADDPG 算法同樣基於觀察者，但與 DDPG 不同的是，MADDPG 算法同時學習多個智能體的動作策略和協同策略，以實現深度協同強化學習。MADDPG 算法在解決多智能體系統協作問題上顯示出優異的應用，如協同控制、對抗性協作等。

Gym 是 Python 中一個功能強大的強化學習工具箱，可以用於編寫和測試強化學習模型。Gym 提供了許多環境，這些環境可以用來模擬現實世界中的情境，如遊戲、控制系統等。Gym 提供的隨機環境、基準套件和工具使得強化學習變得更加容易。

在本篇文章中，我們將探討 MADDPG 算法在 Gym 中的應用，具體而言，我們將學習如何使用 MADDPG 算法設計解決多智能體系統問題的模型。

首先，需要安裝相應的套件，包括 OpenAI Gym、maddpg 和 tensorflow。有關於這些套件的資訊可以在其官方網站上找到。

接下來，要使用 Gym 創建一個多個智能體的環境，可以使用如下的程式碼：

```
import gym

env = gym.make("multi_agent_cartpole-v0")
```

這裡使用 "multi_agent_cartpole-v0" 環境，詳細資訊可以參考 Gym 的官方文件。

我們接著定義 MADDPG 算法模型，包括 MADDPPG agent、replay buffer 和 MADDPG 訓練過程。首先，建立 MADDPG agent 即可獲得多個智能體的動作策略。在 MADDPG 算法中，每個智能體都具有獨立的動作策略，這些動作策略是通過 Agent 類中的 neural network 實現的。同樣地，每個智能體都有自己的目標值網絡和行動網絡，以實現離線回放。每個智能體也使用單獨的 MADDPG 訓練過程進行學習。使用下面的 code 存儲 maddpg agent。

```
import tensorflow as tf
import numpy as np

class Agent:
    def __init__(self, name, state_dim, action_dim, others_action_dim):
        self.name = name
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.others_action_dim = others_action_dim
        self.memory = [] # Replay buffer
        self.gamma = 0.95 # 衰減率

        # neural network for actor and critic
        self.actor = self.create_actor_network()
        self.critic = self.create_critic_network()

        # target network
        self.target_actor = self.create_actor_network()
        self.target_critic = self.create_critic_network()
        self.target_actor.set_weights(self.actor.get_weights())
        self.target_critic.set_weights(self.critic.get_weights())
        
        # Optimizer
        self.actor_optimizer = tf.keras.optimizers.Adam(0.001)
        self.critic_optimizer = tf.keras.optimizers.Adam(0.002)
        
    def create_actor_network(self):
        return tf.keras.Sequential([
        tf.keras.layers.Dense(32, activation='relu', input_shape=(self.state_dim+np.sum(self.others_action_dim),),kernel_initializer=tf.keras.initializers.glorot_normal()),
            tf.keras.layers.Dense(self.action_dim, activation='tanh',kernel_initializer=tf.keras.initializers.glorot_normal())
        ])

    def create_critic_network(self):
        state_input = tf.keras.layers.Input((self.state_dim+np.sum(self.others_action_dim),))
        state = tf.keras.layers.Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal())(state_input)
        action_input = tf.keras.layers.Input((self.action_dim,))
        action = tf.keras.layers.Dense(32, activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal())(action_input)
        concat = tf.keras.layers.Concatenate()([state, action])
        concat = tf.keras.layers.Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal())(concat)
        concat = tf.keras.layers.Dense(32, activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal())(concat)
        output = tf.keras.layers.Dense(1, activation='linear',kernel_initializer=tf.keras.initializers.glorot_normal())(concat)
        return tf.keras.Model([state_input, action_input], output)

    def act(self, state, others_action):
        state = np.reshape(state, [1, -1])
        others_action = np.reshape(others_action, [1, -1])
        # Stochastic actions
        actions = self.actor(np.concatenate([state, others_action], axis=-1))
        # Add random noise for exploration
        actions += tf.random.normal(self.action_dim, stddev=0.1)
        actions = tf.clip_by_value(actions, -1, 1)
        return actions[0]

    def store(self, state, others_action, action, reward, state_next, done):
        self.memory.append([state, others_action, action, reward, state_next, done])

    def learn(self, others_agent):
        state_batch = np.array([x[0] for x in self.memory])
        others_action_batch = []
        for i in range(len(others_agent)):
            other = others_agent[i]
            other_actions = np.array([x[2][i] for x in self.memory])
            others_action_batch.append(other_actions)
        others_action_batch = np.concatenate(others_action_batch, axis=-1)
        action_batch = np.array([x[2][self.name] for x in self.memory])
        reward_batch = np.array([x[3] for x in self.memory])
        state_next_batch = np.array([x[4] for x in self.memory])
        done_batch = np.array([x[5] for x in self.memory])

        # Train critic network
        with tf.GradientTape() as tape:
            target_actions = tf.concat([target_actor(states, np.concatenate(others_action_batch, axis=-1)) for states, target_actor in zip(state_next_batch, [other.target_actor for other in others_agent])], axis=-1)
            q_value_next = tf.squeeze(target_critic([state_next_batch, target_actions]), axis=1)
            y = reward_batch + self.gamma * q_value_next * (1 - done_batch)
            critic_value = tf.squeeze(self.critic([state_batch, action_batch]), axis=1)
            critic_loss = tf.keras.losses.MSE(y, critic_value)
        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))

        # Train actor network
        with tf.GradientTape() as tape:
            others_action_batch_clone = others_action_batch.copy()
            for i in range(len(others_agent)):
                others_action_batch_clone[:, i*others_agent[i].action_dim:(i+1)*others_agent[i].action_dim] = tf.stop_gradient(others_agent[i].actor([states for j, states in enumerate(tf.unstack(state_batch, axis=1)) if i != j]))
            actors_input = tf.concat([state_batch, others_action_batch_clone], axis=-1)
            actor_actions = self.actor(actors_input)
            actor_value = -self.critic([state_batch, actor_actions])
            actor_loss = tf.reduce_mean(actor_value)
        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))

        # Update target network
        self.target_actor.set_weights(self.actor.get_weights())
        self.target_critic.set_weights(self.critic.get_weights())

        # Clean memory
        self.memory = []
```

然後，創建 replay buffer，該 buffer 可用於離線學習和分離記憶：

```
class ReplayBuffer:
    def __init__(self, buffer_size=1000000):
        self.buffer_size = buffer_size
        self.buffer = []

    def add(self, state, others_action, action, reward, state_next, done):
        experience = (state, others_action, action, reward, state_next, done)
        if len(self.buffer) >= self.buffer_size:
            self.buffer.pop(0)
        self.buffer.append(experience)

    def sample(self, batch_size):
        batch = []
        if len(self.buffer) < batch_size:
            batch = self.buffer
        else:
            batch = random.sample(self.buffer, batch_size)

        state, others_action, action, reward, state_next, done = zip(*batch)
        return np.array(state), np.array(others_action), np.array(action), np.array(reward), np.array(state_next), np.array(done)
```

最後，定義 MADDPG 訓練過程，即在多個 epoch 中學習動作策略，這裡用每個 epoch 包含 200 步來設置模型訓練過程。

```
def train_agents():
    state_dim = env.observation_space[0].shape[0] # 向量的維度
    action_dim = env.action_space[0].n # 智能體的動作數
    others_dim = []
    for i in range(env.n_agents):
        others_dim.append(env.action_space[i].n)
    agents = []
    for i in range(env.n_agents):
        agent = Agent(i, state_dim, action_dim, others_dim)
        agents.append(agent)
    replay_buffer = ReplayBuffer()

    episodes = 10000 # 訓練次數
    batch_size = 512 # 每次訓練使用的經驗量

    # Train
    done_episode_reward = []
    for episode in range(episodes):
        state = env.reset()
        episode_reward = 0

        for _ in range(200):
            actions = [] # 記錄所有智能體的行動
            for i in range(env.n_agents):
                action = agents[i].act(state[i], [x for j, x in enumerate(actions) if j != i])
                actions.append(action.numpy())
            state_next, reward, done, _ = env.step(actions) # 計算下一步
            episode_reward += np.sum(reward)
            for i in range(env.n_agents):
                agents[i].store(state[i], [x for j, x in enumerate(actions) if j != i], actions[i], reward[i], state_next[i], done[i]) # 存儲經驗
            state = state_next

            if len(replay_buffer.buffer) >= batch_size:
                for i in range(env.n_agents):
                    others_agent = [agents[j] for j in range(env.n_agents) if i != j]
                    agents[i].learn(others_agent) # 智能體學習

            if all(done):
                print("episode: {}, reward: {}".format(episode, episode_reward))
                done_episode_reward.append(episode_reward)
                break

        if len(replay_buffer.buffer) >= batch_size:
            for i in range(env.n_agents):
                others_agent = [agents[j] for j in range(env.n_agents) if i != j]
                agents[i].learn(others_agent) # 智能體學習

    return done_episode_reward
```

最後，執行訓練並視覺化訓練結果，可以使用如下的程式碼：

```
import matplotlib.pyplot as plt

if __name__ == '__main__':
    env = gym.make("multi_agent_cartpole-v0")
    done_episode_reward = train_agents()
    plt.plot(done_episode_reward)
    plt.ylabel('Total reward')
    plt.xlabel('Episode')
    plt.show()
```

通過訓練后，我們可以得到 multi-agent 學習的相應結果，並進一步優化設計的算法以解決更多的多智能體協作問題。