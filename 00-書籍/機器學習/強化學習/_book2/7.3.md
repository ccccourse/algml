## 7.3. Coopetition 和非 Coopetition 環境

## 1. 引言

在強化學習中，最常應用的場景就是單一代理人學習（single-agent learning），也就是傳統意義上的學習模型。代理人在學習過程中，與環境相互作用，通過評估狀態和奬勵（reward）來進行決策。在這種學習模型中，環境是被動的，不會對代理人的行為進行修改。

除了單一代理人學習，強化學習還可以應用於多智能體系統（multi-agent system, MAS）中。MAS 是一種包含多個智能體，每個智能體都可在環境中進行操作，有其專屬的觀測空間、行動空間和策略。在這種學習模型中，環境不再是被動的，代理人之間彼此影響，並且必須在共享的環境中自我協調。因此，代理人學習必須考慮其他代理人的行為對於環境和自身學習的影響，進行策略的調整。

在多智能體系統中，又包含了和合作（Coopetition）相關的環境，以及非和合作環境。本節將對這諸課題進行介紹。

## 2. 和合作（Coopetition）環境

在和合作環境中，不同智能體共享環境，但能夠進行互相協調之後對環境進行操作。在這種環境中，智能體之間的行為必須協調一致，才能得到最好的結果。

使用強化學習模型來處理Coopetition 環境的一個有趣的例子是捕魚和喂食（fishing and feeding）博弈，模型可以通過 Gym 套件實現，其環境場景如下：

- 有一個魚塘，其中有3隻魚。令捕魚者每捕到一隻魚可以獲得1分（奬勵），最高得分為3分。另外，令喂食者每喂一次魚可以增加魚的數量，使之增加到最大值且不在變化時額外獲得5分的奬勵（成就感）。
- 環境的狀態（state）是魚塘中魚的數量和捕魚者是否已經捕到魚。
- 代理人行動（action）空間：捕魚者針對每一隻魚有2種不同行動，要么釣魚（fishing）要么不釣魚（not fishing），而喂食者針對魚塘數量有2種不同行動，要么喂食（feeding），要么不喂食（not feeding）。

**IsingModel** 可以模擬和合作博弈，並使用基於 Q 學習和深度 Q 學習的強化學習算法進行代理人學習。基於 Q 學習（Q-Learning）的算法會在每次成功操作後更新Q值，即獎勵值，而深度 Q 學習通過深度神經網路實現 Q 值的估計，進行學習。

## 3. 非和合作環境

在非和合作環境中，智能體在同一環境中進行互相操作，並且彼此之間存在「鬥爭」的關係。在這種情況下，代理人需要在環境中尋找最優策略，也就是利用策略去限制或干擾其他代理人的策略。和合作環境中，各個代理人的利益是同一的，而現實世界中，代理人之間的利益往往是不同的。因此，非和合作環境是進一步的強化學習研究的重點方向之一。

在非和合作環境中，強化學習模型可以進一步設計出包含多智能體的引入的戰略 (Synergetic Mean-Field) 模型。 該模型可以根據智能體的進攻和防守行為，進行外交策略（diplomacy strategy）。 我們以進攻者和防守者的非零和博弈 (nonzero-sum game) 為例，進行解釋該環境下如何進行學習：

- 進攻者攻擊防守者，獲得奬勵。在攻擊成功時，奬勵為正，失敗時為負。
- 進攻者和防守者的學習目標是不同的。進攻者希望在短時間內獲得盡可能多的奬勵，而防守者則希望抵禦住進攻。因此，進攻者採用冰山理論（The Tip of the Iceberg Theory）策略，他會分散自己的攻擊力，讓防守者不得不要分散自己的防禦，因而忽略進攻者的大招。
- 為避免進攻者和防守者都採用對方策略，會發生膠著局面，模型中存在一個外交策略技術（Diplomacy Strategy Technique），來約束進攻者和防守者的策略選擇。

## 4. 總結

為了能夠更好地使用強化學習算法來解決不同的場景，特別是多智能體場景下面臨的不同的挑戰，研究人員不斷在探索新的算法和模型。本節介紹了兩種不同的情況 -- 和合作環境和非和合作環境，分別對這兩種場景進行了簡單的介紹。 未來，隨著使用場景的不斷擴大，研究人員仍將會探索更多新的技術。