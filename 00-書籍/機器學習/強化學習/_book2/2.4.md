## 2.4. 常用的 Gym 環境

在使用 Gym 套件進行強化學習任務時，我們需要先選擇一個合適的環境（environment），也就是一個滿足特定規範的遊戲或者課題。

Gym 套件提供了各種各樣的環境，包括了經典的遊戲、動作執行、機器人操作和人工智能相關的課題。這些環境都有一些公共的特徵，例如：

- observation：即智能體觀察到的環境狀態，可以是圖片、文本或者是數值。
- action：智能體可以在某些狀態下採取的動作，通常是一些離散的或連續的數值。
- reward：當智能體執行完某些動作後，回饋給智能體的奬勵信號，用於引導智能體學習正確的行為。
- done：一個布爾型的變量，指示當前的環境是否終止，通常是智能體達到了某種終止條件。

在下面，讓我們介紹一些常用的 Gym 環境。

### 2.4.1. CartPole-v0 環境

CartPole-v0 是一個簡單的環境，它的目標是使能夠控制小車的智能體平衡一個棒子。該環境提供了四個觀察值，包括小車的位置、速度以及棒子的角度和角速度，智能體的動作則為向左或者向右推小車。

在 CartPole-v0 環境中，智能體的任務是要讓棒子保持在直立的狀態，即使有些微小的擺動也是可以忍受的。如果智能體不能夠維持這樣的狀態，那麼環境就會終止，智能體的任務也就失敗了。

該環境通常被用作測試一些簡單的強化學習算法的性能，比如隨機選擇動作或者基於替換法的方法。

```python
# 使用 CartPole-v0 環境的範例程式碼
import gym

env = gym.make('CartPole-v0')

for i_episode in range(20):
    observation = env.reset() # 重置環境，獲取初始狀態
    for t in range(100):
        env.render() # 顯示當前環境
        action = env.action_space.sample() # 選擇一個隨機的動作
        observation, reward, done, info = env.step(action) # 执行動作，獲取下一步狀態、回饋、是否終止等信息
        if done:
            print("Episode finished after {} timesteps".format(t+1)) # 如果終止，輸出總步數並結束本輪模拟
            break

env.close() # 關閉環境
```

### 2.4.2. MountainCar-v0 環境

MountainCar-v0 是另一個簡單的環境，它的目標是使能夠控制小車的智能體爬坡。該環境提供了兩個觀察值，分別表示小車的位置和速度，智能體的動作則為向左、向右或者不動。

在 MountainCar-v0 環境中，智能體的任務是要使小車穿過兩個山峰之間的谷底，並最終爬到山頂。由於小車的引擎功率不足，即使智能體一直往右推小車，也不能夠直接爬到山頂。因此，智能體必須要學會合理地使用動作，通過反復試驗學習出最佳策略，才能成功完成任務。

```python
# 使用 MountainCar-v0 環境的範例程式碼
import gym

env = gym.make('MountainCar-v0')

for i_episode in range(20):
    observation = env.reset() # 重置環境，獲取初始狀態
    for t in range(100):
        env.render() # 顯示當前環境
        action = env.action_space.sample() # 選擇一個隨機的動作
        observation, reward, done, info = env.step(action) # 执行動作，獲取下一步狀態、回饋、是否終止等信息
        if done:
            print("Episode finished after {} timesteps".format(t+1)) # 如果終止，輸出總步數並結束本輪模拟
            break

env.close() # 關閉環境
```

### 2.4.3. LunarLander-v2 環境

LunarLander-v2 是一個比較複雜的環境，它的目標是使能夠控制火箭的智能體成功降落在月球表面。該環境提供了八個觀察值，包括火箭的位置、速度、角度，以及火箭左右和上下引擎的開關狀態。智能體的動作則為不使用引擎、向左旋轉、向右旋轉，或者是左右引擎與上下引擎的組合。

在 LunarLander-v2 環境中，智能體面臨的挑戰包括了着陆位置、速度、角度，以及着陆腳底與月球表面的接觸度等多個因素的影響，因此從其着陆，需要智能體掌握較多的技能和策略。

```python
# 使用 LunarLander-v2 環境的範例程式碼
import gym

env = gym.make('LunarLander-v2')

for i_episode in range(20):
    observation = env.reset() # 重置環境，獲取初始狀態
    for t in range(100):
        env.render() # 顯示當前環境
        action = env.action_space.sample() # 選擇一個隨機的動作
        observation, reward, done, info = env.step(action) # 执行动作，獲取下一步狀態、回饋、是否終止等信息
        if done:
            print("Episode finished after {} timesteps".format(t+1)) # 如果終止，輸出總步數並結束本輪模拟
            break

env.close() # 關閉環境
```

上面三个範例程式碼分別展示了在不同環境下的動作選擇流程，并通過 `render` 方法顯示當前智能體的表現。需要注意的是，`render` 方法在某些環境下可能會使智能體的表現變得較差，因此在實際應用中可能需要消除它。

在後續的範例程式碼中，我們將進一步介紹如何使用 Gym 套件來构建更加高效和精確的智能體，并在特定環境中掌握較好的表現。