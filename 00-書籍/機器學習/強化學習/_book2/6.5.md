## 6.5. Actor-Critic 算法在 Gym 中的應用

Actor-Critic 算法是一種結合了策略梯度 (policy gradient) 方法和值函數 (value function) 方法的強化學習算法。在以上的一些討論中，我們已經探討了 Q-learning 和 DQN 等值函數算法以及 REINFORCE 和 A2C 等策略梯度算法。Actor-Critic 算法的主要思路是將這兩種方法合併在一起。可以看作 Actor-Critic 算法是一種帶有評論家（Critic）的策略梯度方法，在學習策略的過程中，同時學習環境的值函數。詳細的原理可以參考《強化學習導論第二版》一書的 13.3 章，這裡不再贅述。

在 Gym 中實現 Actor-Critic 算法的過程和 A2C 算法很相似。主要區別在於訓練中需要使用 Critic 網路估算環境的值函數，用來計算 Advantage 函數，然後再對 Advantage 函數和 Actor 網路的梯度進行更新。具體的步驟如下：

1. 初始化策略網路 Actor 和值函數網路 Critic 以及他們對應的優化器。
2. 當前環境狀態 $s$ 作為初始狀態，初始化 Advantage 函數值為 0。
3. 重複以下流程，直至終止狀態：

    a. Actor 網路根據當前狀態 $s$ 輸出動作機率分佈 $P(a_t|s_t)$。
    
    b. 按照 $P(a_t|s_t)$ 從動作空間中抽取動作 $a_t$。
    
    c. 將 $(s_t, a_t)$ 傳入環境中得到環境反饋 $r_t$ 和下一步狀態 $s_{t+1}$。
    
    d. Critic 網路輸入狀態 $s_t$，輸出環境值函數 $V_t$。
    
    e. 計算 Advantage 函數 $A_t$，$A_t\leftarrow r_t+\gamma V_{t+1}-V_t$。
    
    f. 計算策略和值函數損失 $\mathcal{L}=\mathcal{L}_{policy}+\mathcal{L}_{value}$，其中 $\mathcal{L}_{policy}$ 是由 Advantage 函數計算的策略梯度損失，$\mathcal{L}_{value}$ 是神經網路回歸目標與真實環境回饋之間的均方誤差。
    
    g. 根據 $\mathcal{L}$ 更新 Actor 和 Critic 網路的權重參數。

在實際實現中，可以將 Critic 網路和 Actor 網路設計成同一個神經網路，但有不同的輸出。這樣做可以減少模型的花費，也可以考慮到 ConvNet 在 CNN 中的優良性能。

下面是一個使用出租車問題進行訓練的範例，其中 Critic 網路使用簡單的多層感知機，而 Actor 網路則使用線性策略。


```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Actor 網路
class LinearPolicy(nn.Module):

    def __init__(self, in_dim, out_dim):
        super(LinearPolicy, self).__init__()
        self.fc = nn.Linear(in_dim, out_dim)

    def forward(self, x):
        x = self.fc(x)
        return x

# Critic 網路
class MLPValue(nn.Module):

    def __init__(self, in_dim):
        super(MLPValue, self).__init__()
        self.fc1 = nn.Linear(in_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 超參數設置
env = gym.make('Taxi-v3')
learning_rate = 0.005
gamma = 0.99
entropy_weight = 0.01
max_episodes = 1000
max_steps = 300
render_interval = 50

# 初始化環境信息和神經網路模型
state_dim = env.observation_space.n
action_dim = env.action_space.n
actor_net = LinearPolicy(state_dim, action_dim)
critic_net = MLPValue(state_dim)
actor_optimizer = optim.Adam(actor_net.parameters(), lr=learning_rate)
critic_optimizer = optim.Adam(critic_net.parameters(), lr=learning_rate)

# 訓練過程
for episode in range(max_episodes):
    state = env.reset()
    actor_loss = 0.0
    critic_loss = 0.0
    total_reward = 0.0

    for t in range(max_steps):
        # 根據策略網路選取動作
        action_probs = nn.functional.softmax(actor_net(torch.FloatTensor(np.eye(state_dim)[state])), dim=-1)
        action = torch.multinomial(action_probs, 1).item()
        next_state, reward, done, _ = env.step(action)
        total_reward += reward

        # 計算 Advantage 函數
        with torch.no_grad():
            state_tensor = torch.FloatTensor(np.eye(state_dim)[state])
            next_state_tensor = torch.FloatTensor(np.eye(state_dim)[next_state])
            value = critic_net(state_tensor)
            next_value = critic_net(next_state_tensor) if not done else 0.0
            advantage = reward + gamma * next_value - value
        
        # 計算 Actor 和 Critic 的損失函數
        actor_loss -= torch.log(action_probs[0][action]) * advantage.detach().clone() - entropy_weight * torch.sum(action_probs * torch.log(action_probs))
        critic_loss += advantage ** 2

        # 更新神經網路模型
        actor_optimizer.zero_grad()
        actor_loss.backward()
        actor_optimizer.step()

        critic_optimizer.zero_grad()
        critic_loss.backward()
        critic_optimizer.step()

        # 更新狀態和時間步
        state = next_state
        if done:
            break

    if episode % render_interval == 0:
        print('Episode:', episode + 1, 'Total reward:', total_reward)

env.close()
```