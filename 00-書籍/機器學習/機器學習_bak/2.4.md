## 2.4 決策樹

## 決策樹概述

決策樹是一種監督式學習演算法，用於回歸和分類問題。它樹狀結構，其中每個非葉節點表示一個特徵(feature)，每個分支表示一個特徵的值，每個葉節點表示一個類標籤(class label)。決策樹的目標是通過將資料分成不同的葉類別，使分類的錯誤率最小化。決策樹算法的目標是創建一個模型，可以從資料特徵中得出明確的結論。

## 決策樹建立的過程

決策樹的建立過程可以分為以下幾步：

1. 選擇一個最佳特徵(feature)作為最上層節點。
2. 根據該特徵將資料集分成多個子集(subset)。
3. 對於每個子集，重複步驟1和2，生成新的子樹。
4. 重複執行步驟1-3，直到樹達到照須的深度或所有資料點已被分類。

在上述步驟中，有幾種算法可以用於選擇最佳特徵：

- ID3：計算特徵的資訊增益或信息增益比。
- C4.5：計算特徵的信息增益率。
- CART：計算特徵的基尼不純度。

## 決策樹分類法

決策樹分類法是指利用決策樹來進行分類的過程。以下是建立決策樹的 Python 程式：

```python
from sklearn.tree import DecisionTreeClassifier

# 創建決策樹模型
clf = DecisionTreeClassifier()

# 訓練模型
clf.fit(X_train, y_train)

# 預測
y_pred = clf.predict(X_test)
```

在上述程式中，`X_train`和`y_train`是訓練資料，`X_test`是測試資料，`y_pred`是模型的預測結果。

## 決策樹回歸法

決策樹回歸法是指利用決策樹來進行回歸分析。以下是建立決策樹回歸模型的 Python 程式：

```python
from sklearn.tree import DecisionTreeRegressor

# 創建決策樹模型
regressor = DecisionTreeRegressor()

# 訓練模型
regressor.fit(X_train, y_train)

# 預測
y_pred = regressor.predict(X_test)
```

在上述程式中，`X_train`和`y_train`是訓練資料，`X_test`是測試資料，`y_pred`是模型的預測結果。

## 決策樹的優點

決策樹演算法有以下的優點：

1. 適用範圍廣：決策樹可用於回歸和分類分析，且不受資料類型的限制。
2. 探索性分析：決策樹可用於探索資料，因為可以通過節點來了解數據。
3. 較少的數據預處理：相對於其他算法，決策樹在數據預處理方面的要求比較低。
4. 易於理解：決策樹生成的模型可以很容易地解釋。

## 決策樹的缺點

決策樹演算法有以下的缺點：

1. 過度擬合：決策樹對訓練資料的擬合度很高，使其對新數據的泛化能力較差。
2. 不穩定性：決策樹中一步不當，整個樹甚至整個演算法都可能出現問題。
3. 模型不一致：相同的資料集在不同解釋器訓練下生成的決策樹不一樣。

過度擬合通常是決策樹中最關鍵的問題，因此為了改善此問題，可以通過以下方法：

- 限制數據的特徵列數。
- 擴大數據集。
- 補充多種數據。

## 小結

決策樹演算法是一種常見的機器學習方法，可以用於回歸和分類問題。決策樹的優點在於適用範圍廣，不需要過多數據預處理，易於理解；缺點在於容易過度擬合和不穩定性等。通過限制特徵數量和擴大樣本數量，可以改善決策樹中過度擬合的問題。