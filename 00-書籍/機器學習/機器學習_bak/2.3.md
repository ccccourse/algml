## 2.3 邏輯回歸

## 1. 邏輯回歸介紹

邏輯回歸 (Logistic Regression) 是常用的一種監督式學習演算法，通常用於二元分類問題 (Binary Classification)，即將樣本分成兩類。邏輯回歸主要是基於多元線性回歸及 Sigmoid 函數的組合而來。在此方法中，邏輯回歸會將代表特定特徵的輸入數據與已知且被限制在二元輸出 0 或 1 的目標值之間建立一個關係模型。

舉例而言，邏輯回歸可能會根據過去兩天的股票價格和天氣來預測明天股票的漲跌情況，其中，二元輸出是股票價格的漲跌（1：漲，0：跌）。在訓練模型時，邏輯回歸將會學習到一些權重，並令選定的損失函數最小化。經過多次迭代之後，這些權重最終可以用來預測新的數據，以及查明各個特徵與目標的相對影響力。

## 2. 使用 Scikit-Learn 實現邏輯回歸

在 Scikit-Learn 中，實現邏輯回歸非常簡單。首先需要導入 LogisticRegression 類，接著使用 fit() 方法擬合繫數，再使用 predict() 方法對新數據進行預測。以下是定義、擬合、預測邏輯回歸模型的基本步驟。

### 2.1 導入模塊
首先，我們需要引用 Scikit-Learn 中的 LogisitcRegression 模塊和其它相關模塊，這裡要求，令一些基本設置在程式啟動時呈現在 Jupyter Notebook 中。

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
```

### 2.2 創建虛擬數據
虛擬數據有助於理解邏輯回歸的概念，現在我們使用 Scikit-Learn 創建一些虛擬數據作為範例。

```python
# 創建虛擬數據
np.random.seed(1)           # 隨機種子
X = np.random.randn(100, 2)  # 100 筆 2 維度的數據
y = (X[:, 0] + X[:, 1] > 0).astype(int) # 如果 x+y > 0，y 會被標記為 1
```

這裡創建出 100 筆資料，每筆資料有 2 個特徵，同時也有一個範圍在 0 和 1 之間的標籤。如果 X 特徵的加總大於 0，則標籤將被設定為 1，否則標籤為 0。

### 2.3 擬合模型

現在我們擬合 LogisticRegression 模型。

```python
clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, y)
```

這裡我們使用了 Scikit-Learn 中的 LogisticRegression()，同時使用 fit() 方法擬合了模型。請注意，solver 參數用於指定求解最大似然算法的算法。在 Scikit-Learn 中，可選的算法有 liblinear（基於 coordinate descent、適用於小數據集）、newton-cg（牛頓法）、lbfgs（L-BFGS），以及 sag（隨機平均梯度下降）。

### 2.4 得到最佳的權重參數

模型擬合後，我們可以使用 coef_ 和 intercept_ 屬性來查看最佳權重參數。

```python
print(clf.coef_)
print(clf.intercept_)
```

這與我們預期的一樣，這個模型根據兩個特徵 (X[0] 和 X[1]) 的值來計算輸出，並使用 S 型函數進行轉換。

### 2.5 預測標籤

預測新數據的標籤很簡單，只需要使用 predict() 方法即可。

```python
# 驗證模型
print(clf.predict([[0, 0]]))
print(clf.predict([[0.5, 0.5]]))
```

上述程式碼將會預測出 [0] 和 [1]，這就是這個演算法的結果。根據模型所學到的參數，當 X[0] 和 X[1] 的加總大於一個特定的閾值時，進行標籤的預測。

## Reference

[1] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html