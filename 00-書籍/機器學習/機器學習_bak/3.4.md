## 3.4 DBSCAN

## DBSCAN 簡介

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一個非常強大的密度聚類演算法。與 k-means 和 hierarchical clustering 不同，DBSCAN 不需要預先指定要有多少個 cluster。而且，DBSCAN 可以自動將噪音資料區分出來，因此比起 k-means 和 hierarchical clustering 還要更具實用性。

## DBSCAN 的運作方式

DBSCAN 使用兩個參數：半徑 (eps) 和最小數量 (min_samples)。

![](https://cdn-images-1.medium.com/max/1200/1*HGsTmY5CJxW_nsBoMfxLcw.png)

- 當執行 DBSCAN 時，將選取一個隨機的資料點 (通常是第一個點)，並將其標記為「visited」，接著尋找以該點為中心、半徑為 eps 的區域範圍。如果該範圍內的點數超過 min_samples，則這些點將被標記為同一個 cluster，反之則被歸類為噪音點。

- 如果有其他非噪音類的點尚未被標記為「visited」，則從這些點中選取一個點時，同樣的作法。

- 重複以上作法，直到每一個點都已被標記為某一個 cluster 或噪音。

## DBSCAN 的優點

- 不需要預先知道 cluster 數量，比 k-means 和層次聚類更具靈活性。

- 能夠區分出噪音點。

- 能夠區分出不同密度的 cluster 區域。

- 可以處理 cluster 內的任何形狀，不像 k-means 只能處理簡單的球形。

## DBSCAN 的缺點

- 需要定義兩個參數：eps 和 min_samples。因為沒有一個標準的方法定義這些參數，所以我們不能保證這些參數會對所有資料有效。

- 所有在 eps 範圍內的點都被視為 cluster 內的一部分，所以當密度差異非常大時，效果可能不如預期。

- 效能問題。對於大型資料集，計算所有的距離、尋找目標範圍內的所有點可能很昂貴。

## DBSCAN 的 Python 實作

讓我們使用 sklearn 庫來實作 DBSCAN。

首先載入必要的庫和資料集：


```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

%matplotlib inline

X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
plt.scatter(X[:,0], X[:,1])
plt.show()
```


![png](output_1_0.png)


我們可以看到，資料集包含兩個半月形的 cluster。

現在，我們使用 sklearn 库的 DBSCAN 來執行 clustering。我們需要指定兩個參數：eps 和 min_samples。我們將 eps 設為 0.25 和 min_samples 設為 5。

然後使用 fit_predict() 執行資料的 clustering，並將標籤儲存在 y_pred 裡。


```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.25, min_samples=5)

y_pred = dbscan.fit_predict(X)
```


```python
colors = ['r', 'g', 'b', 'k', 'c', 'm', 'y']

for i in set(y_pred):
    if i == -1:
        # 默認情況下，需要將噪音標為 -1
        plt.scatter(X[y_pred == i, 0], X[y_pred == i, 1], s=20, c='k')
    else:
        plt.scatter(X[y_pred == i, 0], X[y_pred == i, 1], s=20, c=colors[i % len(colors)])

plt.show()
```


![png](output_4_0.png)


可以看到，由於 DBSCAN 自動分離噪音，所有噪音點都被標記為黑色。結果表明，DBSCAN 確實成功地分離了兩個半月型的 cluster。

## DBSCAN 的調參

在大多數情況下，參數的選擇可能是 DBSCAN 的最難懂的部分。讓我們看看如何每次選擇 eps 和 min_samples。

- eps 的選擇

eps 定義了最大半徑，當將其賦予太小的值時，算法無法標識 cluster，並可能將所有點都視為噪音。相反，如果將其設置為太大，所有點都將被視為同一個 cluster。

DBSCAN 的 eps 的值通常是選擇的方法是繪製距離圖像，也稱為 k-distance graph。

```python
from sklearn.neighbors import NearestNeighbors
import numpy as np

neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(X)
distances, indices = nbrs.kneighbors(X)

distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)

plt.show()
```


![png](output_6_0.png)


- min_samples 的選擇

min_samples 的選擇取決於資料集本身。更大的 min_samples 值使得演算法更具保守性，因為 clusters 需要幾個較緊密相鄰點，才能被標識為有效的 cluster。相反，賦予較小的值將創建更不穩定的 clusters。

## 結論

DBSCAN 是一種非常有用的 clustering 演算法，具有以下優點：

- 不需要預先知道 cluster 數量，比 k-means 和 hierarchical clustering 更具靈活性。

- 能夠區分出噪音點。

- 能夠區分出不同密度的 cluster 區域。

- 可以處理 cluster 內的任何形狀，不像 k-means 只能處理簡單的球形。

但是，它還有缺點：

- 需要定義兩個參數：eps 和 min_samples。沒有最佳的方法定義這些參數，因此我們不能保證這些參數對所有資料都有效。

- 所有在 eps 範圍內的點都被視為 cluster 內的一部分，所以當密度差異非常大時，效果可能不如預期。

- 效能問題。對於大型資料集，計算所有的距離、尋找目標範圍內的所有點可能很昂貴。

但它在大多數情況下，仍然是一個非常有效的 clustering 演算法。