## 5.2 學習曲線

## 簡介
在監督式學習中，學習曲線會顯示出如何隨著訓練樣本數量的增加，模型的性能在發生怎樣的變化。從學習曲線，我們可以估計訓練樣本數量和模型效能的關係，並從而判斷是否需要更多的訓練樣本。在實際應用中，通常都會利用學習曲線來幫助我們選擇合適的模型參數，以及評估模型是否存在過擬合或者欠擬合。

## 範例
在 scikit-learn 中，我們可以通過`learning_curve`函式來繪製學習曲線。

首先，我們需要準備一些訓練數據。在本例中，我們將使用經典的 iris 數據集，該數據集包含 3 種不同品種的 iris 花，每種花各有 50 個樣本，每個樣本包含 4 個特徵 (花萼長度、花萼寬度、花瓣長度、花瓣寬度)。

```python
# 載入所需的套件
from sklearn.datasets import load_iris
from sklearn.model_selection import learning_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import ShuffleSplit
import numpy as np
import matplotlib.pyplot as plt

# 加載 iris 數據集
iris = load_iris()
X, y = iris.data, iris.target

# 定義繪製學習曲線的函式
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    """
    Parameters:
      estimator : 分類器或回歸器
      title : 字符串，圖形的標題
      X : 樣本數據
      y : 樣例的標籤
      ylim : 元組，(ymin, ymax)，設置圖形中縱軸的最小值和最大值
      cv : 整數，決定交叉驗證步驟數
      n_jobs : int 或 None，設置用於執行並行作業的 CPU 數量
      train_sizes : 數組，用於定義用於生成學習曲線的樣本數量
    """
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt
```

在上述程序中，我們定義了一個用於繪製學習曲線的函式。該函式有很多參數，其中比較重要的几個參數如下：

- `estimator`：模型估計器，可以是分類器或回歸器。
- `title`：繪製的圖形的標題。
- `X`：樣本數據。
- `y`：樣本標籤。
- `ylim`：用於定義上下限標度。
- `cv`：交叉驗證數。
- `n_jobs`：CPU 執行並行作業的數目，None 表示全部 CPU。
- `train_sizes`：用於定義用於生成學習曲線的樣本數量。

接著，我們可以使用`plot_learning_curve`函式來繪製學習曲線。

```python
title = "Learning Curves (Decision Tree)"
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
estimator = DecisionTreeClassifier()
plot_learning_curve(estimator, title, X, y, cv=cv, n_jobs=4)
plt.show()
```

在上述程式中，我們首先定義了分類器的名稱 (Decision Tree)。並利用 ShuffleSplit 做出 100 次的交叉驗證，其中每次取出 20% 的樣本作為驗證集。接著我們初始化一個 DecisionTreeClassifier 做為分類器。最後，呼叫 plot_learning_curve 函式，完成學習曲線的繪製。當執行完以上程式後，我們可以得到以下的學習曲線：

![image.png](https://i.imgur.com/057jyFM.png)

從上述學習曲線中，我們可以得到如下結論：

- 隨著訓練樣本數目的增加，模型的訓練得分和驗證得分都在下降，這表明模型的泛化能力隨著樣本數目增加而增強。
- 隨著訓練樣本數目的增加，訓練得分和驗證得分之間的差距在縮小，這表明過擬合的風險在減小，即這個模型過去偏向於過擬合(舉例: Decision Tree 的深度設太大其實容易過擬合)，而隨著樣本數目增加，過擬合的風險逐漸减少。

在實際應用中，我們通常還會繪製一些其他圖形，比如模型超參數的學習曲線。這樣可以幫助我們選擇最優的超參數設置。