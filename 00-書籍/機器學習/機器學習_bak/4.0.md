## 第四章 模型優化

# 簡介

在進行機器學習模型訓練時，我們的目標是找到一個可以將 data 中的 pattern 學習最好的模型，並讓它在新的 data 上表現也越好越好的模型。但是在現實中，訓練出來的模型常常無法百分之百地符合我們對模型的要求，因此在模型訓練後我們需要對模型進行優化，進一步提高模型在新的 data 上的泛化能力。

模型優化可以分為兩種：模型選擇和超參數優化。模型選擇是指選擇不同算法之間的最優解，包括選擇不同的機器學習模型或特徵工程方法等。超參數優化是指通過嘗試不同的超參數組合來找到模型最佳的性能，其中的超參數是需要在模型訓練之前設定的參數。

本文將會介紹幾個常用的模型優化技術，包括交叉驗證（Cross-Validation）、網格搜尋（Grid Search）、隨機搜尋（Random Search）以及貝氏優化（Bayesian Optimization）。同時，我們將使用 scikit-learn 進行範例演示。

# 交叉驗證（Cross-Validation）

在模型訓練的過程中，我們經常會使用交叉驗證來評估模型在新的 data 上的泛化能力。交叉驗證可以分為多種，其中最常使用的是 K 折交叉驗證。K 折交叉驗證可以將 data 按照一定的比例分成 K 個互不相交的子集，其中一個子集用於驗證，其餘子集用於訓練。這樣重複進行 K 次，每次使用不同的子集作為驗證集，並將 K 次驗證集的結果取平均，最終得到模型在新 data 上的泛化能力的評價。

scikit-learn 中提供了 KFold、StratifiedKFold 和 Leave-One-Out 等交叉驗證的實現方法。其中 KFold 是最基本的交叉驗證方法，StratifiedKFold 則保證每個子集中不同類別的樣本在每個子集中的比例和整個數據集中的比例相同，因此在分類問題上的使用更加廣泛。Leave-One-Out 則是將每個樣本單獨拿出來做驗證。

下面是一個以 K 折交叉驗證為例的演示：

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, KFold

# 載入 iris 類別數據
data = load_iris()
X, y = data.data, data.target

# 使用 logistic regression 進行分類
clf = LogisticRegression(solver='liblinear')

# 使用 KFold 對 data 進行 K 折交叉驗證
cv = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(clf, X, y, cv=cv)

# 顯示交叉驗證的平均精度
print('Average score:', scores.mean())
```

# 網格搜尋（Grid Search）

在進行模型訓練時，我們經常需要調整模型的超參數以獲得更好的結果。但是不同的超參數組合可能會導致不同的性能表現，因此需要對多組超參數進行嘗試。網格搜尋就是一種用於 hyperparameter tuning 的方法，通過在一個參數網格上進行搜索來找到最佳的超參數組合。

scikit-learn 中的 GridSearchCV 類實現了網格搜尋的功能，通過將要調整的超參數以字典的形式傳入 Estimator，指定要調整的超參數範圍和步長，就可以得到一組最佳超參數組合。

下面是一個以 `LogisticRegression` 為例的使用網格搜尋進行超參數優化的演示：

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# 創建一個二元分類數據集
X, y = make_classification(n_samples=1000, n_classes=2, random_state=42)

# 創建模型
clf = LogisticRegression()

# 定義要優化的超參數範圍
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'C': [0.1, 1.0, 10.0],
    'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],
    'fit_intercept': [True, False]
}

# 使用網格搜尋進行超參數優化
grid_search = GridSearchCV(clf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)
grid_search.fit(X, y)

# 輸出最佳超參數組合
print('Best hyperparameters:', grid_search.best_params_)
```

# 隨機搜尋（Random Search）

隨機搜尋是另一個用於超參數優化的方法，它跟網格搜尋相似，不同的是它是在超參數範圍內以隨機方式針對每個超參數取樣。由於隨機搜尋的隨機性，不同的隨機搜尋采樣可能會得到不同的超參數組合，因此需要多次進行隨機搜尋，取平均作為模型的最佳超參數組合。

與 GridSearchCV 方法類似，scikit-learn 也提供了 RandomizedSearchCV 類實現了隨機搜尋的功能。

下面是一個以 `LogisticRegression` 為例的使用隨機搜尋進行超參數優化的演示：

```python
from scipy.stats import uniform
from sklearn.datasets import make_classification
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression

# 創建一個二元分類數據集
X, y = make_classification(n_samples=1000, n_classes=2, random_state=42)

# 創建模型
clf = LogisticRegression()

# 定義要優化的超參數分布
param_dist = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'C': uniform(loc=0, scale=4),
    'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],
    'fit_intercept': [True, False]
}

# 使用隨機搜尋進行超參數優化
random_search = RandomizedSearchCV(clf, param_distributions=param_dist, cv=3, n_jobs=-1, verbose=1)
random_search.fit(X, y)

# 輸出最佳超參數組合
print('Best hyperparameters:', random_search.best_params_)
```

# 貝氏優化（Bayesian Optimization）

貝氏優化是一種通過建立超參數的概率模型，根據不斷優化目標函數來更新模型參數的方法。在優化過程中，貝氏優化可以根據不同的超參數組合估計目標函數的值，並在每次迭代時根據已知點的機率分布，預測下一個要試驗的超參數組合。

在 scikit-learn 中，可以通過使用 `skopt` 库實現貝氏優化。透過 SciPy 中的 `minimize` 函数實現目標函數的最小化/最大化，`GaussianProcessRegressor` 函數得出目標函數的 高斯過程模型，然後再使用其他方法來選擇下一個目標超參數。

下面是一個以 `LogisticRegression` 為例的使用貝氏優化進行超參數優化的演示：

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from skopt import gp_minimize
from skopt.space import Real, Categorical, Integer

# 創建一個二元分類數據集
X, y = make_classification(n_samples=1000, n_classes=2, random_state=42)

# 定義要優化的超參數範圍和分布
search_space = [
    Categorical(['l1', 'l2', 'elasticnet', 'none'], name='penalty'),
    Real(0.1, 10.0, name='C'),
    Categorical(['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'], name='solver'),
    Categorical([True, False], name='fit_intercept')
]

# 定義目標函數
def objective(params):
    clf = LogisticRegression(penalty=params[0],
                             C=params[1],
                             solver=params[2],
                             fit_intercept=params[3],
                             random_state=42)
    score = cross_val_score(clf, X, y, cv=3).mean()
    return -score

# 使用 GP-minimize 進行超參數優化
res = gp_minimize(objective, search_space, n_calls=20, random_state=42)

# 輸出最佳超參數組合
print('Best hyperparameters:', res.x)
```

從上面這個例子中可以看出貝氏優化與前面的方法有一些不同，其中最主要的是對超參數範圍的定義，貝氏優化通常需要將超參數範圍轉換為實數空間，便於進行高斯過程建模和搜尋。而最後的結果則是依據針對不同超參數組合的交叉驗證分數獲得的。