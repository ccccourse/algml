### **實現前向傳播與誤差反向傳播**

在神經網路的訓練過程中，前向傳播和誤差反向傳播是核心步驟。前向傳播用於計算神經網路的輸出，而反向傳播則用於計算梯度並更新模型的參數。

以下我們將通過一個簡單的例子來演示如何在 PyTorch 中實現前向傳播與誤差反向傳播。

### **步驟 1：定義模型**

在這個例子中，我們將定義一個簡單的多層感知器（MLP）模型，包含兩個隱藏層和一個輸出層。我們將使用 PyTorch 來實現模型結構。

```python
import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        
        # 定義各層
        self.hidden1 = nn.Linear(input_size, hidden_size)
        self.hidden2 = nn.Linear(hidden_size, hidden_size)
        self.output = nn.Linear(hidden_size, output_size)
        
        # 定義激勵函數
        self.relu = nn.ReLU()

    def forward(self, x):
        # 前向傳播
        x = self.relu(self.hidden1(x))  # 經過第一層
        x = self.relu(self.hidden2(x))  # 經過第二層
        x = self.output(x)  # 經過輸出層
        return x
```

### **步驟 2：定義損失函數和優化器**

在神經網絡訓練中，我們需要選擇損失函數和優化器。在這裡，我們使用均方誤差（MSE）損失函數和隨機梯度下降（SGD）優化器。

```python
# 創建模型實例
model = MLP(input_size=784, hidden_size=128, output_size=10)

# 定義損失函數
criterion = nn.MSELoss()

# 定義優化器
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

### **步驟 3：前向傳播**

在每一個訓練迴圈中，我們首先通過模型進行前向傳播來計算輸出。這一步是計算模型的預測值。

```python
# 假設有一個隨機生成的輸入數據
input_data = torch.randn(1, 784)  # 單個樣本，784 維

# 前向傳播
output_data = model(input_data)
```

這樣，`output_data` 就是模型的預測結果。

### **步驟 4：計算損失**

一旦得到了模型的輸出，我們就可以計算預測值和真實標籤之間的誤差。這裡我們假設真實標籤是 `target_data`，並且使用均方誤差來計算損失。

```python
# 假設有一個隨機生成的目標數據
target_data = torch.randn(1, 10)  # 目標大小是 10，對應於 10 類

# 計算損失
loss = criterion(output_data, target_data)
print(f"Loss: {loss.item()}")
```

### **步驟 5：反向傳播**

在損失計算完成後，我們需要執行反向傳播來計算梯度。這是通過 `loss.backward()` 完成的。

```python
# 清除先前的梯度
optimizer.zero_grad()

# 反向傳播，計算梯度
loss.backward()
```

這樣，所有可訓練的參數（例如權重和偏置）的梯度都將被計算出來。

### **步驟 6：更新參數**

反向傳播計算完畢後，我們使用優化器來更新模型的參數。這一步是梯度下降的一部分，通過 `optimizer.step()` 執行。

```python
# 使用優化器更新參數
optimizer.step()
```

### **步驟 7：重複訓練過程**

通常，我們會將這些步驟放在一個迴圈中，並在每次迭代中進行前向傳播、損失計算、反向傳播和參數更新。以下是一個簡單的訓練迴圈的範例：

```python
# 訓練迴圈
for epoch in range(100):  # 假設訓練 100 個 epoch
    # 假設每次迭代有一個隨機的訓練數據和目標
    input_data = torch.randn(1, 784)  # 隨機生成輸入數據
    target_data = torch.randn(1, 10)  # 隨機生成目標數據

    # 前向傳播
    output_data = model(input_data)
    
    # 計算損失
    loss = criterion(output_data, target_data)

    # 清除先前的梯度
    optimizer.zero_grad()

    # 反向傳播
    loss.backward()

    # 更新參數
    optimizer.step()

    # 每 10 個 epoch 輸出一次損失
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}")
```

### **總結**

- **前向傳播**：將輸入數據傳遞通過網絡，計算預測結果。
- **誤差反向傳播**：通過損失函數計算誤差，並將誤差反向傳播以計算梯度。
- **參數更新**：使用優化器根據計算得到的梯度更新網絡的參數。

這些步驟是訓練神經網絡的基礎。PyTorch 提供了方便的 API 來進行這些操作，使得構建和訓練神經網絡變得更加直觀和高效。