### **前向傳播與反向傳播數學推導**

在多層感知器（MLP）中，前向傳播與反向傳播是訓練神經網絡的核心步驟。前向傳播用來計算網絡的輸出，反向傳播則用來計算損失函數相對於網絡參數（權重和偏置）的梯度，並進行參數更新。

#### **1. 前向傳播（Forward Propagation）**

前向傳播是將輸入數據從輸入層傳遞至輸出層的過程。在這一過程中，輸入經過每一層的神經元進行加權求和，並應用激勵函數。

##### **數學推導**

假設網絡有兩層：
- 輸入層有 \( n \) 個神經元，表示為向量 \( \mathbf{x} = [x_1, x_2, \dots, x_n]^T \)。
- 第一隱藏層有 \( m \) 個神經元，第二隱藏層有 \( p \) 個神經元，輸出層有 \( q \) 個神經元。

對於每一層的操作，我們進行以下推導：

1. **第一層（輸入層到隱藏層）**：
   - 假設輸入層到隱藏層的權重矩陣是 \( W_1 \in \mathbb{R}^{m \times n} \)，偏置是 \( \mathbf{b_1} \in \mathbb{R}^m \)。
   - 每個隱藏層神經元的加權總和 \( z_1 \) 可以表示為：
     \[
     \mathbf{z_1} = W_1 \mathbf{x} + \mathbf{b_1}
     \]
   - 隨後，我們將加權和應用激勵函數（如 ReLU、Sigmoid 或 Tanh），得到隱藏層的輸出 \( \mathbf{a_1} \)：
     \[
     \mathbf{a_1} = f(\mathbf{z_1})
     \]
     其中，\( f \) 是激勵函數。

2. **第二層（隱藏層到輸出層）**：
   - 假設從隱藏層到輸出層的權重矩陣是 \( W_2 \in \mathbb{R}^{q \times m} \)，偏置是 \( \mathbf{b_2} \in \mathbb{R}^q \)。
   - 輸出層的加權總和 \( z_2 \) 可以表示為：
     \[
     \mathbf{z_2} = W_2 \mathbf{a_1} + \mathbf{b_2}
     \]
   - 輸出層的最終輸出 \( \mathbf{y} \) 是激勵函數 \( f_2 \)（通常在回歸問題中為線性激勵，分類問題中為 Softmax 或 Sigmoid）應用於加權總和 \( \mathbf{z_2} \)：
     \[
     \mathbf{y} = f_2(\mathbf{z_2})
     \]

這就是前向傳播的數學推導，從輸入層經過每一層的計算，最終得到模型的預測輸出。

#### **2. 反向傳播（Backward Propagation）**

反向傳播的目的是通過計算損失函數相對於網絡參數（權重和偏置）的梯度，來進行梯度下降優化。反向傳播的核心是鏈式法則，它可以計算出每一層的梯度。

##### **數學推導**

1. **計算損失函數的梯度**：
   假設損失函數為 \( L \)，它是網絡輸出 \( \mathbf{y} \) 和真實標籤 \( \mathbf{y_{\text{true}}} \) 之間的誤差，通常使用均方誤差（MSE）或交叉熵損失。對損失函數相對於輸出層的加權總和 \( \mathbf{z_2} \) 求梯度：
   \[
   \frac{\partial L}{\partial \mathbf{z_2}} = \frac{\partial L}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{z_2}}
   \]
   其中：
   - \( \frac{\partial L}{\partial \mathbf{y}} \) 是損失函數對輸出層的導數，取決於損失函數和激勵函數。
   - \( \frac{\partial \mathbf{y}}{\partial \mathbf{z_2}} \) 是激勵函數的導數。

2. **計算隱藏層的梯度**：
   然後，我們通過鏈式法則來計算隱藏層的梯度。對損失函數相對於隱藏層加權總和 \( \mathbf{z_1} \) 求梯度：
   \[
   \frac{\partial L}{\partial \mathbf{z_1}} = \frac{\partial L}{\partial \mathbf{a_1}} \cdot \frac{\partial \mathbf{a_1}}{\partial \mathbf{z_1}}
   \]
   其中：
   - \( \frac{\partial L}{\partial \mathbf{a_1}} \) 是損失函數對隱藏層輸出 \( \mathbf{a_1} \) 的導數。
   - \( \frac{\partial \mathbf{a_1}}{\partial \mathbf{z_1}} \) 是激勵函數 \( f(\mathbf{z_1}) \) 的導數。

3. **計算梯度並更新權重與偏置**：
   - 對於每一層，計算權重和偏置的梯度：
     \[
     \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial \mathbf{z_2}} \cdot \mathbf{a_1}^T
     \]
     \[
     \frac{\partial L}{\partial \mathbf{b_2}} = \frac{\partial L}{\partial \mathbf{z_2}}
     \]
     \[
     \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{z_1}} \cdot \mathbf{x}^T
     \]
     \[
     \frac{\partial L}{\partial \mathbf{b_1}} = \frac{\partial L}{\partial \mathbf{z_1}}
     \]
   - 然後使用梯度下降法更新權重和偏置：
     \[
     W_1 = W_1 - \eta \frac{\partial L}{\partial W_1}
     \]
     \[
     W_2 = W_2 - \eta \frac{\partial L}{\partial W_2}
     \]
     \[
     \mathbf{b_1} = \mathbf{b_1} - \eta \frac{\partial L}{\partial \mathbf{b_1}}
     \]
     \[
     \mathbf{b_2} = \mathbf{b_2} - \eta \frac{\partial L}{\partial \mathbf{b_2}}
     \]
     其中，\( \eta \) 是學習率。

---

### **總結**

- **前向傳播**：將輸入數據通過每層的權重和偏置，並通過激勵函數，最終得到網絡的預測結果。
- **反向傳播**：根據損失函數計算每層的梯度，並更新權重和偏置，從而使得網絡能夠學習到更合適的參數。這一過程利用了鏈式法則來計算每層的梯度。

前向傳播和反向傳播的數學推導是神經網絡訓練的核心，理解這些推導對於理解深度學習的運作方式至關重要。