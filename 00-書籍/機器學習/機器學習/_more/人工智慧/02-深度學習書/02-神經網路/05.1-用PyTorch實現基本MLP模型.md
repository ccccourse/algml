### **用 PyTorch 實現基本 MLP 模型**

在這一部分，我們將展示如何使用 PyTorch 實現一個基本的多層感知器（MLP）模型。這個模型包含三個層：輸入層、隱藏層和輸出層。我們將會使用 PyTorch 的 `nn.Module` 類來構建這個模型，並展示如何訓練這個模型。

### **步驟 1：導入必要的庫**

首先，我們需要導入 PyTorch 以及其他必要的庫。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
```

### **步驟 2：定義 MLP 模型**

在 PyTorch 中，通常通過繼承 `nn.Module` 類來創建自己的神經網絡模型。我們定義一個包含兩個隱藏層的 MLP 模型。這個模型將接受一個特定形狀的輸入，然後經過兩層隱藏層和一個輸出層。

```python
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        
        # 定義兩層隱藏層
        self.hidden1 = nn.Linear(input_size, hidden_size)
        self.hidden2 = nn.Linear(hidden_size, hidden_size)
        
        # 定義輸出層
        self.output = nn.Linear(hidden_size, output_size)
        
        # 定義激勵函數（此處使用 ReLU）
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # 前向傳播：依次通過隱藏層和激勵函數
        x = self.relu(self.hidden1(x))
        x = self.relu(self.hidden2(x))
        
        # 最後通過輸出層
        x = self.output(x)
        
        return x
```

### **步驟 3：創建模型實例**

現在，我們可以創建一個 MLP 模型的實例，並指定輸入大小、隱藏層大小和輸出大小。

```python
# 假設輸入大小為 784（例如 28x28 像素的圖片），隱藏層大小為 128，輸出大小為 10（例如 10 類分類）
model = MLP(input_size=784, hidden_size=128, output_size=10)
```

### **步驟 4：定義損失函數和優化器**

為了訓練模型，我們需要定義一個損失函數和一個優化器。在這裡，我們選擇交叉熵損失函數，並使用 SGD 優化器來更新模型的參數。

```python
# 使用交叉熵損失函數
criterion = nn.CrossEntropyLoss()

# 使用 SGD 優化器，學習率設定為 0.01
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

### **步驟 5：創建數據集並加載數據**

假設我們有一些訓練數據，並將其封裝為 PyTorch 的 `TensorDataset` 和 `DataLoader`。這樣，我們就能夠批量處理數據。

```python
# 假設訓練數據是隨機生成的
X_train = torch.randn(1000, 784)  # 1000 個 28x28 的圖像，展平為 784 維向量
y_train = torch.randint(0, 10, (1000,))  # 1000 個標籤，範圍為 0 到 9（10 類）

# 創建 TensorDataset 和 DataLoader
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
```

### **步驟 6：訓練模型**

現在，我們可以開始訓練模型。在每個訓練迭代中，我們需要執行以下步驟：
1. **前向傳播**：將輸入數據傳遞給模型以獲得預測。
2. **計算損失**：根據預測值和真實標籤計算損失。
3. **反向傳播**：根據損失計算梯度。
4. **更新參數**：使用優化器來更新模型的權重。

```python
num_epochs = 10  # 訓練 10 輪

for epoch in range(num_epochs):
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        # 清空梯度
        optimizer.zero_grad()
        
        # 前向傳播
        outputs = model(inputs)
        
        # 計算損失
        loss = criterion(outputs, labels)
        
        # 反向傳播
        loss.backward()
        
        # 更新參數
        optimizer.step()
        
        running_loss += loss.item()
    
    # 輸出每一輪的損失
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')
```

### **步驟 7：評估模型**

在訓練過程結束後，我們通常會在驗證集或測試集上評估模型的性能。這裡簡單演示如何對模型進行評估。

```python
# 假設我們有測試數據
X_test = torch.randn(200, 784)
y_test = torch.randint(0, 10, (200,))

# 計算模型的預測
outputs = model(X_test)

# 計算預測結果與真實標籤的準確度
_, predicted = torch.max(outputs, 1)
accuracy = (predicted == y_test).sum().item() / y_test.size(0)
print(f'Test Accuracy: {accuracy * 100:.2f}%')
```

### **總結**

這個基本的 MLP 模型展示了如何用 PyTorch 定義一個簡單的神經網絡。它包含了兩個隱藏層，並使用 ReLU 激勵函數進行非線性變換。訓練過程中使用了交叉熵損失函數來處理分類任務，並通過 SGD 優化器來更新模型的參數。