### **激勵函數的數學分析（ReLU、Sigmoid、Tanh）**

激勵函數是神經網絡中的核心組件，它決定了每一層神經元的輸出。不同的激勵函數具有不同的數學性質和特徵，適用於不同的場景。以下是對常見激勵函數（ReLU、Sigmoid、Tanh）的數學分析。

---

#### **1. ReLU（Rectified Linear Unit）**

**數學表達式**：
\[
f(x) = \max(0, x)
\]

**圖像與性質**：
- ReLU 函數是一個分段線性函數，當輸入 \( x \) 大於零時，輸出與輸入相等；當輸入小於或等於零時，輸出為零。
- 這意味著，ReLU 函數能夠將負值“截斷”，僅保留正值。
- ReLU 的圖像是一條通過原點的線性區間，並且在 \( x < 0 \) 時是水平線。

**優點**：
- **計算效率高**：ReLU 計算非常簡單，只有比較操作。
- **減少梯度消失問題**：由於 ReLU 在 \( x > 0 \) 時的梯度為 1，因此可以有效避免在反向傳播過程中的梯度消失問題。
- **稀疏激活**：ReLU 會將一部分神經元的輸出置為零，這使得神經網絡中大部分神經元的激活值為零，形成“稀疏激活”，這有助於提高計算效率。

**缺點**：
- **死神經元問題**：當輸入始終為負數時，ReLU 的梯度為零，這會導致神經元無法更新，形成“死神經元”問題。

**導數**：
\[
f'(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
\]
這表示 ReLU 在正區間的導數是 1，而在負區間是 0。

---

#### **2. Sigmoid**

**數學表達式**：
\[
f(x) = \frac{1}{1 + e^{-x}}
\]

**圖像與性質**：
- Sigmoid 函數是一個 S 形曲線，其輸出範圍在 (0, 1) 之間。
- 當 \( x \to +\infty \) 時，\( f(x) \to 1 \)，當 \( x \to -\infty \) 時，\( f(x) \to 0 \)。
- Sigmoid 函數的圖像是一條平滑的 S 形曲線，通常用於將輸出映射到 (0, 1) 之間，這使得它非常適合於概率預測問題（如二分類問題）。

**優點**：
- **平滑性**：Sigmoid 是一個平滑的函數，這使得它在優化過程中有較好的性質。
- **概率解釋**：Sigmoid 的輸出範圍 (0, 1) 非常適合作為概率值。

**缺點**：
- **梯度消失問題**：在 \( x \to +\infty \) 或 \( x \to -\infty \) 時，Sigmoid 函數的梯度接近於零，這可能會導致梯度消失問題，從而使得網絡訓練變慢。
- **非零中心**：Sigmoid 的輸出範圍是 (0, 1)，這會使得輸入信號的均值偏向於正數，影響神經網絡的訓練效率。

**導數**：
\[
f'(x) = f(x) \cdot (1 - f(x))
\]
這意味著 Sigmoid 的導數可以用其輸出值來表示，並且具有平滑性。

---

#### **3. Tanh（Hyperbolic Tangent）**

**數學表達式**：
\[
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2 \cdot \sigma(2x) - 1
\]
其中 \( \sigma(x) \) 是 Sigmoid 函數。

**圖像與性質**：
- Tanh 函數是一個 S 形曲線，類似於 Sigmoid，但其輸出範圍在 (-1, 1) 之間。
- 當 \( x \to +\infty \) 時，\( f(x) \to 1 \)，當 \( x \to -\infty \) 時，\( f(x) \to -1 \)。
- Tanh 函數在原點附近具有較強的斜率，因此能夠更快地響應較小的變化。

**優點**：
- **梯度較大**：Tanh 在 \( x \) 附近具有較大的梯度，這使得其在訓練初期的學習速度較快。
- **零均值**：Tanh 的輸出範圍是 (-1, 1)，這意味著其輸出的均值接近零，這有助於加速訓練過程。

**缺點**：
- **梯度消失問題**：與 Sigmoid 類似，Tanh 在 \( x \to +\infty \) 或 \( x \to -\infty \) 時的梯度會接近零，因此也會遇到梯度消失問題。
- **計算成本較高**：相比 Sigmoid，Tanh 的計算相對複雜，因此在計算上有一定的開銷。

**導數**：
\[
f'(x) = 1 - \tanh^2(x)
\]
這表示 Tanh 的導數在輸出接近 -1 或 1 時會變得非常小，因此在訓練中也會遇到梯度消失的問題。

---

### **總結**

- **ReLU**：適用於大部分深度學習任務，特別是在隱藏層中，但會面臨死神經元問題。計算簡單且能夠有效避免梯度消失問題。
- **Sigmoid**：適用於二分類問題，具有概率解釋，但可能會遇到梯度消失問題，且輸出範圍非零中心，影響訓練效率。
- **Tanh**：相比 Sigmoid，有較大的梯度且輸出零中心，通常在隱藏層中使用，但也會遇到梯度消失問題。

選擇合適的激勵函數對於神經網絡的性能和訓練過程至關重要，通常根據具體問題的特徵來選擇激勵函數。