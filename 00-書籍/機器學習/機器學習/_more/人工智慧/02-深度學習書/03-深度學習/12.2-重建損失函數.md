### **12.2 自編碼器的數學背景**
#### **重建損失函數**

在自編碼器（Autoencoder）的訓練過程中，目標是將輸入數據 \( \mathbf{x} \) 通過編碼器轉換為隱藏層表示 \( \mathbf{z} \)，然後通過解碼器重建出輸出 \( \hat{\mathbf{x}} \)。訓練自編碼器的目標是最小化重建誤差，即使得重建的輸出 \( \hat{\mathbf{x}} \) 與原始輸入 \( \mathbf{x} \) 之間的差異盡可能小。

為了衡量重建誤差，通常使用 **重建損失函數**（Reconstruction Loss Function）。這個損失函數可以根據具體的任務和數據選擇不同的形式。最常見的損失函數有 **均方誤差（MSE）** 和 **交叉熵損失**。這些損失函數都衡量了重建數據與原始數據之間的差異。

---

### **1. 均方誤差（MSE, Mean Squared Error）**

均方誤差是自編碼器中最常用的重建損失函數，尤其適用於連續數據。MSE 衡量的是原始數據 \( \mathbf{x} \) 和重建數據 \( \hat{\mathbf{x}} \) 之間的平方差。數學表達式如下：

\[
L_{\text{MSE}}(\mathbf{x}, \hat{\mathbf{x}}) = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2
\]

其中：
- \( \mathbf{x}_i \) 是第 \( i \) 個樣本的真實輸入數據。
- \( \hat{\mathbf{x}}_i \) 是第 \( i \) 個樣本的重建輸出。
- \( N \) 是樣本的數量。

均方誤差的意圖是最小化每一對原始數據和重建數據之間的差異。當 MSE 較小時，表示重建數據與原始數據非常相似。

---

### **2. 交叉熵損失（Cross-Entropy Loss）**

交叉熵損失常用於處理分類問題，當輸入和重建是二元或多類標籤時，它比均方誤差更有效。交叉熵衡量的是兩個概率分佈之間的差異，通常用於處理像是圖像生成這樣的問題，其中輸入和輸出可能是二進制圖像或類別標籤。

對於二元分類問題，交叉熵損失的數學形式為：

\[
L_{\text{CE}}(\mathbf{x}, \hat{\mathbf{x}}) = - \sum_{i=1}^{N} \left( x_i \log(\hat{x}_i) + (1 - x_i) \log(1 - \hat{x}_i) \right)
\]

其中：
- \( x_i \) 是第 \( i \) 個樣本的真實標籤。
- \( \hat{x}_i \) 是第 \( i \) 個樣本的重建標籤。

交叉熵損失在二元或多類標籤問題中表現出色，它通過將真實值和預測值之間的概率分佈差異進行最小化來促使模型學會更準確的重建。

---

### **3. 其他重建損失函數**

除了均方誤差和交叉熵損失之外，還有一些其他的重建損失函數可根據具體的應用進行選擇，像是：
- **Huber Loss**：在處理含有離群值的數據時，Huber Loss 可以作為比 MSE 更穩健的選擇。
- **KL 散度（Kullback-Leibler Divergence）**：如果自編碼器是變分自編碼器（VAE），則常會使用 KL 散度來衡量編碼分佈和標準正態分佈之間的差異。

---

### **4. 重建損失函數的最小化**

訓練自編碼器的目標是通過最小化重建損失函數來更新模型的參數。這通常通過梯度下降法或其他優化方法進行。對於均方誤差損失，優化過程通常會使用梯度下降來最小化損失：

\[
\mathbf{\theta}_e, \mathbf{\theta}_d = \arg \min_{\mathbf{\theta}_e, \mathbf{\theta}_d} L(\mathbf{x}, \hat{\mathbf{x}})
\]

這樣，編碼器和解碼器的參數會根據重建誤差進行更新，最終學到能夠有效表示和重建數據的映射。

---

### **總結**

重建損失函數在自編碼器中扮演著關鍵角色，衡量了原始數據與重建數據之間的誤差。根據數據的特徵和應用需求，可以選擇不同的損失函數（如均方誤差、交叉熵等）來進行最小化，從而訓練自編碼器模型學會高效的數據表示和重建。