### **RNN的數學背景**

**遞歸神經網絡（Recurrent Neural Network, RNN）** 是一類專門處理序列數據的神經網絡結構，與常規的前向神經網絡不同，RNN 能夠考慮序列中元素之間的時間依賴性。它在時間步（Time Step）上進行遞歸運算，使得前一時刻的輸出成為下一時刻的輸入，這樣就能夠捕捉到序列數據中的時間順序信息。

### **1. RNN 結構與遞歸運算**

在RNN中，神經網絡的每個時間步的輸出不僅取決於當前的輸入，還與前一時刻的隱藏狀態（Hidden State）有關。具體來說，RNN的運算是遞歸的，它的結構可以被描述為以下數學公式：

#### **RNN 結構**
- **輸入**：\( x_t \) 是當前時刻的輸入（在第 \( t \) 時刻的序列元素）。
- **隱藏狀態**：\( h_t \) 是當前時刻的隱藏狀態，它反映了網絡記住的序列的過去信息。
- **輸出**：\( y_t \) 是當前時刻的預測結果（即模型的輸出）。
- **權重矩陣**：\( W_{xh}, W_{hh}, W_{hy} \) 是權重矩陣，分別用來控制輸入到隱藏狀態、隱藏狀態到隱藏狀態、以及隱藏狀態到輸出的轉換。

RNN的基本遞歸公式如下：

#### **隱藏狀態更新公式：**

\[
h_t = \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
\]

其中：
- \( \sigma \) 是激勵函數，常用的激勵函數包括 tanh 或 ReLU。
- \( W_{xh} \) 是將當前輸入 \( x_t \) 映射到隱藏狀態空間的權重矩陣。
- \( W_{hh} \) 是將前一時刻隱藏狀態 \( h_{t-1} \) 映射到當前隱藏狀態的權重矩陣。
- \( b_h \) 是隱藏狀態的偏置項。

#### **輸出計算公式：**

\[
y_t = W_{hy} h_t + b_y
\]

其中：
- \( W_{hy} \) 是將隱藏狀態映射到輸出空間的權重矩陣。
- \( b_y \) 是輸出的偏置項。

這些公式表明，每一個時間步的隱藏狀態 \( h_t \) 不僅由當前的輸入 \( x_t \) 決定，還受到前一時刻隱藏狀態 \( h_{t-1} \) 的影響，這使得 RNN 能夠記住序列中的上下文信息。

### **2. RNN的遞歸特性**

RNN的核心思想在於其 **遞歸性**，即隱藏狀態 \( h_t \) 會依賴於前一時刻的隱藏狀態 \( h_{t-1} \)，從而將序列中的時間依賴關係進行“傳遞”。這種傳遞過程讓RNN能夠對序列中的長期依賴關係進行建模。

從數學上看，RNN 的遞歸結構可以展開為：

\[
h_t = \sigma(W_{xh} x_t + W_{hh} (\sigma(W_{xh} x_{t-1} + W_{hh} (\sigma(\dots)))))
\]

這樣的遞歸結構在每一層中都會將前一時間步的信息與當前時間步的輸入結合，產生當前的隱藏狀態。然而，這樣的設計也帶來了 **梯度消失** 和 **梯度爆炸** 的問題，這使得RNN在處理長序列時會遇到困難。

### **3. 梯度消失與爆炸問題**

在訓練RNN時，梯度會隨著時間步的推移進行反向傳播。由於遞歸結構的特性，反向傳播的梯度會通過權重矩陣 \( W_{hh} \) 在每一層中進行乘法操作。這樣如果 \( W_{hh} \) 的數值過大，會使得梯度變得非常大，造成 **梯度爆炸**；相反，如果 \( W_{hh} \) 的數值過小，會使得梯度逐漸減小，最終造成 **梯度消失**。

#### **梯度消失的數學描述：**

假設我們在反向傳播中計算梯度時，對於時間步 \( t \)，梯度的更新公式如下：

\[
\frac{\partial L}{\partial W_{xh}} = \sum_{t=1}^T \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W_{xh}}
\]

由於每一層的隱藏狀態 \( h_t \) 都會受到前一層隱藏狀態 \( h_{t-1} \) 的影響，因此梯度計算的過程中，會涉及到多次的矩陣乘法。如果權重 \( W_{hh} \) 的元素較小（例如小於1），那麼梯度會在每一層中被“壓縮”或減小，最終導致梯度消失，使得模型難以學習長期依賴關係。

### **4. 解決梯度消失與爆炸的技術**

為了解決RNN中存在的梯度消失與爆炸問題，後來出現了許多變種結構，如 **長短期記憶網絡（LSTM）** 和 **門控循環單元（GRU）**，它們通過引入不同類型的門控機制來控制梯度的流動，從而有效解決了這些問題。

### **總結**

RNN的遞歸結構使其能夠捕捉序列數據中的時間依賴關係。它通過隱藏狀態的更新公式將當前的輸入與前一時刻的隱藏狀態進行結合。然而，RNN在處理長序列時會遇到梯度消失和爆炸問題，這使得它在長期依賴學習上存在局限性。為了解決這些問題，LSTM和GRU等變種網絡被提出來克服這些挑戰。