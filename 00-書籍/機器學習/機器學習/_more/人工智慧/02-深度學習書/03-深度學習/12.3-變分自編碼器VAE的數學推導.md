### **12.3 變分自編碼器（VAE）的數學推導**

變分自編碼器（Variational Autoencoder, VAE）是一種基於概率模型的生成模型，它擴展了傳統自編碼器的概念，引入了隱變量（latent variables）和變分推斷。VAE 目的是從數據中學習一個潛在的變量模型，使得能夠從潛在變量中生成新的樣本。其數學推導主要圍繞如何進行變分推斷，並最小化變分下界（Variational Lower Bound）。

---

### **1. 生成模型與目標**

VAE的核心思想是將數據視為由潛在變量 \( \mathbf{z} \) 通過某個生成過程 \( p(\mathbf{x} | \mathbf{z}) \) 生成的。給定一個觀察數據 \( \mathbf{x} \)，我們的目標是推斷潛在變量 \( \mathbf{z} \)，以及學習生成 \( \mathbf{x} \) 的條件概率分佈。

最初的生成模型可以表示為：

\[
p(\mathbf{x}, \mathbf{z}) = p(\mathbf{z}) p(\mathbf{x} | \mathbf{z})
\]

其中：
- \( p(\mathbf{z}) \) 是潛在變量的先驗分佈，通常假設是標準正態分佈 \( p(\mathbf{z}) = \mathcal{N}(\mathbf{z} | \mathbf{0}, \mathbf{I}) \)。
- \( p(\mathbf{x} | \mathbf{z}) \) 是生成模型的條件分佈，表示在給定潛在變量 \( \mathbf{z} \) 的情況下生成觀察數據 \( \mathbf{x} \) 的概率。

VAE的目標是最大化觀察數據的邊際對數似然：

\[
\log p(\mathbf{x}) = \log \int p(\mathbf{x}, \mathbf{z}) d\mathbf{z}
\]

由於積分不可解析，直接最大化似然變得不可行，因此需要引入變分推斷。

---

### **2. 變分推斷**

為了進行有效的推斷，我們引入了一個變分分佈 \( q(\mathbf{z} | \mathbf{x}) \)，用來近似後驗分佈 \( p(\mathbf{z} | \mathbf{x}) \)。根據變分推斷的基本原理，我們希望最小化後驗分佈 \( p(\mathbf{z} | \mathbf{x}) \) 和變分分佈 \( q(\mathbf{z} | \mathbf{x}) \) 之間的 Kullback-Leibler 散度 \( D_{\text{KL}} \)，從而找到一個好的近似分佈。

通過變分推斷，我們將邊際對數似然的最大化轉換為最小化變分下界（Variational Lower Bound, ELBO）：

\[
\log p(\mathbf{x}) \geq \mathbb{E}_{q(\mathbf{z} | \mathbf{x})} \left[ \log p(\mathbf{x} | \mathbf{z}) \right] - D_{\text{KL}} \left( q(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) \right)
\]

這個不等式的右側即為變分下界（ELBO），由兩個部分組成：
1. **重建誤差**（log likelihood term）：這部分衡量的是如何從潛在變量 \( \mathbf{z} \) 重建觀察數據 \( \mathbf{x} \)，即 \( \mathbb{E}_{q(\mathbf{z} | \mathbf{x})} \left[ \log p(\mathbf{x} | \mathbf{z}) \right] \)。
2. **KL 散度**（regularization term）：這部分測量變分分佈 \( q(\mathbf{z} | \mathbf{x}) \) 和先驗分佈 \( p(\mathbf{z}) \) 之間的差異，即 \( D_{\text{KL}} \left( q(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) \right) \)。

---

### **3. 變分自編碼器的具體實現**

在VAE中，變分分佈 \( q(\mathbf{z} | \mathbf{x}) \) 通常被設計為高斯分佈，其均值和方差由神經網絡進行參數化。具體而言，給定一個觀察數據 \( \mathbf{x} \)，編碼器（Encoder）會輸出潛在變量 \( \mathbf{z} \) 的均值 \( \mu(\mathbf{x}) \) 和標準差 \( \sigma(\mathbf{x}) \)，即：

\[
q(\mathbf{z} | \mathbf{x}) = \mathcal{N}(\mathbf{z} | \mu(\mathbf{x}), \sigma^2(\mathbf{x}))
\]

解碼器（Decoder）則會根據 \( \mathbf{z} \) 重建觀察數據 \( \mathbf{x} \)，即：

\[
p(\mathbf{x} | \mathbf{z}) \approx \hat{p}(\mathbf{x} | \mathbf{z})
\]

因此，VAE的訓練目標變為最小化以下損失函數：

\[
L_{\text{VAE}} = \mathbb{E}_{q(\mathbf{z} | \mathbf{x})} \left[ \log p(\mathbf{x} | \mathbf{z}) \right] - D_{\text{KL}} \left( q(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) \right)
\]

這個損失函數包含了重建誤差和KL散度，訓練過程會同時最小化兩者。

---

### **4. 變分自編碼器的訓練**

VAE的訓練通常使用變分推斷和梯度下降進行，具體步驟如下：
1. **編碼器**：將觀察數據 \( \mathbf{x} \) 映射到潛在變量 \( \mathbf{z} \) 的分佈 \( q(\mathbf{z} | \mathbf{x}) \)，並抽樣得到潛在變量 \( \mathbf{z} \)。
2. **重建**：將抽樣得到的 \( \mathbf{z} \) 輸入解碼器 \( p(\mathbf{x} | \mathbf{z}) \)，生成重建的數據 \( \hat{\mathbf{x}} \)。
3. **計算損失**：計算重建誤差和KL散度，並將它們加總得到VAE的損失。
4. **反向傳播**：根據損失對模型參數進行反向傳播，並使用梯度下降進行優化。

---

### **總結**

變分自編碼器（VAE）通過引入潛在變量和變分推斷，使得生成模型的訓練變得可行。它的數學推導關鍵在於最大化變分下界（ELBO），這一過程同時最小化了重建誤差和KL散度，從而學會一個有效的潛在變量表示，並能夠生成新的樣本。VAE的成功之處在於它能夠將生成模型的訓練轉化為可微分的優化問題，使得可以使用梯度下降等優化方法進行訓練。