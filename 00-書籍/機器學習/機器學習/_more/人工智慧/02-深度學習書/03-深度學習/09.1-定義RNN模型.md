### **9. RNN與LSTM的PyTorch實現**

在PyTorch中，實現RNN、LSTM和GRU非常簡單，因為PyTorch提供了內建的模組`nn.RNN`、`nn.LSTM`和`nn.GRU`來定義這些結構。這些模組都可以處理批量（batch）數據，並且具有許多可調參數來適應不同的需求。

下面我們將介紹如何使用這些模組來定義基本的RNN、LSTM和GRU架構。

---

### **1. 使用 `nn.RNN` 定義 RNN 架構**

`nn.RNN`是PyTorch中用來實現基本RNN結構的模組。它的主要輸入是序列數據，並會返回隱藏狀態（hidden states）和最終的隱藏狀態。

#### **RNN 示例**

```python
import torch
import torch.nn as nn

# 定義RNN模型
class RNNModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(RNNModel, self).__init__()
        # 定義RNN層
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        # 定義全連接層（線性層）來預測輸出
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # RNN的輸出為（output, hidden_state）
        out, _ = self.rnn(x)
        # 我們通常使用最後一個時間步的隱藏狀態來做預測
        out = self.fc(out[:, -1, :])
        return out

# 假設輸入數據的特徵數量是10，隱藏層大小是20，層數是2，輸出數量是1
model = RNNModel(input_size=10, hidden_size=20, num_layers=2, output_size=1)
print(model)
```

#### **關鍵參數：**
- `input_size`: 每個時間步的輸入特徵數量。
- `hidden_size`: 隱藏層的神經元數量。
- `num_layers`: RNN層的數量（可以疊加多層RNN）。
- `batch_first`: 是否將輸入的第一維度設為batch size。
- `output_size`: 最終的輸出大小，通常用於預測。

---

### **2. 使用 `nn.LSTM` 定義 LSTM 架構**

LSTM是RNN的改進版本，用於處理長期依賴問題。`nn.LSTM`模組具有與`nn.RNN`相似的用法，但它支持更複雜的內部結構。

#### **LSTM 示例**

```python
import torch
import torch.nn as nn

# 定義LSTM模型
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        # 定義LSTM層
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        # 定義全連接層（線性層）來預測輸出
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # LSTM的輸出為（output, (hidden_state, cell_state)）
        out, _ = self.lstm(x)
        # 使用最後一個時間步的隱藏狀態來進行預測
        out = self.fc(out[:, -1, :])
        return out

# 假設輸入數據的特徵數量是10，隱藏層大小是20，層數是2，輸出數量是1
model = LSTMModel(input_size=10, hidden_size=20, num_layers=2, output_size=1)
print(model)
```

#### **關鍵參數：**
- `input_size`: 每個時間步的輸入特徵數量。
- `hidden_size`: 隱藏層的神經元數量。
- `num_layers`: LSTM層的數量。
- `batch_first`: 是否將輸入的第一維度設為batch size。
- `output_size`: 最終的輸出大小，通常用於預測。

---

### **3. 使用 `nn.GRU` 定義 GRU 架構**

GRU（門控循環單元）是另一種變體，與LSTM相比結構較為簡單，並且可以有效地減少計算量。它與LSTM非常相似，但它只有兩個門控：更新門和重置門。

#### **GRU 示例**

```python
import torch
import torch.nn as nn

# 定義GRU模型
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(GRUModel, self).__init__()
        # 定義GRU層
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        # 定義全連接層（線性層）來預測輸出
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # GRU的輸出為（output, hidden_state）
        out, _ = self.gru(x)
        # 使用最後一個時間步的隱藏狀態來進行預測
        out = self.fc(out[:, -1, :])
        return out

# 假設輸入數據的特徵數量是10，隱藏層大小是20，層數是2，輸出數量是1
model = GRUModel(input_size=10, hidden_size=20, num_layers=2, output_size=1)
print(model)
```

#### **關鍵參數：**
- `input_size`: 每個時間步的輸入特徵數量。
- `hidden_size`: 隱藏層的神經元數量。
- `num_layers`: GRU層的數量。
- `batch_first`: 是否將輸入的第一維度設為batch size。
- `output_size`: 最終的輸出大小，通常用於預測。

---

### **總結**

- `nn.RNN`、`nn.LSTM` 和 `nn.GRU` 模組都能夠有效地處理序列數據，並且它們具有相似的接口。
- `nn.LSTM` 和 `nn.GRU` 提供了比標準RNN更好的性能，特別是在處理長期依賴問題時。
- 在實現中，所有這些模型都返回兩個主要的結果：
  1. `output`：包含每個時間步的隱藏狀態的張量。
  2. `hidden_state` 或 `cell_state`：對應於每個時間步的隱藏狀態或LSTM的內部狀態（cell state）。
- 使用這些模型的時候，通常會選擇最後一個時間步的隱藏狀態來進行預測。

這些基本的模型結構可以用來處理各種序列預測任務，如語音識別、語言模型和時間序列預測等。