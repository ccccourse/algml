### **12.1 自編碼器的數學背景**
#### **編碼與解碼過程**

自編碼器（Autoencoder, AE）是一種無監督學習算法，它由兩個主要組件組成：編碼器（Encoder）和解碼器（Decoder）。它的目標是將輸入數據進行有效的壓縮與重建，通過最小化輸入和重建之間的誤差來學習數據的低維表示。

自編碼器的運作過程可以分為兩個主要步驟：

1. **編碼（Encoding）：** 編碼器將高維度的輸入數據映射到較低維度的隱藏層表示（也叫做編碼或潛在變量）。這個隱藏層表示捕捉了輸入數據的主要特徵。

2. **解碼（Decoding）：** 解碼器將低維度的隱藏表示映射回原來的高維度空間，目標是重建出與原始輸入數據相似的輸出。

---

### **編碼過程**

設 \( \mathbf{x} \) 為輸入數據，並且假設輸入數據有 \( d \) 維，則編碼器的目標是將 \( \mathbf{x} \) 映射到一個較低維度的隱藏表示 \( \mathbf{z} \)。這個過程可以通過一個非線性映射來完成，通常是通過一個神經網絡來實現。

在數學上，編碼過程可以表示為：
\[
\mathbf{z} = f(\mathbf{x}; \mathbf{\theta}_e)
\]
其中：
- \( \mathbf{z} \in \mathbb{R}^m \)，表示編碼後的隱藏表示，這是低維的表示。
- \( f(\mathbf{x}; \mathbf{\theta}_e) \) 是編碼器的映射函數，這個函數通常是由神經網絡實現，並由參數 \( \mathbf{\theta}_e \) 定義。

例如，若編碼器使用一個全連接神經網絡來進行映射，則 \( \mathbf{z} \) 可以表示為：
\[
\mathbf{z} = \sigma(\mathbf{W}_e \mathbf{x} + \mathbf{b}_e)
\]
其中：
- \( \sigma \) 是激勵函數（如 ReLU 或 Sigmoid）。
- \( \mathbf{W}_e \) 和 \( \mathbf{b}_e \) 是編碼器神經網絡的權重和偏差。

---

### **解碼過程**

解碼器的目標是將編碼器的輸出 \( \mathbf{z} \) 映射回原始數據空間 \( \mathbf{x} \)，並且盡量讓重建的數據與原始數據 \( \mathbf{x} \) 相似。這一過程也是非線性的，通常由另一個神經網絡來實現。

數學上，解碼過程可以表示為：
\[
\hat{\mathbf{x}} = g(\mathbf{z}; \mathbf{\theta}_d)
\]
其中：
- \( \hat{\mathbf{x}} \) 是重建的輸出，應該與原始輸入數據 \( \mathbf{x} \) 盡量相似。
- \( g(\mathbf{z}; \mathbf{\theta}_d) \) 是解碼器的映射函數，由參數 \( \mathbf{\theta}_d \) 定義。

假如解碼器也是由一個全連接神經網絡實現的，則 \( \hat{\mathbf{x}} \) 可以表示為：
\[
\hat{\mathbf{x}} = \sigma(\mathbf{W}_d \mathbf{z} + \mathbf{b}_d)
\]
其中：
- \( \mathbf{W}_d \) 和 \( \mathbf{b}_d \) 是解碼器神經網絡的權重和偏差。

---

### **自編碼器的訓練目標**

自編碼器的訓練目標是最小化輸入和重建之間的誤差，這通常通過最小化某種損失函數來實現。最常用的損失函數是均方誤差（MSE），它衡量輸入和重建之間的差異：

\[
L(\mathbf{x}, \hat{\mathbf{x}}) = \frac{1}{N} \sum_{i=1}^{N} \| \mathbf{x}_i - \hat{\mathbf{x}}_i \|^2
\]
其中：
- \( \mathbf{x}_i \) 是第 \( i \) 個樣本的真實輸入。
- \( \hat{\mathbf{x}}_i \) 是第 \( i \) 個樣本的重建輸出。

訓練過程中，我們會通過梯度下降法最小化這個損失函數，調整編碼器和解碼器的參數 \( \mathbf{\theta}_e \) 和 \( \mathbf{\theta}_d \)。

---

### **總結**

自編碼器的編碼過程將高維的輸入數據映射到低維的隱藏表示，而解碼過程則將低維的表示重新映射回高維的輸出。最終目標是學習到一個有效的數據表示，並能夠在不丟失關鍵信息的情況下重建輸入數據。這種方法通常用於降維、數據壓縮、特徵學習等任務。