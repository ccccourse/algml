### **LSTM 與 GRU 的數學推導**

在處理長期依賴問題時，傳統的RNN（遞歸神經網絡）容易遇到梯度消失和梯度爆炸問題，這使得學習長期依賴變得困難。為了解決這個問題，LSTM（長短期記憶網絡）和GRU（門控循環單元）被提出，這兩種網絡結構通過引入門控機制來更有效地捕捉長期依賴。

### **1. LSTM (Long Short-Term Memory)**

LSTM的核心特徵是它的**記憶單元（cell state）**，它能夠選擇性地存儲、遺忘和更新信息。LSTM的計算過程包括四個主要部分：遺忘門（forget gate）、輸入門（input gate）、輸出門（output gate）和記憶單元（cell state）。下面是LSTM的數學推導。

#### **LSTM的計算公式**

LSTM的每個時間步都有以下四個主要操作：

1. **遺忘門（Forget Gate）**：
   遺忘門決定了有多少先前的記憶單元（cell state）被遺忘。遺忘門的輸出是介於0和1之間的數字，表示應該保留多少過去的信息。

   \[
   f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
   \]

   其中，\( f_t \) 是遺忘門的輸出，\( W_f \) 是權重矩陣，\( b_f \) 是偏置，\( \sigma \) 是sigmoid激活函數，\( h_{t-1} \) 是上一時間步的隱藏狀態，\( x_t \) 是當前時間步的輸入。

2. **輸入門（Input Gate）**：
   輸入門決定了當前的輸入如何影響更新的記憶單元。它包含兩部分：
   - 新的候選記憶單元（candidate cell state）：由tanh激活函數產生。
   - 輸入門的開關，表示多少信息應該被加入到記憶單元中。

   \[
   i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
   \]
   \[
   \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
   \]

   其中，\( i_t \) 是輸入門的輸出，\( \tilde{C}_t \) 是候選記憶單元，並且 \( W_i \)、\( W_C \) 是權重矩陣。

3. **記憶單元（Cell State）更新**：
   記憶單元會結合上一時間步的記憶和當前時間步的更新。它首先由遺忘門決定保留多少過去的記憶，然後由輸入門決定要加入多少新的信息。

   \[
   C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
   \]

   其中，\( C_t \) 是當前時間步的記憶單元，\( C_{t-1} \) 是上一時間步的記憶單元。

4. **輸出門（Output Gate）**：
   輸出門決定了從記憶單元中提取多少信息來生成當前時間步的隱藏狀態（即網絡的輸出）。

   \[
   o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
   \]
   \[
   h_t = o_t \cdot \tanh(C_t)
   \]

   其中，\( o_t \) 是輸出門的輸出，\( h_t \) 是當前時間步的隱藏狀態，並且 \( W_o \) 是權重矩陣。

#### **LSTM簡要總結：**

- **遺忘門**控制保留多少過去的信息。
- **輸入門**決定當前輸入對記憶單元的影響。
- **記憶單元**儲存了長期依賴的關鍵信息。
- **輸出門**控制從記憶單元中提取信息。

### **2. GRU (Gated Recurrent Unit)**

GRU是LSTM的一個簡化版本，並且在很多應用中表現得與LSTM一樣有效。GRU去除了LSTM中的“記憶單元”並合併了遺忘門和輸入門。GRU有兩個主要的門控結構：更新門（update gate）和重置門（reset gate）。

#### **GRU的計算公式**

1. **重置門（Reset Gate）**：
   重置門決定了之前隱藏狀態有多少部分應該被丟棄，這樣可以決定多少先前的信息對當前的輸出有貢獻。

   \[
   r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
   \]

   其中，\( r_t \) 是重置門的輸出，\( W_r \) 是權重矩陣，\( b_r \) 是偏置，\( \sigma \) 是sigmoid激活函數。

2. **更新門（Update Gate）**：
   更新門決定了多少新狀態應該被保留，以及多少舊狀態應該被保留。在GRU中，這一門控決定了新信息和舊信息的混合。

   \[
   z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
   \]

   其中，\( z_t \) 是更新門的輸出。

3. **隱藏狀態的更新（Hidden State Update）**：
   隱藏狀態的更新過程相對簡單。它是基於上一時間步的隱藏狀態和當前時間步的候選隱藏狀態的加權平均。

   - 計算候選隱藏狀態：

     \[
   \tilde{h}_t = \tanh(W_h \cdot [r_t \cdot h_{t-1}, x_t] + b_h)
   \]

   - 計算最終的隱藏狀態：

     \[
   h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t
   \]

   其中，\( h_t \) 是當前時間步的隱藏狀態，\( \tilde{h}_t \) 是候選隱藏狀態，\( z_t \) 是更新門。

#### **GRU簡要總結：**

- **重置門**決定丟棄多少先前隱藏狀態的信息。
- **更新門**控制保留多少新狀態和舊狀態。
- **隱藏狀態**是上一時間步的隱藏狀態與候選隱藏狀態的加權平均。

### **LSTM與GRU的區別**

- **LSTM**擁有更多的門控結構（遺忘門、輸入門、輸出門），這使得它對長期依賴問題的建模能力更強，但也增加了模型的複雜性。
- **GRU**將LSTM中的遺忘門和輸入門合併為一個更新門，並且不使用獨立的記憶單元，使得它的結構較簡單，計算上也更高效。

總的來說，LSTM和GRU都是為了解決RNN中梯度消失和梯度爆炸問題而設計的，並且在許多自然語言處理和時間序列預測任務中取得了優異的表現。