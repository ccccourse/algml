### **13.1 用 `nn.Linear` 層構建自編碼器**

自編碼器（Autoencoder）是一種無監督學習模型，用於學習數據的低維表示。其結構主要由兩部分組成：
1. **編碼器**（Encoder）：將輸入數據映射到潛在空間（低維表示）。
2. **解碼器**（Decoder）：將潛在空間的表示映射回原始數據空間，實現數據的重建。

在 PyTorch 中，我們可以利用 `nn.Linear` 層來構建自編碼器，這是一種簡單的全連接層，適用於將數據從高維映射到低維，再從低維映射回高維。

以下是用 `nn.Linear` 層構建自編碼器的具體步驟：

---

### **1. 自編碼器模型結構**

自編碼器通常由以下幾層組成：
- **編碼器**：將輸入 \( \mathbf{x} \) 映射到一個低維表示 \( \mathbf{z} \)。
- **解碼器**：將低維表示 \( \mathbf{z} \) 重建回原始輸入 \( \mathbf{x} \)。

假設我們的輸入數據維度為 \( D_{\text{in}} \)，潛在空間的維度為 \( D_{\text{latent}} \)，那麼自編碼器的結構可以簡單表示為：

- 編碼器：\( \mathbf{z} = \text{Encoder}(\mathbf{x}) \)
- 解碼器：\( \hat{\mathbf{x}} = \text{Decoder}(\mathbf{z}) \)

其中，編碼器和解碼器分別由 `nn.Linear` 層實現。

---

### **2. PyTorch 實現**

首先，導入所需的 PyTorch 模塊：

```python
import torch
import torch.nn as nn
import torch.optim as optim
```

然後，我們定義自編碼器模型：

```python
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()
        
        # 編碼器（Encoder）
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),    # 將輸入數據映射到 128 維
            nn.ReLU(),                    # 激活函數
            nn.Linear(128, latent_dim)    # 將 128 維映射到潛在空間
        )
        
        # 解碼器（Decoder）
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),   # 將潛在空間映射回 128 維
            nn.ReLU(),                    # 激活函數
            nn.Linear(128, input_dim),    # 重建輸入數據，輸出為原始維度
            nn.Sigmoid()                  # 輸出範圍在 [0, 1] 之間，適合二元數據
        )

    def forward(self, x):
        z = self.encoder(x)      # 通過編碼器獲得潛在變量
        reconstructed_x = self.decoder(z)  # 通過解碼器重建輸入數據
        return reconstructed_x
```

在這個模型中：
- 編碼器的第一層 `nn.Linear(input_dim, 128)` 將輸入數據映射到128維的空間，接著使用 ReLU 激活函數進行非線性變換。
- 然後，再通過 `nn.Linear(128, latent_dim)` 將數據映射到潛在空間，維度為 `latent_dim`。
- 解碼器的結構則是將潛在空間的表示映射回原始維度，首先將其映射到128維，然後再映射回 `input_dim`，並使用 Sigmoid 激活函數來確保輸出在合理的範圍內。

---

### **3. 訓練模型**

訓練自編碼器的過程主要包括定義損失函數、優化器以及訓練循環。以下是訓練自編碼器的基本流程：

#### 定義損失函數與優化器：

```python
# 損失函數：均方誤差（MSE），這是自編碼器最常用的損失函數
criterion = nn.MSELoss()

# 優化器：使用 Adam 優化器
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

#### 訓練過程：

```python
# 訓練數據集（假設有 X_train）
# X_train: [batch_size, input_dim]，這是訓練樣本
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()  # 梯度清零
    
    # 前向傳播
    output = model(X_train)  # 模型預測
    loss = criterion(output, X_train)  # 計算重建誤差（損失函數）
    
    # 反向傳播與優化
    loss.backward()  # 計算梯度
    optimizer.step()  # 更新權重
    
    # 輸出訓練過程中的損失
    if (epoch + 1) % 5 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

---

### **4. 測試與應用**

訓練完成後，可以使用自編碼器進行測試。將測試數據輸入編碼器，得到潛在變量，再由解碼器重建數據。

```python
# 假設有測試數據 X_test
model.eval()  # 設置模型為評估模式
with torch.no_grad():  # 減少計算圖的建立，節省內存
    reconstructed = model(X_test)  # 重建數據
```

---

### **總結**

通過以上步驟，我們可以利用 PyTorch 中的 `nn.Linear` 層構建一個簡單的自編碼器。自編碼器的目的是學習數據的低維表示，並通過這個表示進行數據重建。訓練過程中，我們使用均方誤差（MSE）作為損失函數，並使用 Adam 優化器來更新模型的參數。這種簡單的結構可以應用於多種數據降維、數據壓縮、特徵學習等任務。