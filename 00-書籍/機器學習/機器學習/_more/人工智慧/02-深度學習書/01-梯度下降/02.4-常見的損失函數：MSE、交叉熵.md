### **常見的損失函數：均方誤差 (MSE) 和 交叉熵 (Cross-Entropy)**

損失函數（Loss Function）是機器學習和深度學習中用來衡量模型預測結果與真實結果之間差異的函數。不同的任務和模型會使用不同的損失函數。以下是兩個最常見的損失函數——**均方誤差 (MSE)** 和 **交叉熵 (Cross-Entropy)**。

---

### **1. 均方誤差 (Mean Squared Error, MSE)**

**均方誤差 (MSE)** 是回歸問題中最常用的損失函數之一。它衡量的是預測值與實際值之間的平均平方差。MSE 的目標是最小化預測值與真實值之間的誤差，通常用於處理連續型變數的預測問題。

#### **公式**：
給定一組預測值 \( \hat{y} = \{ \hat{y_1}, \hat{y_2}, \dots, \hat{y_n} \} \) 和實際值 \( y = \{ y_1, y_2, \dots, y_n \} \)，則 MSE 的計算公式為：

\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
\]

其中，\( n \) 是樣本的數量，\( \hat{y}_i \) 是第 \( i \) 個樣本的預測值，\( y_i \) 是第 \( i \) 個樣本的實際值。

#### **特性**：
- MSE 的值總是非負的，且越小表示預測模型越準確。
- MSE 強調較大的誤差，因為誤差被平方，所以大誤差會被放大。
  
#### **優缺點**：
- **優點**：
  - 計算簡單且直觀。
  - 在回歸問題中表現良好。
- **缺點**：
  - 對離群點（Outliers）比較敏感，因為離群點會大幅提高誤差平方，影響訓練效果。

#### **PyTorch實現**：
在 PyTorch 中，均方誤差損失函數可以使用 `nn.MSELoss` 類來計算：

```python
import torch
import torch.nn as nn

# 假設有一些預測值和真實值
y_pred = torch.tensor([2.5, 0.0, 2.1, 7.8])
y_true = torch.tensor([3.0, -0.5, 2.0, 7.5])

# 創建均方誤差損失函數
mse_loss = nn.MSELoss()

# 計算損失
loss = mse_loss(y_pred, y_true)
print(loss)
```

---

### **2. 交叉熵 (Cross-Entropy)**

**交叉熵 (Cross-Entropy)** 損失函數是分類問題中常用的損失函數，特別是在二分類和多分類任務中。它衡量的是模型預測的概率分佈與真實分佈之間的差異。交叉熵損失的目標是最小化預測的類別概率分佈與真實分佈之間的距離。

#### **公式**：
對於二分類問題，交叉熵損失函數的公式為：

\[
\text{Binary Cross-Entropy Loss} = - \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\]

其中，\( y_i \) 是第 \( i \) 個樣本的真實標籤（0 或 1），\( \hat{y}_i \) 是第 \( i \) 個樣本的預測概率。

對於多分類問題，交叉熵損失函數的公式為：

\[
\text{Categorical Cross-Entropy Loss} = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
\]

其中，\( C \) 是類別數量，\( y_i \) 是實際標籤的 one-hot 編碼，\( \hat{y}_i \) 是預測的概率。

#### **特性**：
- 交叉熵損失函數對於概率分佈的差異進行度量，因此它特別適合用於分類任務。
- 在處理分類問題時，交叉熵能夠反映模型預測錯誤的嚴重程度。
  
#### **優缺點**：
- **優點**：
  - 可以處理多類別的分類問題，對於概率分佈的學習效果良好。
  - 相比於 MSE，交叉熵在處理分類問題時更加高效且直觀。
- **缺點**：
  - 當預測概率接近 0 或 1 時，交叉熵損失的梯度可能會非常大，可能導致梯度爆炸的問題。

#### **PyTorch實現**：
在 PyTorch 中，交叉熵損失函數可以使用 `nn.CrossEntropyLoss` 類來計算。對於二分類問題，可以使用 `nn.BCELoss` 或 `nn.BCEWithLogitsLoss`，後者直接接受未經激活函數的輸出。

**多分類交叉熵**：
```python
import torch
import torch.nn as nn

# 假設有一些預測的 logits 和真實標籤
logits = torch.tensor([[1.0, 2.0, 0.1], [0.5, 1.5, 3.0], [2.0, 0.5, 2.5]])
labels = torch.tensor([1, 2, 0])

# 創建交叉熵損失函數
cross_entropy_loss = nn.CrossEntropyLoss()

# 計算損失
loss = cross_entropy_loss(logits, labels)
print(loss)
```

**二分類交叉熵**：
```python
import torch
import torch.nn as nn

# 假設有一些預測的概率和真實標籤
y_pred = torch.tensor([0.9, 0.3, 0.8, 0.5])
y_true = torch.tensor([1, 0, 1, 0])

# 創建二分類交叉熵損失函數
bce_loss = nn.BCELoss()

# 計算損失
loss = bce_loss(y_pred, y_true)
print(loss)
```

---

### **總結**

- **均方誤差 (MSE)**：主要用於回歸問題，測量預測值與實際值之間的平方誤差，對於離群點較為敏感。
- **交叉熵 (Cross-Entropy)**：用於分類問題，衡量模型預測的概率分佈與真實分佈之間的差異，適合於二分類和多分類問題，並且對於預測錯誤進行有效處理。

選擇適當的損失函數對於提升模型性能至關重要，根據不同的任務類型來選擇適合的損失函數。