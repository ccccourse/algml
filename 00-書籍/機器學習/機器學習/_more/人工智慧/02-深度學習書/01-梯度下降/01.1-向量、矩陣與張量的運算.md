### **數學基礎：向量與矩陣運算**

在神經網路中，數學運算是核心概念之一，尤其是在處理數據和調整模型時，向量、矩陣與張量的運算非常重要。這些數學結構幫助我們有效地表示和處理多維數據。以下是向量、矩陣與張量的基礎運算：

#### 1. **向量（Vector）**
   向量是具有大小和方向的數量，可以用來表示數據點或參數。在計算中，向量通常表示為一維數組。
   
   - 向量的加法：
     \[
     \mathbf{v_1} = [v_{11}, v_{12}, \dots, v_{1n}], \quad \mathbf{v_2} = [v_{21}, v_{22}, \dots, v_{2n}]
     \]
     \[
     \mathbf{v_1} + \mathbf{v_2} = [v_{11} + v_{21}, v_{12} + v_{22}, \dots, v_{1n} + v_{2n}]
     \]
   
   - 向量的點積（內積）：
     \[
     \mathbf{v_1} \cdot \mathbf{v_2} = v_{11} \cdot v_{21} + v_{12} \cdot v_{22} + \dots + v_{1n} \cdot v_{2n}
     \]
     這個運算的結果是標量。

#### 2. **矩陣（Matrix）**
   矩陣是二維數據結構，可以看作是由多個向量組成的集合。矩陣運算對神經網路的前向傳播和後向傳播非常重要。

   - 矩陣的加法：
     \[
     \mathbf{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{bmatrix}
     \]
     \[
     \mathbf{A} + \mathbf{B} = \begin{bmatrix} a_{11} + b_{11} & a_{12} + b_{12} \\ a_{21} + b_{21} & a_{22} + b_{22} \end{bmatrix}
     \]

   - 矩陣的乘法：
     \[
     \mathbf{A} \cdot \mathbf{B} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \cdot \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{bmatrix}
     = \begin{bmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\ a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \end{bmatrix}
     \]

   - 矩陣與向量的乘法：
     \[
     \mathbf{A} \cdot \mathbf{v} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \cdot \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} a_{11}v_1 + a_{12}v_2 \\ a_{21}v_1 + a_{22}v_2 \end{bmatrix}
     \]

#### 3. **張量（Tensor）**
   張量是多維數據結構，可以看作是矩陣的延伸。它是深度學習中用來表示和處理數據的基本結構。在PyTorch中，張量是其最基本的數據結構。

   - 張量的運算：
     張量的運算規則類似於向量和矩陣，當維度更高時，這些運算的規則也會相應延伸。例如，兩個三維張量相加，會對應每個元素進行加法運算。

   - 張量與矩陣的乘法：
     例如，對一個三維張量與一個矩陣進行乘法時，計算會對應於每一層進行矩陣運算。

#### 4. **PyTorch中的操作**
   在PyTorch中，向量、矩陣和張量的操作非常直觀，可以通過以下常見操作實現：
   
   - 向量與矩陣的乘法：
     ```python
     import torch
     v1 = torch.tensor([1.0, 2.0])
     v2 = torch.tensor([3.0, 4.0])
     dot_product = torch.dot(v1, v2)
     print(dot_product)  # 結果是 11.0
     ```

   - 矩陣乘法：
     ```python
     A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
     B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])
     result = torch.matmul(A, B)
     print(result)
     # 輸出為：
     # tensor([[19., 22.],
     #         [43., 50.]])
     ```

   - 張量運算：
     ```python
     T1 = torch.randn(2, 3, 4)
     T2 = torch.randn(2, 3, 4)
     result_tensor = T1 + T2  # 兩個同維度張量的加法
     print(result_tensor)
     ```

掌握這些基本的數學運算及其在PyTorch中的應用，可以幫助我們高效地進行神經網絡的構建與訓練。