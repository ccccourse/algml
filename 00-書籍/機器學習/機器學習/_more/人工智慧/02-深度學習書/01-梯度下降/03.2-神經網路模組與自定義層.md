### **3. PyTorch 基礎**
#### **神經網路模組與自定義層**

在 PyTorch 中，神經網路是由不同的層（layers）組成的，而每一層都對輸入數據進行某種形式的處理。PyTorch 使用 `torch.nn` 模組來定義神經網路。這些層通常是通過 `nn.Module` 類來實現的，`nn.Module` 提供了許多方便的功能，如自動追蹤模型中的參數、前向傳播等。

在此部分，我們將介紹如何利用 PyTorch 提供的模組來構建神經網路，以及如何定義自定義層。

---

### **1. 使用 `nn.Module` 創建神經網路**

`nn.Module` 是所有神經網路層的基礎類，無論是預定義層還是自定義層，都應該繼承自 `nn.Module`。

#### **建立一個簡單的神經網路**
假設我們需要創建一個簡單的多層感知器（MLP）模型，包括一個線性層和 ReLU 激活層，並定義前向傳播方法。

```python
import torch
import torch.nn as nn

# 定義一個簡單的神經網路類
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        # 定義網路中的層
        self.fc1 = nn.Linear(784, 128)  # 全連接層
        self.relu = nn.ReLU()  # ReLU 激活函數
        self.fc2 = nn.Linear(128, 10)  # 輸出層 (假設有 10 個類別)
    
    # 定義前向傳播
    def forward(self, x):
        x = self.fc1(x)  # 第一層
        x = self.relu(x)  # ReLU 激活
        x = self.fc2(x)  # 第二層
        return x

# 創建模型
model = SimpleNN()
print(model)
```

在這個範例中，我們創建了兩個全連接層 `fc1` 和 `fc2`，並且使用 ReLU 激活函數來進行非線性轉換。在 `forward()` 方法中，我們定義了前向傳播過程，這是神經網路中數據如何流動的部分。

---

### **2. 自定義層的定義**

有時候，我們需要設計一些特定的層，而這些層不會在 PyTorch 預定義的模組中找到。在這種情況下，我們可以通過繼承 `nn.Module` 來創建自定義層。

#### **自定義一個簡單的加法層**
假設我們想創建一個層，它會對輸入進行簡單的加法操作，將一個常數加到每個輸入元素上。

```python
class AddConstantLayer(nn.Module):
    def __init__(self, constant):
        super(AddConstantLayer, self).__init__()
        self.constant = constant
    
    def forward(self, x):
        return x + self.constant

# 創建自定義層
add_layer = AddConstantLayer(5)
x = torch.tensor([1.0, 2.0, 3.0])
output = add_layer(x)
print(output)  # Output: tensor([6.0, 7.0, 8.0])
```

這個 `AddConstantLayer` 層會將 `constant` 加到輸入 `x` 上。在 `__init__()` 方法中，我們設置了常數，並在 `forward()` 方法中定義了加法操作。

---

### **3. 自定義層中的參數**

在 PyTorch 中，大多數層都包含可以學習的參數，例如 `nn.Linear` 層的權重和偏置。對於自定義層，我們也可以在層中包含學習參數，並且這些參數會像其他層一樣被自動註冊和優化。

#### **自定義帶有可學習參數的層**
例如，假設我們創建一個自定義的層，這個層會將輸入的每個元素乘以一個學習的係數。

```python
class LearnableScaleLayer(nn.Module):
    def __init__(self):
        super(LearnableScaleLayer, self).__init__()
        # 定義一個可學習的參數
        self.scale = nn.Parameter(torch.ones(1))  # 這是一個可學習的參數
    
    def forward(self, x):
        return x * self.scale

# 創建自定義層
scale_layer = LearnableScaleLayer()
x = torch.tensor([1.0, 2.0, 3.0])
output = scale_layer(x)
print(output)  # Output: tensor([1.0, 2.0, 3.0])，初始化的係數為1
```

在這裡，我們使用 `nn.Parameter` 來創建一個可學習的參數 `scale`，並將其乘到輸入上。這個參數會像 `nn.Linear` 層中的權重一樣被自動訓練。

---

### **4. 模型訓練與自定義層的使用**

在訓練過程中，我們可以將自定義層與其他 PyTorch 預定義的層結合使用。這樣，我們可以自由設計並訓練具有特定行為的層。

#### **訓練神經網路與自定義層**
```python
import torch.optim as optim

# 創建模型（包括自定義層）
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    AddConstantLayer(5),  # 使用自定義層
    nn.Linear(128, 10)
)

# 定義損失函數與優化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 假設我們有一些訓練數據
inputs = torch.randn(64, 784)  # 假設有 64 條數據，每條數據有 784 維
labels = torch.randint(0, 10, (64,))  # 假設這是分類標籤

# 訓練循環
optimizer.zero_grad()  # 清除舊的梯度
outputs = model(inputs)  # 前向傳播
loss = criterion(outputs, labels)  # 計算損失
loss.backward()  # 反向傳播
optimizer.step()  # 更新模型參數

print(loss.item())  # 顯示損失值
```

在這裡，我們使用了 `nn.Sequential` 來組合各種層，其中包括我們的自定義層 `AddConstantLayer`。這樣就可以將自定義層與標準的 PyTorch 層進行協同工作，並進行模型訓練。

---

### **總結**

- **神經網路模組**：PyTorch 中的 `nn.Module` 類用於創建神經網路，它提供了基本的功能來定義層、管理參數以及實現前向傳播。
- **自定義層**：使用繼承自 `nn.Module` 的方式，可以創建具有自定義邏輯的層，如加法層、縮放層等。
- **自定義層中的參數**：使用 `nn.Parameter` 可以創建可學習的參數，這些參數會在訓練過程中進行更新。

掌握了這些技巧後，你可以根據需要設計和實現自定義的層，並與 PyTorch 的其他功能結合使用，從而構建出更加靈活和創新的神經網路模型。