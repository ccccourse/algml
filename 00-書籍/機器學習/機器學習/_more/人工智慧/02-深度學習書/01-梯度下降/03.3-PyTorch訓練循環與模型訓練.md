### **3. PyTorch 基礎**
#### **PyTorch 訓練循環與模型訓練**

在 PyTorch 中，模型訓練循環（training loop）是訓練深度學習模型的核心部分。這一過程涉及從數據加載、模型前向傳播、損失計算、反向傳播到模型參數更新等步驟。通常，訓練循環會被重複多次，直到模型達到預期的精度。

### **1. 訓練循環的基本步驟**

訓練深度學習模型通常包含以下步驟：

1. **數據加載**：從數據集讀取訓練數據。
2. **模型前向傳播**：將輸入數據傳遞給模型並獲取輸出。
3. **損失計算**：根據模型的預測和真實標籤計算損失。
4. **反向傳播**：根據損失計算梯度並更新模型參數。
5. **參數更新**：使用優化器更新模型的參數。
6. **重複以上步驟**：對所有訓練數據進行多次迭代。

### **2. 實現訓練循環**

下面是一個簡單的 PyTorch 訓練循環範例，展示了如何進行模型訓練。

#### **範例：簡單的 MLP 模型訓練**

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 創建一個簡單的多層感知器（MLP）模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 創建隨機數據（假設每個樣本有784個特徵，並且有10個類別）
inputs = torch.randn(1000, 784)  # 1000 個樣本，每個樣本有 784 維
labels = torch.randint(0, 10, (1000,))  # 1000 個標籤，每個標籤在 0 到 9 之間

# 創建數據加載器
dataset = TensorDataset(inputs, labels)
train_loader = DataLoader(dataset, batch_size=64, shuffle=True)

# 初始化模型、損失函數和優化器
model = SimpleNN()
criterion = nn.CrossEntropyLoss()  # 多類別分類使用交叉熵損失函數
optimizer = optim.SGD(model.parameters(), lr=0.01)  # 使用 SGD 優化器

# 訓練循環
num_epochs = 5
for epoch in range(num_epochs):
    model.train()  # 設置模型為訓練模式
    running_loss = 0.0
    for inputs_batch, labels_batch in train_loader:
        optimizer.zero_grad()  # 清除上一步的梯度
        outputs = model(inputs_batch)  # 前向傳播
        loss = criterion(outputs, labels_batch)  # 計算損失
        loss.backward()  # 反向傳播
        optimizer.step()  # 更新參數
        
        running_loss += loss.item()  # 累加損失

    # 輸出每個 epoch 的平均損失
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')
```

### **3. 訓練循環的解釋**

1. **數據加載**：
   - 使用 `DataLoader` 將訓練數據集分成小批次（batch）。這樣做的目的是提高訓練效率，避免一次加載過多數據導致內存溢出。
   
2. **前向傳播**：
   - 在每次迭代中，將當前的輸入批次傳遞給模型，並獲得模型的預測結果。

3. **損失計算**：
   - 使用損失函數（如 `CrossEntropyLoss`）來計算模型預測結果與真實標籤之間的誤差。

4. **反向傳播**：
   - 調用 `loss.backward()` 來計算每個參數的梯度。這是反向傳播的步驟。

5. **參數更新**：
   - 使用優化器（如 `SGD`）來更新模型的參數，使損失最小化。

6. **迭代**：
   - 通常會進行多次訓練輪次（epoch），每次訓練一遍整個訓練數據集。

---

### **4. 測試與驗證**

訓練過程中的一個重要部分是測試或驗證模型的性能，這通常會在每個訓練輪次後進行。

#### **測試過程**

測試通常是在不計算梯度的情況下進行的，因此需要禁用梯度計算。

```python
# 測試過程
model.eval()  # 設置模型為評估模式
with torch.no_grad():  # 禁用梯度計算
    correct = 0
    total = 0
    for inputs_batch, labels_batch in test_loader:  # test_loader 是測試數據集的 DataLoader
        outputs = model(inputs_batch)
        _, predicted = torch.max(outputs, 1)  # 獲得最大概率的索引
        total += labels_batch.size(0)
        correct += (predicted == labels_batch).sum().item()

accuracy = correct / total
print(f'Accuracy: {accuracy * 100:.2f}%')
```

- **`model.eval()`**：將模型設置為評估模式，這樣像 `Dropout` 和 `BatchNorm` 這類層會在測試期間處於靜態模式。
- **`torch.no_grad()`**：禁用梯度計算，這樣可以節省內存，並提高測試效率。

---

### **5. 優化技巧**

在訓練過程中，有一些技巧可以幫助提高模型性能：

- **學習率調整**：隨著訓練進行，可以動態調整學習率，使用學習率衰減（例如使用 `lr_scheduler`）。
- **動量**：使用動量來加速收斂，減少訓練的震盪。
- **梯度裁剪**：防止梯度爆炸，特別是在深層網絡中，通過梯度裁剪來限制梯度的最大值。

---

### **總結**

- **訓練循環**是深度學習中非常關鍵的一部分，涉及數據加載、前向傳播、損失計算、反向傳播和參數更新。
- **訓練過程**會進行多次迭代，直到模型的性能達到預期。
- **測試和驗證**是檢驗模型效果的重要步驟，通常會在每個 epoch 後進行。

掌握了這些基礎，您就可以開始用 PyTorch 訓練各種神經網絡模型了。