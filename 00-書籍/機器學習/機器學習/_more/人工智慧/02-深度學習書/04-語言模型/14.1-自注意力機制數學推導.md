### **14.1 自注意力機制（Self-Attention）數學推導**

自注意力機制（Self-Attention）是Transformer模型中的核心概念之一，它允許模型在處理序列時考慮序列中各個位置之間的關聯，無論這些位置有多遠。這種機制尤其對於長序列數據（如文本或語音）非常有效，因為它解決了傳統RNN和LSTM在處理長程依賴關係時的困難。

#### **1. 自注意力的概述**

在自注意力機制中，對於序列中的每個元素，我們根據該元素與其他所有元素的關聯來進行加權。這樣，每個元素的表示都會被重新計算，使得它在捕捉到序列中其他部分的信息後變得更加豐富。具體來說，對於一個給定的輸入序列，對每個位置的表示進行加權求和，以形成新的輸出表示。

#### **2. 計算步驟**

在自注意力機制中，我們通常有三個重要的向量：
- **Query（Q）：** 查詢向量，表示需要關注的元素。
- **Key（K）：** 鍵向量，與查詢向量比較，用來確定與其他元素的相關性。
- **Value（V）：** 值向量，表示與該位置相關的特徵。

這些向量是從輸入嵌入向量中通過線性變換得到的。假設輸入序列的長度為 \( n \)，每個元素的向量表示為 \( d \)-維。

#### **3. 計算步驟詳細推導**

##### 3.1 計算注意力權重

首先，我們計算查詢向量 \( Q \) 與鍵向量 \( K \) 的相似度。常見的相似度計算方式是**點積**。對於每一對查詢和鍵，我們計算它們的點積，再通過一個縮放因子進行標準化。具體公式如下：

\[
\text{Attention Scores} = QK^T
\]

這裡的 \( Q \in \mathbb{R}^{n \times d} \)，\( K \in \mathbb{R}^{n \times d} \)，其點積結果是 \( n \times n \) 的矩陣，表示每個查詢與每個鍵之間的相似度。為了避免內積的值過大，我們會將內積結果除以 \( \sqrt{d} \)，其中 \( d \) 是向量的維度。

\[
\text{Scaled Attention Scores} = \frac{QK^T}{\sqrt{d}}
\]

##### 3.2 計算權重分佈

將相似度分數通過**Softmax**函數進行處理，這樣可以將其轉換為一個概率分佈，使得所有權重的和為1：

\[
\text{Attention Weights} = \text{Softmax} \left( \frac{QK^T}{\sqrt{d}} \right)
\]

這樣每個元素對於其他元素的關注程度就得到了權重。

##### 3.3 計算加權值

最後，我們使用這些注意力權重來對值向量 \( V \) 進行加權求和，以生成最終的自注意力輸出：

\[
\text{Attention Output} = \text{Attention Weights} \cdot V
\]

這樣，對於每個位置，我們根據其與其他位置的關聯計算加權和，得到最終的表示。

#### **4. 自注意力的總結公式**

總結上述步驟，自注意力機制的數學公式如下：

\[
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{QK^T}{\sqrt{d}} \right) V
\]

這是單一的自注意力計算。對於一個輸入序列，我們會進行多頭注意力（multi-head attention），即多次進行自注意力計算，並將結果進行拼接和線性變換，這有助於模型學習更多不同的關聯模式。

#### **5. 實際中用到的技巧：多頭注意力**

在實際應用中，我們不僅僅計算一個查詢、鍵和值向量，而是使用多組查詢、鍵和值向量進行計算，這就是所謂的**多頭注意力（Multi-head Attention）**。多頭注意力的優點在於它能夠並行捕捉序列中不同位置之間的多種關聯模式。多頭注意力的計算公式為：

\[
\text{Multi-Head Attention}(Q, K, V) = \text{Concat}(head_1, head_2, \dots, head_h)W^O
\]

其中，每個 \( head_i \) 是對應的自注意力結果，並且 \( W^O \) 是最終的線性變換矩陣。每個頭的計算使用不同的線性變換來生成多組查詢、鍵和值。

### **6. 直觀解釋**

簡單來說，**自注意力機制**使得序列中的每個位置都可以根據其與其他位置的關聯來調整其表示。這種機制使得模型能夠捕捉到長程依賴關係，並能有效地處理各種不同長度的序列數據。

- **查詢向量（Query）：** 你對序列中的某個位置感興趣，想了解該位置與其他位置的關聯。
- **鍵向量（Key）：** 每個位置的特徵，用來與查詢向量進行匹配。
- **值向量（Value）：** 每個位置的輸出特徵，經過加權後更新最終表示。

### **總結**

自注意力機制使得每個位置可以根據整個序列中的其他位置的特徵來更新自己，使得模型在處理長序列時，能夠捕捉到更複雜的依賴關係。這種機制是Transformer架構的核心，並且對於NLP、計算機視覺等領域的許多任務有著廣泛的應用。