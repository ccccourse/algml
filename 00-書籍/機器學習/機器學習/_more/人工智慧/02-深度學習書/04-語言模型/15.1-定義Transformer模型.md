### **15.1 用 `nn.Transformer` 和自定義模型構建 Transformer**

在PyTorch中，`nn.Transformer`提供了一個高效且靈活的接口，讓我們可以方便地實現Transformer架構。`nn.Transformer`是PyTorch內建的模組，用來實現Transformer模型，它包括編碼器和解碼器，並支持自注意力、多頭注意力、位置編碼等功能。

以下將展示如何使用`nn.Transformer`構建一個基本的Transformer模型。

#### **1. 定義Transformer模型**

首先，我們需要定義一個Transformer模型。此模型會包括：
1. **嵌入層（Embedding Layer）**：將輸入的單詞（或符號）映射到一個向量空間。
2. **位置編碼（Positional Encoding）**：在序列中加入每個位置的信息，幫助模型捕捉序列的順序性。
3. **Transformer層**：使用`nn.Transformer`來處理序列數據。
4. **線性層**：將Transformer的輸出映射到最終的預測（例如語言建模中可能是詞彙表大小的預測）。

#### **2. 使用 `nn.Transformer` 實現**

```python
import torch
import torch.nn as nn
import torch.optim as optim

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_len):
        super(TransformerModel, self).__init__()
        
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)  # 嵌入層
        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))  # 位置編碼
        
        # 定義Transformer模型
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward
        )
        
        # 定義輸出層（將Transformer輸出映射到詞彙表大小）
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt):
        # src和tgt分別是輸入序列和目標序列
        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))  # 嵌入並調整尺度
        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))  # 嵌入並調整尺度
        
        # 將位置編碼加到嵌入層的輸出中
        src += self.positional_encoding[:, :src.size(1), :]
        tgt += self.positional_encoding[:, :tgt.size(1), :]
        
        # 將處理後的src和tgt輸入到Transformer中
        output = self.transformer(src, tgt)
        
        # 通過線性層映射到最終的預測結果
        out = self.fc_out(output)
        
        return out

# 初始化模型參數
vocab_size = 10000   # 假設詞彙表大小為10000
d_model = 512        # 模型維度
nhead = 8            # 多頭注意力頭數
num_encoder_layers = 6
num_decoder_layers = 6
dim_feedforward = 2048
max_seq_len = 512    # 最大序列長度

# 創建模型
model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_len)

# 輸入數據示範
src = torch.randint(0, vocab_size, (32, 50))  # 假設批次大小為32，序列長度為50
tgt = torch.randint(0, vocab_size, (32, 50))  # 目標序列長度為50

# 前向傳播
output = model(src, tgt)
print(output.shape)  # 輸出形狀應該是 (batch_size, tgt_seq_len, vocab_size)
```

### **2. 代碼解釋**

- **嵌入層（Embedding Layer）**：將詞彙表的每個單詞（索引）映射為固定維度的向量（`d_model`）。這樣的嵌入層有助於在模型中表示語言的語義。
- **位置編碼（Positional Encoding）**：由於Transformer不具備序列順序的自動感知能力，因此需要將位置編碼添加到嵌入向量中。這個編碼是基於每個詞在序列中的位置而生成的，並與單詞的嵌入向量相加。
- **Transformer層（Transformer Layer）**：`nn.Transformer` 是一個完整的序列到序列的處理模組，包含多層自注意力、前向傳播層及其相應的殘差連接。
- **輸出層（Output Layer）**：最後的線性層將Transformer的輸出（形狀為 `(batch_size, seq_len, d_model)`）映射到詞彙表大小，生成每個位置的預測（例如詞彙表中每個詞的機率）。

### **3. 訓練與優化**

在訓練Transformer時，通常會使用常見的優化器（如Adam）來最小化損失函數。以下是一個基本的訓練循環：

```python
# 訓練參數設置
optimizer = optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()

# 訓練循環
for epoch in range(10):  # 假設訓練10個epochs
    model.train()  # 設置模型為訓練模式
    optimizer.zero_grad()  # 清空梯度

    # 假設有輸入和目標數據
    src = torch.randint(0, vocab_size, (32, 50))
    tgt = torch.randint(0, vocab_size, (32, 50))

    # 前向傳播
    output = model(src, tgt[:-1])  # 目標序列是從tgt[:-1]開始，並對每一個位置預測
    output = output.view(-1, vocab_size)  # 拉平成一個向量

    # 計算損失
    loss = criterion(output, tgt[1:].view(-1))  # 計算損失（對應的真實標籤是tgt[1:]）
    
    # 反向傳播
    loss.backward()
    optimizer.step()  # 更新權重
    
    # 輸出損失
    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')
```

### **4. 進一步的擴展**

- **學習率調度（Learning Rate Scheduling）**：可以使用學習率調度器來隨著訓練進行逐步調整學習率。
- **多GPU訓練**：對於更大的模型或數據集，可以考慮使用`DataParallel`或`DistributedDataParallel`來進行多GPU訓練。
- **正則化技術**：在訓練過程中，可以考慮使用Dropout、Label Smoothing等技術來防止過擬合。

### **總結**

這個簡單的示範展示了如何使用PyTorch的`nn.Transformer`來構建一個基本的Transformer模型。該模型包含了編碼器、解碼器、嵌入層、位置編碼以及最終的輸出層，並能夠用於語言建模、機器翻譯等序列到序列的任務。