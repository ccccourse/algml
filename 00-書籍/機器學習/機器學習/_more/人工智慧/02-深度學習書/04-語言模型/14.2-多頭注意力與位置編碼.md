### **14.2 多頭注意力與位置編碼**

在Transformer模型中，**多頭注意力**和**位置編碼**是非常重要的組件，它們分別解決了模型捕捉序列中多種依賴關係的需求以及處理序列中元素順序的問題。

#### **1. 多頭注意力（Multi-Head Attention）**

多頭注意力機制的目的是讓模型能夠在多個子空間中同時學習不同的依賴關係。相比於單一注意力頭（單個自注意力機制），多頭注意力讓模型在計算注意力時能夠並行處理多個不同的"視角"，從而捕捉更多樣化的關聯。

##### **1.1 多頭注意力的計算過程**

在多頭注意力中，首先會將查詢（Query）、鍵（Key）和值（Value）向量分成多個**頭**。每個頭會學習一個獨立的注意力權重，並根據其學到的權重對值向量進行加權。這樣，每個頭可以專注於序列中不同的部分或模式，捕捉到多樣的依賴關係。

1. **線性變換：** 將輸入的查詢、鍵和值向量分別進行多個線性變換，得到不同的"頭"（即分別生成 \( h \) 個查詢、鍵和值矩陣）。

2. **計算每個頭的自注意力：** 對每個頭分別計算自注意力，得到每個頭的輸出。

3. **拼接與線性變換：** 最後，將所有頭的輸出拼接起來，並通過一個最終的線性變換來獲得多頭注意力的最終輸出。

##### **1.2 數學公式**

假設有 \( h \) 個頭，每個頭的維度是 \( d_k \)，則多頭注意力的計算公式如下：

1. **分割查詢、鍵和值：**
   
   \[
   Q_i = XW_i^Q, \quad K_i = XW_i^K, \quad V_i = XW_i^V \quad \text{for} \ i = 1, 2, \dots, h
   \]

   其中，\( X \) 是輸入序列，\( W_i^Q, W_i^K, W_i^V \) 是對應的線性變換矩陣。

2. **每個頭的自注意力計算：**

   \[
   \text{Attention}_i(Q_i, K_i, V_i) = \text{Softmax} \left( \frac{Q_i K_i^T}{\sqrt{d_k}} \right) V_i
   \]

3. **拼接所有頭的輸出並進行線性變換：**

   \[
   \text{Multi-Head Attention}(Q, K, V) = \text{Concat}(\text{Attention}_1, \text{Attention}_2, \dots, \text{Attention}_h) W^O
   \]

   其中，\( W^O \) 是最終的線性變換矩陣，將拼接後的結果映射到預期的輸出維度。

##### **1.3 多頭注意力的優勢**

- **捕捉多樣的關聯：** 多頭注意力能夠在不同的子空間中捕捉到不同的特徵，從而學習到多種依賴關係。
- **提升學習能力：** 每個頭獨立學習，可以有效提高模型的表示能力。

#### **2. 位置編碼（Positional Encoding）**

Transformer模型與RNN和LSTM不同，它不使用循環結構來處理序列中的元素，而是依賴於**自注意力機制**來學習序列中元素之間的依賴。然而，自注意力機制本身並不關注序列中元素的順序，因此我們需要**位置編碼**來引入元素在序列中的相對或絕對位置資訊。

##### **2.1 位置編碼的作用**

位置編碼的目的是讓模型在處理序列時能夠知道每個元素的位置，從而學會捕捉序列中的順序信息。位置編碼會與輸入的詞嵌入（embedding）相加，並提供給模型。

##### **2.2 位置編碼的數學公式**

在Transformer模型中，位置編碼通常使用正弦和餘弦函數來表示不同位置的特徵。這樣的設計有助於模型捕捉不同位置之間的相對關係，且這些編碼具有周期性，能夠對長序列中的位置進行推廣。

對於位置編碼的計算，假設輸入序列的長度為 \( n \)，每個位置的編碼維度為 \( d_{model} \)，位置編碼 \( PE \) 的公式如下：

\[
PE_{(p, 2i)} = \sin\left( \frac{p}{10000^{\frac{2i}{d_{model}}}} \right)
\]

\[
PE_{(p, 2i+1)} = \cos\left( \frac{p}{10000^{\frac{2i}{d_{model}}}} \right)
\]

其中，\( p \) 是位置，\( i \) 是維度索引，\( d_{model} \) 是位置編碼的維度。

##### **2.3 位置編碼的實現**

位置編碼會與輸入的詞嵌入相加，這樣每個位置的詞向量就包含了其位置信息：

\[
\text{Input Embedding} = \text{Word Embedding} + PE
\]

這樣，模型就可以在處理序列時同時考慮到每個元素的內容和它在序列中的位置。

#### **3. 位置編碼與多頭注意力的結合**

在Transformer中，位置編碼和多頭注意力機制是密切配合的。位置編碼提供了序列順序的資訊，而多頭注意力則讓模型能夠從多個角度學習序列中的依賴關係。這種組合使得Transformer能夠處理長序列並捕捉長程依賴。

#### **4. 結論**

- **多頭注意力**能夠讓模型並行捕捉序列中不同部分之間的多種關聯，有效地提升了學習能力。
- **位置編碼**是Transformer中重要的組件，通過將位置信息引入模型，使其能夠理解序列中元素的相對位置。

這兩個機制的結合，使得Transformer成為一個強大的模型，廣泛應用於自然語言處理、圖像處理等領域。