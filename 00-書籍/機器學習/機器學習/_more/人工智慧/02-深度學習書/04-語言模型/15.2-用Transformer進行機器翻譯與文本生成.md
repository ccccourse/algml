### **15.2 用 Transformer 進行機器翻譯與文本生成**

Transformer架構在機器翻譯和文本生成等自然語言處理（NLP）任務中表現卓越。這是因為其自注意力機制能夠有效捕捉長距離的依賴關係，從而使得模型在處理語言結構時更加靈活和強大。

以下介紹如何使用Transformer進行機器翻譯和文本生成。

### **1. 用 Transformer 進行機器翻譯**

在機器翻譯中，Transformer模型被用來將一種語言的文本（源語言）轉換為另一種語言的文本（目標語言）。這是一個序列到序列（Sequence-to-Sequence, Seq2Seq）的任務，其中源語言文本作為輸入，目標語言文本作為輸出。

#### **步驟1：數據處理**

機器翻譯任務的第一步是準備數據集。假設我們有一個語料庫，其中每一對源語言句子和目標語言句子都被對應在一起。

- **數據格式**：通常，源語言和目標語言的句子會進行分詞（tokenization）並轉換為數字索引（例如使用字典將詞語映射為索引）。

```python
# 假設已經有源語言和目標語言的數據
src_sentences = ['I am a student', 'Hello world', ...]  # 源語言句子
tgt_sentences = ['我是學生', '你好，世界', ...]  # 目標語言句子

# 進行詞彙分詞和數字映射（這里使用的只是簡單示範，實際情況可能會更復雜）
src_vocab = {'I': 0, 'am': 1, 'a': 2, 'student': 3, ...}
tgt_vocab = {'我是': 0, '學生': 1, '你好': 2, '世界': 3, ...}

# 編碼：將每個句子轉換為對應的數字索引
src_input = [src_vocab[word] for word in src_sentences[0].split()]
tgt_input = [tgt_vocab[word] for word in tgt_sentences[0].split()]
```

#### **步驟2：定義模型架構**

Transformer模型的架構和之前的一樣，我們需要在這裡使用源語言和目標語言來進行編碼和解碼。

```python
class Seq2SeqTransformer(nn.Module):
    def __init__(self, vocab_size_src, vocab_size_tgt, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_len):
        super(Seq2SeqTransformer, self).__init__()
        
        self.src_embedding = nn.Embedding(vocab_size_src, d_model)  # 源語言嵌入層
        self.tgt_embedding = nn.Embedding(vocab_size_tgt, d_model)  # 目標語言嵌入層
        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))  # 位置編碼
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward
        )
        
        self.fc_out = nn.Linear(d_model, vocab_size_tgt)  # 輸出層，映射到目標語言詞彙表
        
    def forward(self, src, tgt):
        # 嵌入和位置編碼
        src = self.src_embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))
        tgt = self.tgt_embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))
        
        # 位置編碼加到嵌入向量中
        src += self.positional_encoding[:, :src.size(1), :]
        tgt += self.positional_encoding[:, :tgt.size(1), :]
        
        # Transformer的前向傳播
        output = self.transformer(src, tgt)
        
        # 通過線性層生成最終的翻譯結果
        out = self.fc_out(output)
        return out
```

#### **步驟3：訓練模型**

在訓練階段，模型學習如何從源語言（`src`）生成目標語言（`tgt`）的對應翻譯。訓練過程中會使用`CrossEntropyLoss`來計算每個位置的預測與真實標籤之間的差異。

```python
# 假設已經定義好模型
model = Seq2SeqTransformer(vocab_size_src=5000, vocab_size_tgt=5000, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, max_seq_len=512)

optimizer = optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()

# 訓練循環
for epoch in range(10):  # 假設訓練10個epochs
    model.train()
    optimizer.zero_grad()

    src = torch.randint(0, 5000, (32, 50))  # 假設的批次大小32，序列長度50
    tgt = torch.randint(0, 5000, (32, 50))  # 假設目標序列長度50

    output = model(src, tgt[:-1])  # 不使用tgt的最後一個詞作為輸入
    output = output.view(-1, 5000)  # 拉平成向量

    loss = criterion(output, tgt[1:].view(-1))  # tgt[1:]是目標序列的真實標籤
    loss.backward()
    optimizer.step()

    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')
```

#### **步驟4：生成翻譯結果**

在推理階段，當我們給模型一個源語言句子時，模型將生成目標語言的翻譯。由於翻譯是逐步進行的，每次生成一個詞並將其作為下一個輸入。

```python
def translate(model, src_sentence, max_len=50):
    model.eval()
    src = torch.tensor([src_vocab[word] for word in src_sentence.split()]).unsqueeze(0)  # 將源句子轉換為數字
    tgt = torch.tensor([tgt_vocab['<sos>']]).unsqueeze(0)  # 開始符號
    
    for _ in range(max_len):
        output = model(src, tgt)  # 將源句子和當前目標句子輸入模型
        next_word = output.argmax(dim=-1)[:, -1]  # 取得模型預測的下一個詞
        tgt = torch.cat([tgt, next_word.unsqueeze(0)], dim=1)  # 將下一個詞加到目標序列中
        
        if next_word.item() == tgt_vocab['<eos>']:  # 如果遇到結束符號，則停止
            break
    
    return ' '.join([list(tgt_vocab.keys())[list(tgt_vocab.values()).index(word.item())] for word in tgt.squeeze(0)])

# 假設要翻譯的句子
src_sentence = "I am a student"
translation = translate(model, src_sentence)
print(f"翻譯結果: {translation}")
```

### **2. 用 Transformer 進行文本生成**

Transformer在文本生成方面也表現出色。生成模型的目的是根據給定的文本（例如，語句的開頭）自動生成其後的文本。

在文本生成中，Transformer將基於先前的上下文生成下一個最可能的詞，直到生成結束符號或達到最大長度。

#### **文本生成訓練與推理**

與機器翻譯類似，文本生成的訓練與推理方法也使用相同的Transformer架構，並以自回歸方式進行生成。模型通過已知的上下文逐步預測未來的詞。

### **總結**

這一部分介紹了如何使用Transformer進行機器翻譯和文本生成。透過自注意力機制，Transformer可以有效捕捉長距離的依賴關係，並在各種自然語言處理任務中提供強大的性能。在實際應用中，Transformer模型的大小和訓練數據集的規模會影響最終效果，因此需要根據具體問題進行調整和優化。