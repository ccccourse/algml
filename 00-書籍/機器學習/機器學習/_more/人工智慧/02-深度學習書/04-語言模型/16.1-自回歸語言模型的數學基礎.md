### **16.1 自回歸語言模型的數學基礎**

自回歸語言模型是一種根據已有的上下文信息預測下一個單詞或標記的生成模型。以下是其核心數學基礎的細節：

---

#### **1. 條件概率分佈的定義與建模**  

- **條件概率公式**：  
  給定一個詞序列 \((x_1, x_2, \dots, x_T)\)，自回歸模型通過條件概率分佈來表示該序列的聯合機率：
  \[
  P(x_1, x_2, \dots, x_T) = \prod_{t=1}^T P(x_t \mid x_1, x_2, \dots, x_{t-1})
  \]  

- **建模目標**：  
  模型的目的是學習 \( P(x_t \mid x_1, x_2, \dots, x_{t-1}) \)，即根據先前所有詞生成當前詞的條件機率。

---

#### **2. 自回歸模型的公式推導**  

- **鏈式法則**：  
  透過鏈式法則將序列生成問題分解為多個條件概率問題：
  \[
  P(x_1, x_2, \dots, x_T) = P(x_1) \cdot P(x_2 \mid x_1) \cdot P(x_3 \mid x_1, x_2) \cdots P(x_T \mid x_1, \dots, x_{T-1}).
  \]  

- **特徵表示**：  
  在神經網路模型中，輸入的序列 \( (x_1, x_2, \dots, x_{t-1}) \) 通常轉換為嵌入向量，再輸入模型進行條件概率的計算。

---

#### **3. 單詞生成的機率分佈與最大似然估計**  

- **單詞生成的機率分佈**：  
  給定上下文 \( x_1, x_2, \dots, x_{t-1} \)，模型輸出每個可能詞 \( x_t \) 的機率分佈：
  \[
  P(x_t \mid x_1, x_2, \dots, x_{t-1}) = \text{softmax}(W h_t + b)
  \]
  - \( h_t \)：通過模型計算的上下文向量（例如 Transformer 的輸出）。
  - \( W \) 和 \( b \)：線性投影的權重和偏置。

- **損失函數（最大似然估計）**：  
  使用交叉熵作為損失函數，最大化訓練數據中序列的對數機率：
  \[
  \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T_i} \log P(x_t^{(i)} \mid x_1^{(i)}, x_2^{(i)}, \dots, x_{t-1}^{(i)})
  \]

---

#### **4. 自回歸假設對序列生成的影響**  

- **優勢**：  
  - 自回歸模型能夠捕捉序列的上下文相關性，生成的序列流暢且語義一致。
  
- **限制**：  
  - **生成過程的時間成本高**：單詞逐一生成，無法並行化。
  - **長距離依賴問題**：模型可能難以捕捉長文本中的全局依賴性。

---

透過以上數學基礎，GPT 能夠將語言生成建模為條件概率分佈的學習問題，並通過最大似然估計進行高效的參數優化。