### **14.3 Transformer中的編碼器與解碼器**

Transformer模型的核心由兩個主要部分組成：**編碼器（Encoder）**和**解碼器（Decoder）**。這兩個部分在結構上是相似的，但它們的功能和處理方式有所不同。編碼器將輸入序列轉換為一組隱含表示，解碼器則根據這些隱含表示生成輸出序列。

#### **1. 編碼器（Encoder）**

編碼器的任務是接收一個輸入序列，並將其轉換為一組上下文相關的隱含表示。這些隱含表示捕捉了輸入序列中各個元素之間的依賴關係。編碼器的主要組成部分是：**多頭自注意力（Self-Attention）**和**前向傳播層（Feed-Forward Network）**。

##### **1.1 編碼器的結構**

一個編碼器層由以下幾個部分組成：

1. **多頭自注意力層（Multi-Head Self-Attention Layer）**：
   - 該層通過自注意力機制捕捉序列中元素之間的依賴關係。每個輸入元素會根據查詢、鍵和值計算注意力權重，並加權計算相應的輸出。
   
2. **前向傳播層（Feed-Forward Neural Network）**：
   - 這是一個兩層的全連接神經網絡，其中包含ReLU激勵函數。該層的作用是對每個位置的隱含表示進行非線性變換。

3. **殘差連接與層正則化（Residual Connection & Layer Normalization）**：
   - 每個子層（注意力層和前向傳播層）都會有一個殘差連接，並使用層正則化來加速訓練並防止過擬合。

編碼器的輸入是嵌入向量（包括位置編碼），並經過若干個編碼器層進行處理。每個編碼器層的輸出會傳遞給下一層，直到最後一層。

##### **1.2 編碼器的數學公式**

假設輸入序列為 \( X \)，每個位置的表示為 \( X_i \)，編碼器的計算過程可以分為以下幾步：

1. **多頭自注意力層**：

   對於每個自注意力頭 \( i \)，我們計算以下公式：

   \[
   \text{Attention}_i = \text{Softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i
   \]

   其中，\( Q_i, K_i, V_i \) 分別是查詢、鍵和值，並且是經過線性變換得到的。

   然後將所有注意力頭的結果進行拼接，並通過一個線性層進行映射。

2. **前向傳播層**：

   \[
   \text{FFN}(X) = \text{max}(0, XW_1 + b_1) W_2 + b_2
   \]

   其中，\( W_1, W_2 \) 是前向傳播層的權重矩陣，\( b_1, b_2 \) 是偏置項。

3. **殘差連接與層正則化**：

   最後，經過每層處理後，我們進行殘差連接並使用層正則化：

   \[
   \text{Output} = \text{LayerNorm}(X + \text{SubLayer}(X))
   \]

#### **2. 解碼器（Decoder）**

解碼器的任務是根據編碼器生成的隱含表示來生成輸出序列。解碼器的結構與編碼器類似，但它還引入了一個額外的機制，即**編碼器-解碼器注意力（Encoder-Decoder Attention）**。這個層允許解碼器利用編碼器的輸出來生成更準確的預測。

##### **2.1 解碼器的結構**

一個解碼器層由以下幾個部分組成：

1. **多頭自注意力層（Masked Multi-Head Self-Attention）**：
   - 解碼器的自注意力層會進行“遮掩”（masked）處理，確保每個位置只能關注它之前的位置，這樣可以防止模型提前看到未來的詞語（這是自回歸模型的要求）。

2. **編碼器-解碼器注意力層（Encoder-Decoder Attention）**：
   - 這個層通過對編碼器的輸出進行注意力計算，將編碼器學到的序列表示與解碼器當前的隱含狀態相結合。這樣解碼器可以依賴編碼器的輸出來生成預測。

3. **前向傳播層（Feed-Forward Neural Network）**：
   - 與編碼器層中的前向傳播層相同，該層用來進行非線性變換。

4. **殘差連接與層正則化（Residual Connection & Layer Normalization）**：
   - 同樣，解碼器層也包含殘差連接和層正則化來加速訓練並保持模型穩定。

##### **2.2 解碼器的數學公式**

解碼器的計算過程類似於編碼器，但需要額外處理編碼器-解碼器注意力層。對於每個解碼器層，計算過程如下：

1. **Masked Multi-Head Self-Attention**：

   \[
   \text{Attention}_i = \text{Softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i
   \]

   這裡的自注意力計算是“遮掩”的，即遮蔽未來的詞語。

2. **編碼器-解碼器注意力**：

   \[
   \text{Attention}_{\text{enc-dec}} = \text{Softmax}\left(\frac{Q_{\text{dec}} K_{\text{enc}}^T}{\sqrt{d_k}}\right) V_{\text{enc}}
   \]

   這裡的查詢來自解碼器的輸入，而鍵和值來自編碼器的輸出。

3. **前向傳播層**：

   \[
   \text{FFN}(X) = \text{max}(0, XW_1 + b_1) W_2 + b_2
   \]

4. **殘差連接與層正則化**：

   \[
   \text{Output} = \text{LayerNorm}(X + \text{SubLayer}(X))
   \]

#### **3. 編碼器與解碼器的協同作用**

編碼器和解碼器之間的協同作用是Transformer模型的核心。編碼器將輸入序列轉換為一組上下文相關的隱含表示，而解碼器則使用這些隱含表示來生成預測。這種結構使得Transformer能夠在各種序列到序列的任務（如機器翻譯、文本生成等）中取得優異的表現。

#### **4. 結論**

- **編碼器**將輸入序列映射為隱含表示，並捕捉序列中的依賴關係。
- **解碼器**根據編碼器的隱含表示生成輸出，並且通過自注意力和編碼器-解碼器注意力學習如何生成每一個詞。
- 這兩部分結合起來，使得Transformer成為一個強大的序列到序列模型。