### 1.3 策略與回報

在強化學習中，策略（Policy）和回報（Reward）是核心概念，決定了智能體如何選擇行為以及如何評估其行為的效果。本節將介紹策略的定義與表示，以及回報的計算與折扣因子的作用。

#### 1.3.1 策略的定義與表示

**策略（Policy）** 是智能體決定在每個狀態下應該採取何種行為的規則或映射。策略指引了智能體如何選擇行動，從而影響其與環境的互動過程。策略是強化學習問題中的基本概念，它決定了智能體的行為模式，並且是學習過程的核心。

策略可以分為兩類：
- **確定性策略（Deterministic Policy）**：在每個狀態下，智能體選擇一個確定的行為。對於狀態\(s\)，策略將行為\(a\)明確地給出，通常用符號\(\pi(s) = a\)來表示。
- **隨機策略（Stochastic Policy）**：在每個狀態下，智能體選擇行為的概率分佈而非確定的行為。這意味著在同一狀態下，智能體可能會選擇不同的行為，其概率由策略決定。隨機策略通常用\(\pi(a|s)\)表示，其中\(\pi(a|s)\)表示在狀態\(s\)下選擇行為\(a\)的概率。

數學上，策略\(\pi\)是一個映射：
- 確定性策略：\(\pi: S \to A\)，從狀態空間\(S\)到行為空間\(A\)的映射。
- 隨機策略：\(\pi: S \times A \to [0, 1]\)，從狀態空間\(S\)到行為空間\(A\)的概率分佈。

例如，在圍棋遊戲中，一個確定性策略可能是在每個棋盤配置下選擇唯一的下一步棋，而一個隨機策略則會在每個棋盤配置下，以一定概率選擇不同的下一步棋。

策略的目標是最大化智能體的回報，通過學習，智能體會調整其策略，使其在各種狀態下都能做出最優的選擇。

#### 1.3.2 回報與折扣因子

**回報（Reward）** 是強化學習中用來衡量智能體行為效果的即時反饋。在每一步的交互中，智能體執行某個行為，並根據這個行為獲得環境的回報。回報可以是正的或負的，表示行為是否有助於達成目標。

回報的作用是引導智能體的學習過程。智能體的最終目標是選擇那些能夠最大化其總回報的行為，這些回報的積累將指引智能體學會最優策略。

通常，強化學習的回報分為即時回報（Immediate Reward）和總回報（Cumulative Reward）：
- **即時回報（Immediate Reward）**：在某一時刻，智能體執行某一行為後所獲得的回報，記作\( r_t \)，它是環境對當前行為的反應。
- **總回報（Cumulative Reward）**：從當前時刻起，智能體在未來一段時間內所獲得的所有回報的總和。這是智能體學習的長期目標。

在強化學習中，我們通常會考慮**折扣回報**來計算總回報。這是因為智能體在做決策時，通常會更關注短期內能夠獲得的回報，而對長期回報的關注會隨著時間的推移而減弱。為了引入這種時間偏好，我們使用一個稱為**折扣因子（Discount Factor）**的參數來調整未來回報的權重。

- **折扣因子（Discount Factor）**：折扣因子通常用符號\(\gamma\)表示，取值範圍為\([0, 1]\)。折扣因子反映了未來回報的重要性。當\(\gamma\)接近1時，智能體會更加關注長期回報；而當\(\gamma\)接近0時，智能體則更關注短期回報。

折扣回報的計算公式為：
\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
\]
其中，\(G_t\)表示從時間步\(t\)開始的總回報，\(r_{t+k}\)是時間步\(t+k\)的即時回報，\(\gamma^k\)是對未來回報的折扣因子。

**折扣因子\(\gamma\)**的作用是使得智能體將長期回報的價值折算為較小的權重。當\(\gamma\)較小時，智能體會偏向短期的行為，而忽略長期回報。相反，當\(\gamma\)較大時，智能體則會注重長期回報，並更傾向於做出有長遠影響的行為。

例如，在機器人導航問題中，當\(\gamma = 0.9\)時，機器人會在選擇行為時將未來幾步的回報考慮進去，但會把越遠的回報折算為較低的價值。如果\(\gamma = 0.1\)，機器人則會更加重視當前的回報，而對未來的回報不那麼敏感。

### 小結

策略決定了智能體如何在不同狀態下選擇行為，而回報則是衡量智能體行為好壞的指標。折扣因子\(\gamma\)對回報的計算方式進行調整，幫助智能體平衡短期與長期回報。在強化學習中，智能體的目標是學習一個策略，使得其所獲得的總回報最大化。