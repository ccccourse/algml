### 6.3 策略梯度法與Actor-Critic方法

策略梯度方法與Actor-Critic方法是強化學習中的兩種重要算法，它們都旨在解決傳統Q學習方法在高維度問題中面臨的挑戰，並且直接在策略空間中進行優化。這些方法使用神經網絡來逼近策略或值函數，並通過梯度下降或其他優化技術來更新參數。以下將介紹策略梯度法的數學推導以及Actor-Critic架構的數學背景。

#### 6.3.1 策略梯度法數學推導

策略梯度方法是基於直接優化策略的思想，目標是最大化期望回報。與基於值的Q學習方法不同，策略梯度法的目標是學習一個參數化的策略函數 \( \pi_\theta(a|s) \)，其中 \( \pi_\theta(a|s) \) 代表在狀態 \( s \) 下選擇行為 \( a \) 的概率。這樣的策略是由參數 \( \theta \) 定義的，並且我們希望通過更新這些參數來提高回報。

策略梯度的基本思想是根據策略的性能來調整其參數，這樣可以使得策略選擇的行為更有可能帶來高回報。

##### 6.3.1.1 期望回報的計算

強化學習中的目標是最大化期望回報。對於一個策略 \( \pi_\theta \)，期望回報 \( J(\theta) \) 是所有可能的狀態-行為序列的加權回報的期望值：

\[
J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
\]

其中，\( r_t \) 是時間步 \( t \) 的回報，\( \gamma \) 是折扣因子，\( T \) 是回合的終止時間。

##### 6.3.1.2 策略的梯度

為了最大化期望回報，我們對策略參數 \( \theta \) 進行梯度上升。根據策略梯度定理，策略的梯度可以表示為：

\[
\nabla_\theta J(\theta) = \mathbb{E} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot G_t \right]
\]

其中：
- \( \nabla_\theta \log \pi_\theta(a|s) \) 是策略的對數梯度，表示對策略參數的變化率。
- \( G_t \) 是從時間步 \( t \) 開始的回報，通常使用回報的折扣總和來表示：

\[
G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k
\]

這樣，策略梯度法的基本步驟就是根據每個行為的梯度和相應的回報進行更新。這樣的策略更新可以使得代理傾向於選擇那些帶來更高回報的行為。

##### 6.3.1.3 策略梯度的估計

實際上，期望回報的計算通常難以精確求得，因此策略梯度的估計需要通過蒙特卡羅方法來進行。具體來說，我們可以通過多次交互和回合來估計梯度：

\[
\hat{\nabla_\theta J(\theta)} = \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \log \pi_\theta(a_i | s_i) \cdot G_i
\]

其中 \( N \) 是樣本數量，\( G_i \) 是第 \( i \) 次回合的回報。

#### 6.3.2 Actor-Critic架構的數學背景

Actor-Critic方法結合了基於值的學習和基於策略的學習，旨在通過兩個主要組件來解決強化學習中的挑戰：**Actor** 和 **Critic**。

- **Actor**：負責根據當前策略選擇行為，它學習的就是策略函數 \( \pi_\theta(a|s) \)，並且根據策略梯度法進行更新。
- **Critic**：負責估計狀態值函數或行為值函數，通常用一個Q函數或V函數來近似，並用來提供策略更新的“指導”信號。

Actor-Critic方法的核心思想是利用Critic來估計價值函數（通常是狀態價值函數 \( V^\pi(s) \) 或行為價值函數 \( Q^\pi(s, a) \)），然後根據這些估計來調整Actor的策略。

##### 6.3.2.1 策略更新

在Actor-Critic方法中，策略的更新依賴於Critic提供的價值函數。具體來說，策略的更新方向來自於策略梯度的估計，但這個估計是根據Critic的價值估計進行的。Actor的策略更新公式如下：

\[
\nabla_\theta J(\theta) = \mathbb{E} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot A_t \right]
\]

其中：
- \( A_t \) 是優勢函數（Advantage Function），用來衡量某個行為相比於平均行為的“優勢”程度。通常，優勢函數的估計可以使用價值函數來近似：

\[
A_t = G_t - V(s_t)
\]

其中 \( G_t \) 是回報，\( V(s_t) \) 是Critic估計的狀態價值函數。

##### 6.3.2.2 Critic的更新

Critic的目標是估計狀態價值或行為價值函數，並且它的更新是基於TD誤差（Temporal Difference Error）。對於狀態價值函數 \( V(s) \)，TD誤差定義為：

\[
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
\]

對於行為價值函數 \( Q(s, a) \)，TD誤差定義為：

\[
\delta_t = r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)
\]

Critic根據這些TD誤差來更新價值函數，通常使用梯度下降方法來最小化損失函數：

\[
L(\theta_C) = \frac{1}{2} \mathbb{E}[\delta_t^2]
\]

這裡 \( \theta_C \) 是Critic的參數。

##### 6.3.2.3 結合Actor和Critic

最終，Actor和Critic通過相互協作來達成強化學習的目標。Actor負責生成行為，Critic則負責提供這些行為的價值估計，並通過這些估計來指導Actor的學習。兩者交替更新，最終實現策略的優化和價值函數的收斂。

#### 小結

策略梯度法直接通過梯度上升最大化期望回報，而Actor-Critic方法則結合了基於策略和基於價值的學習。Actor-Critic方法在實際應用中具有很高的靈活性和效率，尤其在處理連續行為空間和高維度問題中表現突出。