以下是針對 **第2章** 主題所設計的簡短範例，其中根據內容需求，會採用純 Python、PyTorch 或是 Gym 實現。

---

## **2.1 隨機過程與馬爾可夫性**

### **2.1.1 馬爾可夫鏈**
馬爾可夫鏈的一個簡單範例：  
定義一個狀態轉移矩陣，並模擬過程。

```python
import numpy as np

# 狀態轉移矩陣 P
P = np.array([
    [0.7, 0.3],  # 狀態 0 -> 狀態 0 和 狀態 1
    [0.4, 0.6]   # 狀態 1 -> 狀態 0 和 狀態 1
])

# 初始狀態
state = 0
num_steps = 10

print("馬爾可夫鏈的狀態轉移過程:")
for t in range(num_steps):
    print(f"時間步 {t}: 狀態 {state}")
    state = np.random.choice([0, 1], p=P[state])
```

---

### **2.1.2 馬爾可夫決策過程（MDP）**
MDP 需要定義狀態、動作、獎勵和轉移機率。這裡是一個簡化版，使用 Python 定義一個小型 MDP 環境。

```python
import numpy as np

# 定義 MDP 組件
states = [0, 1, 2]  # 狀態空間
actions = [0, 1]    # 動作空間
rewards = {
    (0, 0): 1, (0, 1): 0,
    (1, 0): 0, (1, 1): 1,
    (2, 0): 1, (2, 1): 2
}
P = {  # 轉移機率
    (0, 0): 1, (0, 1): 2,
    (1, 0): 2, (1, 1): 0,
    (2, 0): 0, (2, 1): 1
}

state = 0
num_steps = 5

print("MDP 模擬過程:")
for t in range(num_steps):
    action = np.random.choice(actions)
    next_state = P[(state, action)]
    reward = rewards[(state, action)]
    print(f"狀態 {state}，執行動作 {action} -> 下個狀態 {next_state}，獎勵 {reward}")
    state = next_state
```

---

## **2.2 貝葉斯推斷與強化學習**
這裡用貝葉斯定理計算動作執行後的後驗機率。

```python
def bayes_posterior(prior, likelihood):
    posterior = [p * l for p, l in zip(prior, likelihood)]
    normalization = sum(posterior)
    return [p / normalization for p in posterior]

# 先驗機率
prior = [0.5, 0.5]  # 假設兩種動作的先驗機率相等

# 觀察到的數據的似然性
likelihood = [0.8, 0.4]

posterior = bayes_posterior(prior, likelihood)
print(f"後驗機率: {posterior}")
```

---

## **2.3 蒙特卡羅方法與估計**

### **2.3.1 蒙特卡羅回報估計**
模擬一個簡單的回報估計過程。

```python
import numpy as np

# 設定參數
num_episodes = 1000
discount_factor = 0.9

# 蒐集模擬的回報
returns = []
for _ in range(num_episodes):
    rewards = np.random.choice([1, -1, 0], size=10)  # 每個 episode 10 步，隨機回報
    G = 0  # 累積回報
    for t, r in enumerate(rewards):
        G += (discount_factor ** t) * r
    returns.append(G)

print(f"蒙特卡羅估計的平均回報: {np.mean(returns):.2f}")
```

---

### **2.3.2 蒙特卡羅方法與策略評估**  
使用 Gym 模擬一個簡單的策略評估範例。

```python
import gym
import numpy as np

env = gym.make("CartPole-v1")  # Gym 環境

num_episodes = 100
discount_factor = 0.99
returns = []

for _ in range(num_episodes):
    state = env.reset()[0]
    G = 0
    done = False
    t = 0
    
    while not done:
        action = env.action_space.sample()  # 隨機策略
        next_state, reward, done, _, _ = env.step(action)
        G += (discount_factor ** t) * reward
        t += 1
    
    returns.append(G)

print(f"平均累積回報 (策略評估): {np.mean(returns):.2f}")
```

---

這些範例展示了如何用簡單的 Python 代碼實現馬爾可夫鏈、MDP、貝葉斯推斷、蒙特卡羅方法和策略評估。如果有其他需求或希望更深入的實現，請告訴我！