以下是**第3章 動態規劃與最優控制**相關主題的範例代碼，使用簡單的 Python、PyTorch 和 Gym 示範。

---

## **3.1 動態規劃的基本概念**

### **3.1.1 值迭代 (Value Iteration)**  
值迭代算法計算最優價值函數，並從中獲得最優策略。以下為一個簡單的離散 MDP 值迭代實現：

```python
import numpy as np

# 定義 MDP 組件
states = [0, 1, 2, 3, 4]   # 狀態空間
actions = [0, 1]           # 動作空間 (左右移動)
gamma = 0.9                # 折扣因子
theta = 1e-6               # 收斂閾值

# 獎勵函數 R(s, a)
rewards = {
    (0, 0): 0, (0, 1): 0,
    (1, 0): 0, (1, 1): 0,
    (2, 0): 0, (2, 1): 1,
    (3, 0): 0, (3, 1): 0,
    (4, 0): 0, (4, 1): 0
}

# 狀態轉移函數 P(s'|s, a)
P = {
    (0, 0): 0, (0, 1): 1,
    (1, 0): 0, (1, 1): 2,
    (2, 0): 1, (2, 1): 3,
    (3, 0): 2, (3, 1): 4,
    (4, 0): 3, (4, 1): 4
}

# 值迭代算法
V = np.zeros(len(states))  # 初始化價值函數
while True:
    delta = 0
    for s in states:
        v = V[s]
        V[s] = max(rewards[(s, a)] + gamma * V[P[(s, a)]] for a in actions)
        delta = max(delta, abs(v - V[s]))
    if delta < theta:
        break

# 根據價值函數計算最優策略
policy = {}
for s in states:
    policy[s] = np.argmax([rewards[(s, a)] + gamma * V[P[(s, a)]] for a in actions])

print("最優價值函數:", V)
print("最優策略:", policy)
```

---

### **3.1.2 策略迭代 (Policy Iteration)**  
策略迭代由策略評估和策略改善組成。

```python
import numpy as np

# 定義 MDP 組件
states = [0, 1, 2, 3, 4]  # 狀態空間
actions = [0, 1]          # 動作空間
gamma = 0.9
theta = 1e-6

rewards = {  # 獎勵函數
    (0, 0): 0, (0, 1): 0,
    (1, 0): 0, (1, 1): 0,
    (2, 0): 0, (2, 1): 1,
    (3, 0): 0, (3, 1): 0,
    (4, 0): 0, (4, 1): 0
}

P = {  # 狀態轉移
    (0, 0): 0, (0, 1): 1,
    (1, 0): 0, (1, 1): 2,
    (2, 0): 1, (2, 1): 3,
    (3, 0): 2, (3, 1): 4,
    (4, 0): 3, (4, 1): 4
}

# 策略迭代算法
V = np.zeros(len(states))  # 初始化價值函數
policy = np.zeros(len(states), dtype=int)  # 初始化策略

while True:
    # 策略評估
    while True:
        delta = 0
        for s in states:
            v = V[s]
            a = policy[s]
            V[s] = rewards[(s, a)] + gamma * V[P[(s, a)]]
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break
    
    # 策略改善
    policy_stable = True
    for s in states:
        old_action = policy[s]
        policy[s] = np.argmax([rewards[(s, a)] + gamma * V[P[(s, a)]] for a in actions])
        if old_action != policy[s]:
            policy_stable = False
    if policy_stable:
        break

print("最優價值函數:", V)
print("最優策略:", policy)
```

---

## **3.2 貝爾曼方程**

### **3.2.1 值函數與 Q 函數**
實現一個 Q 函數更新的範例，利用貝爾曼方程計算 Q 值。

```python
import numpy as np

# 狀態空間與動作空間
states = [0, 1]
actions = [0, 1]
gamma = 0.9
alpha = 0.1  # 學習率

# 初始化 Q 函數
Q = np.zeros((len(states), len(actions)))

# 更新 Q 函數
for episode in range(1000):
    state = np.random.choice(states)
    action = np.random.choice(actions)
    reward = np.random.choice([0, 1])
    next_state = np.random.choice(states)
    
    # 貝爾曼更新
    Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])

print("學習到的 Q 函數:")
print(Q)
```

---

## **3.3 最優控制與強化學習的關聯**
強化學習中的最優控制可透過 Gym 環境實現。以下範例展示用 PyTorch 訓練一個策略網路。

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim

# 建立環境
env = gym.make("CartPole-v1")

# 策略網路
class PolicyNet(nn.Module):
    def __init__(self, state_size, action_size):
        super(PolicyNet, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        return self.fc(x)

# 初始化策略網路
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
policy_net = PolicyNet(state_size, action_size)
optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)

# 訓練策略
for episode in range(500):
    state = env.reset()[0]
    done = False
    while not done:
        state_tensor = torch.FloatTensor(state)
        action_probs = policy_net(state_tensor)
        action = torch.multinomial(action_probs, 1).item()
        next_state, reward, done, _, _ = env.step(action)
        state = next_state

print("策略網路訓練完成!")
```

---

這些範例涵蓋了動態規劃、貝爾曼方程和最優控制，並展示了值迭代、策略迭代及 Q 函數更新的過程。若有更多細節或擴展需求，請告訴我！