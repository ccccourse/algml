以下是**第4章 優化方法與強化學習**各主題的簡單程式碼範例，示範如何用 Python、PyTorch 和 Gym 實現相關概念。

---

## **4.1 梯度下降與最優策略**

### **4.1.1 基本的梯度下降方法**

以下是簡單的梯度下降示例，最小化一個二次函數 \( f(x) = (x - 3)^2 \)。

```python
import torch

# 初始化參數
x = torch.tensor([0.0], requires_grad=True)  # 初始值
optimizer = torch.optim.SGD([x], lr=0.1)  # 使用 SGD 優化器

# 梯度下降循環
for i in range(100):
    optimizer.zero_grad()
    loss = (x - 3) ** 2  # 目標函數
    loss.backward()  # 計算梯度
    optimizer.step()  # 更新 x 的值

print("最優解:", x.item())  # x 應該接近 3
```

---

### **4.1.2 策略梯度方法**

策略梯度法利用梯度更新策略參數。在 Gym 的 `CartPole-v1` 環境中，以下範例訓練一個簡單的策略網路。

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim

# 建立環境
env = gym.make('CartPole-v1')

# 策略網路
class PolicyNet(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        return self.fc(x)

# 初始化策略網路
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
policy_net = PolicyNet(state_dim, action_dim)
optimizer = optim.Adam(policy_net.parameters(), lr=1e-2)

# 策略梯度訓練
for episode in range(500):
    state = env.reset()[0]
    done = False
    log_probs = []
    rewards = []
    while not done:
        state_tensor = torch.FloatTensor(state)
        action_probs = policy_net(state_tensor)
        action = torch.multinomial(action_probs, 1).item()
        log_prob = torch.log(action_probs[action])
        log_probs.append(log_prob)
        
        next_state, reward, done, _, _ = env.step(action)
        rewards.append(reward)
        state = next_state
    
    # 計算累積回報
    total_reward = sum(rewards)
    loss = -total_reward * torch.stack(log_probs).sum()
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("策略訓練完成！")
```

---

## **4.2 近似動態規劃與價值函數近似**

### **4.2.1 線性回歸與非線性近似**

這是一個簡單的線性回歸示例，用 PyTorch 訓練模型來近似一個價值函數。

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 隨機生成訓練數據
x = torch.rand(100, 1)  # 狀態特徵
y = 3 * x + 2 + 0.1 * torch.randn(100, 1)  # 真實價值函數: y = 3x + 2

# 定義模型
model = nn.Linear(1, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# 訓練模型
for epoch in range(1000):
    y_pred = model(x)
    loss = criterion(y_pred, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("學到的權重:", model.weight.item(), "學到的偏置:", model.bias.item())
```

---

### **4.2.2 神經網絡在強化學習中的應用**

用神經網路進行價值函數近似。

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim

# 建立環境
env = gym.make('CartPole-v1')

# 定義價值網路
class ValueNet(nn.Module):
    def __init__(self, state_dim):
        super(ValueNet, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        return self.fc(x)

# 初始化
state_dim = env.observation_space.shape[0]
value_net = ValueNet(state_dim)
optimizer = optim.Adam(value_net.parameters(), lr=1e-2)
criterion = nn.MSELoss()

# 訓練價值網路
for episode in range(500):
    state = env.reset()[0]
    done = False
    while not done:
        state_tensor = torch.FloatTensor(state)
        value = value_net(state_tensor)
        next_state, reward, done, _, _ = env.step(env.action_space.sample())
        
        next_state_tensor = torch.FloatTensor(next_state)
        target = reward + 0.9 * value_net(next_state_tensor).detach()
        loss = criterion(value, target)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        state = next_state

print("價值網路訓練完成！")
```

---

## **4.3 隨機最優化方法**

### **4.3.1 高斯過程 (示意隨機取樣)**

這裡使用簡單的高斯噪聲來進行隨機取樣並優化目標函數。

```python
import numpy as np

def objective_function(x):
    return -(x - 3)**2 + 10

# 隨機取樣優化
best_x = None
best_value = -np.inf

for i in range(1000):
    x = np.random.normal(0, 2)  # 高斯分佈取樣
    value = objective_function(x)
    if value > best_value:
        best_x = x
        best_value = value

print(f"最優解: x = {best_x}, value = {best_value}")
```

---

### **4.3.2 隨機梯度上升**

用隨機梯度上升法最大化一個目標函數。

```python
import torch

# 目標函數
def objective_function(x):
    return -x**2 + 4*x

x = torch.tensor([0.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.1)

# 隨機梯度上升
for _ in range(100):
    optimizer.zero_grad()
    loss = -objective_function(x)  # 最大化目標函數，取負號
    loss.backward()
    optimizer.step()

print("最優解:", x.item())
```

---

這些範例涵蓋了梯度下降、策略梯度、價值函數近似與隨機最優化的實現。如果有需要更深入的講解或其他主題擴展，請告訴我！