
#### 2.1 隨機過程與馬爾可夫性

強化學習的本質是對環境的探索與利用，而這種探索通常是在隨機環境中進行的。環境的隨機性和不確定性使得馬爾可夫過程（Markov Process）成為建模和理解強化學習問題的核心概念。理解隨機過程和馬爾可夫性有助於理解強化學習中的決策過程。

##### 2.1.1 馬爾可夫鏈

馬爾可夫鏈（Markov Chain）是一種隨機過程，它的特點是“無記憶性”，即當前狀態的轉移只與當前狀態有關，而與過去的歷史無關。這一特性被稱為馬爾可夫性（Markov Property），其數學表述如下：

對於一個隨機過程\(\{X_t\}\)，若滿足
\[
P(X_{t+1} | X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
\]
則稱該過程是馬爾可夫過程，或者是具有馬爾可夫性的過程。這意味著，系統的未來狀態完全由當前狀態決定，而與過去的歷史無關。

馬爾可夫鏈是一種特殊的隨機過程，其中每個狀態的轉移都由一個概率矩陣表示。假設有\(N\)個狀態，則轉移概率矩陣\(\mathbf{P}\)的元素\(P_{ij}\)表示從狀態\(i\)轉移到狀態\(j\)的概率：
\[
P_{ij} = P(X_{t+1} = j | X_t = i)
\]

馬爾可夫鏈的穩定性可以通過**平穩分佈**來理解。當馬爾可夫鏈達到平衡時，系統的狀態分佈不再改變，這時的狀態分佈稱為**平穩分佈**，並且滿足：
\[
\pi_j = \sum_i \pi_i P_{ij}
\]
其中，\(\pi_i\)是狀態\(i\)的平穩概率，\(P_{ij}\)是從狀態\(i\)到狀態\(j\)的轉移概率。

在強化學習中，馬爾可夫鏈的概念幫助我們理解智能體在環境中的狀態轉移，並且是馬爾可夫決策過程的基礎。

##### 2.1.2 馬爾可夫決策過程（MDP）

馬爾可夫決策過程（MDP，Markov Decision Process）是強化學習中的核心數學框架，它擴展了馬爾可夫鏈的概念，並引入了決策行為（action）和回報（reward）元素。MDP為描述智能體與環境互動的過程提供了形式化的模型。

MDP由五個元素組成：
1. **狀態空間（S）**：描述環境的所有可能狀態的集合。每個狀態\(s \in S\)代表環境的某一具體情況。
2. **行為空間（A）**：描述智能體在每個狀態下可以選擇的所有可能行為的集合。每個行為\(a \in A(s)\)是從狀態\(s\)出發，智能體可以選擇的操作。
3. **轉移機率（P）**：描述在某一狀態下，執行某個行為後，轉移到另一狀態的概率。轉移機率\(P(s'|s, a)\)表示在狀態\(s\)執行行為\(a\)後轉移到狀態\(s'\)的概率。
4. **回報函數（R）**：每執行一個行為後，環境給智能體的即時回報。回報函數\(R(s, a)\)表示在狀態\(s\)下執行行為\(a\)所獲得的回報。
5. **折扣因子（\(\gamma\)）**：衡量未來回報的相對重要性，\(\gamma\)的取值範圍是[0, 1]。當\(\gamma = 1\)時，智能體關心所有未來的回報；當\(\gamma = 0\)時，智能體只關心當前的回報。

在馬爾可夫決策過程中，智能體的目標是通過選擇一個策略\(\pi\)來最大化長期回報。策略\(\pi\)是從每個狀態\(s\)出發，選擇相應行為\(a\)的規則。

MDP的核心概念是**回報的期望**。如果智能體從狀態\(s_0\)出發，遵循策略\(\pi\)，則智能體在時間步\(t\)的回報\(G_t\)可以表示為：
\[
G_t = R(s_t, a_t) + \gamma R(s_{t+1}, a_{t+1}) + \gamma^2 R(s_{t+2}, a_{t+2}) + \dots
\]
智能體的目標是最大化回報的期望值，即：
\[
V^\pi(s) = \mathbb{E}\left[G_t | s_t = s, \pi\right]
\]
這裡，\(V^\pi(s)\)是策略\(\pi\)在狀態\(s\)下的價值函數，它表示從狀態\(s\)開始，遵循策略\(\pi\)所能獲得的期望總回報。

#### 小結

在強化學習中，馬爾可夫鏈和馬爾可夫決策過程提供了處理不確定性和決策過程的數學基礎。馬爾可夫鏈的“無記憶性”特性使得每個狀態的轉移只與當前狀態相關，而馬爾可夫決策過程則進一步引入了智能體的行為和回報，形成了一個完整的框架，用於建模智能體在隨機環境中的學習過程。了解這些基本概念對深入理解強化學習算法及其應用至關重要。