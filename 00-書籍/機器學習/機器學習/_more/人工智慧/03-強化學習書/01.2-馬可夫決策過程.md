### 1.2 馬爾可夫決策過程（MDP）

馬爾可夫決策過程（Markov Decision Process，簡稱MDP）是強化學習中的數學模型，用來描述智能體與環境之間的互動。MDP提供了一個框架，幫助我們理解和分析強化學習問題，並為智能體的學習過程提供數學基礎。

在MDP中，環境的動態行為依賴於智能體的決策，這些決策通過特定的規則影響環境的演化。馬爾可夫決策過程的關鍵特徵是**馬爾可夫性**，即系統的未來狀態只依賴於當前狀態，而與過去的歷史無關。

#### 1.2.1 狀態與行為

- **狀態（State）**：狀態描述了環境在某一時刻的具體情況。狀態包含了環境所需的所有信息，以便智能體根據該狀態選擇合適的行為。狀態通常用符號\( s \)表示，並來自於一個稱為**狀態空間（State Space）**的集合，這是一個描述所有可能狀態的集合。

  例如，在一個機器人導航問題中，狀態可以表示機器人的位置、速度以及周圍的障礙物情況。在圍棋遊戲中，狀態可以表示棋盤的當前布局。

- **行為（Action）**：行為是智能體根據當前狀態選擇的一個動作，它會對環境造成影響，並引發狀態的變化。行為通常用符號\( a \)表示，並來自於一個稱為**行為空間（Action Space）**的集合，這是一個描述智能體在每個狀態下可以採取的所有行為的集合。

  在機器人導航的例子中，行為可以是「向前移動」、「向左轉」、「向右轉」等。在圍棋遊戲中，行為就是選擇一個棋盤上的位置放置棋子。

在每一個時間步，智能體根據當前的狀態\( s_t \)選擇一個行為\( a_t \)，並執行這個行為。這樣，環境的狀態將根據這個行為發生變化，並進入下一個狀態\( s_{t+1} \)。

#### 1.2.2 轉移機率與回報

- **轉移機率（Transition Probability）**：轉移機率描述了環境如何從一個狀態轉換到另一個狀態，並且是強化學習中最重要的概念之一。轉移機率由\( P(s'|s, a) \)表示，表示在當前狀態\( s \)下執行行為\( a \)後，系統進入狀態\( s' \)的概率。

  轉移機率滿足以下條件：
  - **馬爾可夫性**：系統的未來狀態僅依賴於當前狀態和行為，與過去的狀態和行為無關。即：
    \[
    P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots)
    \]
  - **隨機性**：轉移機率通常是隨機的，即使在相同的狀態和行為下，每次轉移也可能會進入不同的狀態。這意味著，強化學習問題中的環境是隨機的，具有一定的不可預測性。

  例如，在機器人導航問題中，如果機器人執行「向前移動」的行為，它可能會到達預期的位置，也可能由於外部干擾（如地面不平）而偏離預期位置。這種偏差由轉移機率來描述。

- **回報（Reward）**：回報是智能體根據其行為和環境反應獲得的即時反饋。回報可以是正的（代表成功）或負的（代表失敗），並且它通常用符號\( r_t \)表示。

  在強化學習中，回報是智能體學習的主要驅動力，智能體的目標是最大化其累積回報。回報與狀態和行為有關，對於每一個時間步，智能體在執行行為\( a_t \)後，會根據當前的狀態\( s_t \)獲得一個回報\( r_t \)。

  回報的設計至關重要，因為它直接影響智能體的學習過程。在某些情況下，回報可能是簡單的數字，而在其他情況下，它可能涉及更複雜的計算，例如根據多個因素來計算回報。

  例如，在圍棋遊戲中，智能體可能會因為在棋盤上放置了一個成功的棋子而獲得正回報，或者因為失敗的行為（如被對方吃掉棋子）而得到負回報。在機器人導航問題中，正回報可以來自到達目標位置，而負回報則來自撞牆或偏離目標。

### 小結

馬爾可夫決策過程（MDP）是強化學習的基礎模型，它描述了智能體如何在一個隨機的環境中進行學習和決策。狀態、行為、轉移機率和回報是MDP的四個關鍵要素，這些要素構成了強化學習的核心。在這個框架下，智能體的目標是學習一個最優策略，使得在每個狀態下選擇的行為能夠最大化其期望的長期回報。