### 6.2 深度Q網絡（DQN）

深度Q網絡（DQN, Deep Q-Network）是將深度學習應用於Q學習的一個突破性方法，解決了傳統Q學習在高維度狀態空間中的問題。Q學習（Q-Learning）是強化學習中的一種無模型學習方法，它學習一個Q函數來表示在某一狀態下執行某一行為所能獲得的最大回報。DQN則利用神經網絡來近似Q函數，從而能夠處理大量且複雜的狀態空間。

#### 6.2.1 Q學習與深度學習的結合

Q學習是強化學習中的一種基於值的方法，其目的是學習一個行為值函數Q(s, a)，表示在狀態s下採取行為a所能期望獲得的回報。Q學習的基本思想是根據經驗來更新Q值，並利用Q值來決策最優策略。

Q學習的更新公式如下：

\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
\]

其中：
- \( Q(s_t, a_t) \) 是在時間步 \( t \) 執行動作 \( a_t \) 在狀態 \( s_t \) 下的Q值。
- \( \alpha \) 是學習率，決定了新信息的權重。
- \( r_t \) 是在步驟 \( t \) 觀察到的回報。
- \( \gamma \) 是折扣因子，表示對未來回報的重視程度。
- \( \max_{a'} Q(s_{t+1}, a') \) 是在下一狀態 \( s_{t+1} \) 下的最大Q值。

然而，當狀態空間和行為空間非常大（例如，在影像或其他高維度輸入中），傳統的Q學習難以處理，因為需要為每一對狀態和行為維護一個Q值表。這時，深度學習被引入來近似Q函數。

在DQN中，我們使用神經網絡來近似Q函數，具體來說，神經網絡學習一個映射：

\[
Q_{\theta}(s, a) \approx Q(s, a)
\]

其中，\( \theta \) 是神經網絡的參數。這樣，神經網絡就可以處理高維度的狀態空間，並且自動學習如何表示狀態和行為之間的複雜關係。

#### 6.2.2 DQN算法及其數學推導

DQN算法是Q學習的一種深度學習擴展，它利用深度神經網絡來近似Q值函數，並使用幾種技術來穩定訓練過程。這些技術包括經驗回放（Experience Replay）、目標網絡（Target Network）等。

##### 6.2.2.1 目標網絡與經驗回放

1. **經驗回放（Experience Replay）**：
   在傳統的Q學習中，每次更新Q值時會依賴當前的狀態和行為，但這會導致訓練過程中的數據相關性過高，從而使學習過程不穩定。DQN使用經驗回放來打破這種相關性。具體地，DQN會將智能體在環境中經歷的每一個狀態轉移（狀態、行為、回報、下一狀態）存儲到一個經驗池中。每次從經驗池中隨機抽取一小批樣本來進行訓練，這樣可以提高樣本的多樣性，並減少訓練過程中的偏差。

2. **目標網絡（Target Network）**：
   目標網絡是Q學習中的一種穩定化技術。在DQN中，我們維護兩個Q網絡：
   - 一個是**主網絡**（Online Network），用來估計當前的Q值。
   - 另一個是**目標網絡**（Target Network），用來計算目標Q值。在每一段時間更新時，目標網絡的參數會周期性地從主網絡拷貝過來。

   使用目標網絡的目的是減少Q值更新過程中的波動，使得學習過程更加穩定。目標網絡的更新公式如下：

   \[
   \hat{y}_t = r_t + \gamma \max_{a'} Q_{\theta^-}(s_{t+1}, a')
   \]

   其中，\( Q_{\theta^-} \) 是目標網絡的參數，\( \hat{y}_t \) 是DQN中的目標Q值。

##### 6.2.2.2 DQN的更新規則

DQN的目標是最小化Q值函數的損失函數，這個損失函數衡量了當前網絡輸出的Q值與目標Q值之間的差異。具體來說，我們使用均方誤差（Mean Squared Error, MSE）來計算損失：

\[
L(\theta) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim D} \left[ \left( y_t - Q_{\theta}(s_t, a_t) \right)^2 \right]
\]

其中：
- \( y_t = r_t + \gamma \max_{a'} Q_{\theta^-}(s_{t+1}, a') \) 是目標Q值（使用目標網絡計算）。
- \( Q_{\theta}(s_t, a_t) \) 是使用主網絡計算的當前Q值。
- \( D \) 是經驗回放池中的樣本分佈。

每次訓練過程中，我們隨機抽取一小批經驗樣本來計算損失，並對主網絡進行梯度下降優化：

\[
\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)
\]

其中 \( \alpha \) 是學習率，並且 \( \nabla_{\theta} L(\theta) \) 是損失函數相對於網絡參數的梯度。

##### 6.2.2.3 DQN的訓練過程

1. 初始化主網絡和目標網絡，並設置經驗回放池。
2. 在每一回合中，智能體與環境交互，並將每個時間步的經驗（狀態、行為、回報、下一狀態）存入經驗回放池。
3. 隨機從經驗池中抽取一小批樣本，計算目標Q值 \( y_t \)，並使用均方誤差損失函數來更新主網絡。
4. 每隔固定的步數，將主網絡的參數複製到目標網絡。
5. 重複訓練，直到收斂。

#### 小結

DQN通過將深度學習與Q學習結合，成功地處理了高維度狀態空間中的問題，並且引入了經驗回放和目標網絡等穩定化技術來改善訓練的穩定性和效率。這些技術使得DQN成為解決複雜強化學習問題的有效工具，並在許多領域取得了顯著的成果。
