以下是針對各主題的**簡單 Python 範例**，能用基本 Python 實現的地方，就不會加入 Gym 和 PyTorch。這樣更方便理解基礎概念。

---

## **第1章 強化學習基礎**

### **1.1 強化學習簡介**

強化學習的基礎是**狀態**、**行動**、**回報**三要素。以下是一個簡單的循環，展示如何根據回報進行學習：

```python
import random

# 簡單的環境：兩個行動 (0: 左, 1: 右)
actions = [0, 1]

# 初始狀態與目標
state = 0
goal = 5

for episode in range(3):
    print(f"\nEpisode {episode+1}")
    state = 0  # 每次回到初始狀態
    while state != goal:
        action = random.choice(actions)  # 隨機選擇行動
        if action == 1:  # 向右移動
            state += 1
        else:  # 向左移動
            state = max(0, state - 1)  # 避免低於 0
        
        reward = 1 if state == goal else 0
        print(f"State: {state}, Action: {action}, Reward: {reward}")
```

### **解釋**：
- 狀態：位置 `state`
- 行動：左右移動
- 回報：到達 `goal` 時獲得 `reward = 1`

---

### **1.2 馬爾可夫決策過程（MDP）**

#### **1.2.1 狀態與行為**

展示狀態轉移與行動的影響：

```python
# 狀態轉移表
transitions = {
    0: {0: 0, 1: 1},  # 狀態 0: 左 -> 0, 右 -> 1
    1: {0: 0, 1: 2},  # 狀態 1: 左 -> 0, 右 -> 2
    2: {0: 1, 1: 3},  # ...
    3: {0: 2, 1: 4},
    4: {0: 3, 1: 4},  # 到達終點
}

state = 0
actions = [0, 1]  # 0: 左, 1: 右

print("狀態轉移示例：")
for t in range(6):
    action = random.choice(actions)
    next_state = transitions[state][action]
    print(f"Time: {t}, State: {state}, Action: {action}, Next State: {next_state}")
    state = next_state
```

---

#### **1.2.2 轉移機率與回報**

假設一個簡單的隨機轉移模型：

```python
import random

# 定義轉移機率與回報
P = {  # 狀態轉移機率
    0: {0: [(0.8, 0), (0.2, 1)], 1: [(0.1, 0), (0.9, 1)]},
    1: {0: [(0.7, 0), (0.3, 1)], 1: [(0.2, 1), (0.8, 2)]},
}

state = 0
for t in range(5):
    action = 1  # 固定選擇向右
    probs = P[state][action]  # 取得轉移機率
    next_state = random.choices([s for _, s in probs], weights=[p for p, _ in probs])[0]
    print(f"Time: {t}, State: {state}, Action: {action}, Next State: {next_state}")
    state = next_state
```

### **解釋**：
- 轉移機率：隨機選擇下一個狀態，基於指定機率。

---

### **1.3 策略與回報**

#### **1.3.2 回報與折扣因子**

計算回報的總和，加入折扣因子 \( \gamma \)：

```python
gamma = 0.9  # 折扣因子
rewards = [1, 0, 0, 1]  # 假設一個 episode 的回報

G = 0  # 累計折扣回報
for t, reward in enumerate(reversed(rewards)):
    G = reward + gamma * G
    print(f"Step {len(rewards)-1 - t}, Reward: {reward}, Discounted Return: {G:.2f}")
```

---

## **第2章 機率論與統計在強化學習中的應用**

### **2.1.1 馬爾可夫鏈**

簡單展示馬爾可夫鏈的隨機轉移特性：

```python
import random

states = [0, 1, 2, 3]
transitions = {0: [0.5, 0.5, 0, 0], 1: [0.3, 0.3, 0.4, 0], 2: [0, 0.5, 0.5, 0], 3: [0, 0, 0, 1]}

state = 0
print("馬爾可夫鏈隨機轉移：")
for t in range(10):
    probs = transitions[state]
    state = random.choices(states, weights=probs)[0]
    print(f"Step {t}, State: {state}")
```

---

### **2.3.1 蒙特卡羅回報估計**

計算一個 episode 的回報估計：

```python
gamma = 0.9
rewards = [1, 2, 3, 4]  # 一個 episode 的回報
G = 0
returns = []

for reward in reversed(rewards):
    G = reward + gamma * G
    returns.insert(0, G)

print("每一步的回報估計：")
for t, G in enumerate(returns):
    print(f"Step {t}, Return: {G:.2f}")
```

---

### **2.3.2 策略評估**

估算每個狀態的價值：

```python
states = [0, 1, 2]
gamma = 0.9
state_values = {s: 0 for s in states}
rewards = {0: 1, 1: 2, 2: 3}
transitions = {0: [1, 1], 1: [2, 2], 2: [2, 2]}

# 更新價值函數
for _ in range(10):  # 迭代 10 次
    for state in states:
        next_states = transitions[state]
        state_values[state] = rewards[state] + gamma * sum(state_values[s] for s in next_states) / len(next_states)

print("狀態價值估計：")
for state, value in state_values.items():
    print(f"State {state}: Value = {value:.2f}")
```

---

這些範例簡單直觀，幫助理解強化學習中基礎概念與數學計算。若有進一步需求，再結合 PyTorch 與 Gym。