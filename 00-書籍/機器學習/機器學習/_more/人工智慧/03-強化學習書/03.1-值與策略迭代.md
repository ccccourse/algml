
#### 3.1 動態規劃的基本概念

動態規劃是由理查德·貝爾曼（Richard Bellman）提出的一種數學方法，旨在解決具有重疊子問題的最優化問題。在強化學習中，動態規劃通常涉及使用已知的環境模型（如轉移概率和回報函數）來計算最優策略和最優回報。動態規劃的基本思路是自底向上地解決問題，即從小的子問題開始，逐步構建最優解。

在強化學習中，動態規劃的核心是**貝爾曼方程**，該方程描述了某一狀態的價值與下一步狀態的價值之間的關係。對於值函數\(V(s)\)和行為價值函數\(Q(s, a)\)，貝爾曼方程的形式分別為：

- **值函數的貝爾曼方程**：
  
  \[
  V(s) = \max_a \left( \sum_{s'} P(s'|s, a) [ R(s, a, s') + \gamma V(s') ] \right)
  \]

- **行為價值函數的貝爾曼方程**：

  \[
  Q(s, a) = \sum_{s'} P(s'|s, a) [ R(s, a, s') + \gamma \max_{a'} Q(s', a') ]
  \]

其中，\(P(s'|s, a)\)是狀態轉移概率，\(R(s, a, s')\)是即時回報，\(\gamma\)是折扣因子，\(V(s)\)和\(Q(s, a)\)分別是狀態和狀態-行為對的價值函數。

動態規劃的目標是解這些方程，從而得到最優策略和最優回報。

#### 3.1.1 值迭代

值迭代是一種基於貝爾曼方程的動態規劃方法，用於計算最優價值函數。值迭代通過迭代地更新每個狀態的價值，最終收斂到最優價值函數。在每次迭代中，值迭代算法會根據當前的價值函數估計來更新每個狀態的價值，直到收斂為止。

具體步驟如下：
1. **初始化**：隨機初始化每個狀態的價值函數\(V(s)\)，通常可以設置為零或隨機數。
2. **迭代更新**：對每個狀態\(s\)，計算所有可能行為\(a\)的期望回報，並選擇其中的最大值來更新\(V(s)\)：

   \[
   V(s) \leftarrow \max_a \left( \sum_{s'} P(s'|s, a) [ R(s, a, s') + \gamma V(s') ] \right)
   \]

3. **收斂判斷**：當所有狀態的價值函數的變化小於某個預設的閾值時，停止迭代，認為算法已經收斂。

值迭代的核心思想是基於貝爾曼方程的遞推關係，通過不斷地估計並更新價值函數，逐步逼近最優解。它的優點是簡單且易於理解，並且可以提供精確的最優解，特別是在環境模型已知的情況下。

#### 3.1.2 策略迭代

策略迭代是另一種動態規劃方法，它包括兩個主要步驟：策略評估和策略改進。在每次迭代中，策略迭代算法會首先對當前策略進行評估，然後根據評估結果來改進策略。通過交替進行這兩個步驟，最終可以收斂到最優策略。

策略迭代的步驟如下：

1. **初始化**：隨機初始化策略\(\pi\)，通常可以設置為隨機行為選擇。
2. **策略評估**：對於當前策略\(\pi\)，使用動態規劃計算每個狀態的價值函數\(V^\pi(s)\)，即計算在策略\(\pi\)下每個狀態的期望回報。

   策略評估的更新公式如下：

   \[
   V^\pi(s) = \sum_{s'} P(s'|s, \pi(s)) [ R(s, \pi(s), s') + \gamma V^\pi(s') ]
   \]

3. **策略改進**：根據計算出的價值函數\(V^\pi(s)\)，對策略\(\pi\)進行改進，選擇每個狀態\(s\)下能夠使回報最大化的行為\(a\)，從而更新策略。

   策略改進的更新公式為：

   \[
   \pi'(s) = \arg\max_a \left( \sum_{s'} P(s'|s, a) [ R(s, a, s') + \gamma V^\pi(s') ] \right)
   \]

4. **收斂判斷**：當策略不再改變時，算法停止，並且當前策略就是最優策略。

策略迭代的關鍵在於它通過交替進行策略評估和策略改進來不斷逼近最優策略。與值迭代相比，策略迭代的收斂速度較快，因為每次迭代後策略都會明確改進。

#### 小結

動態規劃是求解強化學習問題中最優策略和最優回報的基本方法，特別適用於已知環境模型的情況。值迭代和策略迭代是兩種經典的動態規劃方法，各有其特點。值迭代通過不斷更新價值函數來計算最優策略，而策略迭代則通過交替進行策略評估和策略改進來實現最優策略的學習。在實際應用中，根據具體的問題和計算資源的限制，可以選擇合適的動態規劃方法來求解強化學習問題。