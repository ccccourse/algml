以下是**第6章 深度強化學習的數學基礎**的各主題簡要說明與對應程式碼範例，涵蓋深度 Q 網絡（DQN）與策略梯度法（Actor-Critic）的核心內容。

---

## **6.1 神經網絡與深度學習概述**

在深度強化學習中，神經網絡用於近似策略函數或值函數。以下是一個簡單的神經網絡範例：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 神經網絡架構 (輸入層 -> 隱藏層 -> 輸出層)
class SimpleNet(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, output_size)
        )
    
    def forward(self, x):
        return self.fc(x)

# 建立網絡
input_size = 4  # 假設輸入維度
output_size = 2  # 假設輸出維度（兩個動作）
net = SimpleNet(input_size, output_size)

# 測試網絡
sample_input = torch.FloatTensor([1.0, 0.5, -0.5, -1.0])
output = net(sample_input)
print("網絡輸出:", output)
```

---

## **6.2 深度Q網絡（DQN）**

### **6.2.1 Q學習與深度學習的結合**

DQN 通過神經網絡來近似 Q 函數，將 Q-Learning 與深度學習結合。以下是 DQN 的核心實現步驟。

1. **使用神經網絡表示 Q 函數**。
2. **採用經驗回放（Replay Buffer）** 減少相關性。
3. **使用目標網絡（Target Network）** 提高訓練穩定性。

### **6.2.2 DQN 算法及其數學推導**

數學公式：  
\[ Q(s, a) = r + \gamma \max_{a'} Q(s', a') \]  
其中，  
- \( Q(s, a) \) 是當前 Q 函數值，  
- \( r \) 是即時獎勵，  
- \( \gamma \) 是折扣因子。

以下是 DQN 的程式碼範例：

```python
import gym
import random
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque

# Q網絡
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, x):
        return self.fc(x)

# 訓練參數
env = gym.make("CartPole-v1")
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

q_network = QNetwork(state_dim, action_dim)
target_network = QNetwork(state_dim, action_dim)
target_network.load_state_dict(q_network.state_dict())

optimizer = optim.Adam(q_network.parameters(), lr=0.001)
replay_buffer = deque(maxlen=10000)
gamma = 0.99
batch_size = 64

# 經驗回放
def sample_experiences():
    batch = random.sample(replay_buffer, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)
    return (
        torch.FloatTensor(states),
        torch.LongTensor(actions),
        torch.FloatTensor(rewards),
        torch.FloatTensor(next_states),
        torch.FloatTensor(dones)
    )

# 訓練迴圈
for episode in range(500):
    state = env.reset()[0]
    done = False
    while not done:
        # 選擇行動 (ε-貪婪策略)
        epsilon = max(0.01, 0.1 - 0.01 * (episode / 200))
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                action = q_network(torch.FloatTensor(state)).argmax().item()
        
        next_state, reward, done, _, _ = env.step(action)
        replay_buffer.append((state, action, reward, next_state, done))
        state = next_state

        # 訓練 Q 網絡
        if len(replay_buffer) > batch_size:
            states, actions, rewards, next_states, dones = sample_experiences()
            q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
            next_q_values = target_network(next_states).max(1)[0]
            targets = rewards + gamma * next_q_values * (1 - dones)
            loss = nn.MSELoss()(q_values, targets.detach())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    # 每隔幾步更新目標網絡
    if episode % 10 == 0:
        target_network.load_state_dict(q_network.state_dict())

print("DQN 訓練完成")
```

---

## **6.3 策略梯度法與Actor-Critic方法**

### **6.3.1 策略梯度法數學推導**

策略梯度法目標函數為：  
\[ J(\theta) = \mathbb{E}_{\pi_\theta} [ R ] \]  

其梯度為：  
\[ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta(a|s) R ] \]  

---

### **6.3.2 Actor-Critic 架構的數學背景**

Actor-Critic 方法結合策略梯度法與值函數近似：  
- **Actor** 更新策略網絡 \( \pi_\theta \)。  
- **Critic** 評估值函數 \( V(s) \)。

以下是 Actor-Critic 方法的實現程式碼：

```python
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorCritic, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, state):
        action_prob = self.actor(state)
        state_value = self.critic(state)
        return action_prob, state_value

# 初始化
model = ActorCritic(state_dim, action_dim)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 訓練循環
for episode in range(300):
    state = env.reset()[0]
    log_probs = []
    values = []
    rewards = []
    done = False
    
    while not done:
        state_tensor = torch.FloatTensor(state)
        action_probs, state_value = model(state_tensor)
        action = torch.multinomial(action_probs, 1).item()
        
        log_prob = torch.log(action_probs[action])
        next_state, reward, done, _, _ = env.step(action)
        
        log_probs.append(log_prob)
        values.append(state_value)
        rewards.append(reward)
        state = next_state

    # 計算損失並更新
    returns = []
    G = 0
    for reward in reversed(rewards):
        G = reward + gamma * G
        returns.insert(0, G)
    returns = torch.FloatTensor(returns)
    values = torch.cat(values)
    advantage = returns - values

    actor_loss = -(torch.stack(log_probs) * advantage.detach()).mean()
    critic_loss = advantage.pow(2).mean()
    loss = actor_loss + critic_loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("Actor-Critic 訓練完成")
```

---

這些程式碼涵蓋了深度強化學習的基礎，包括 DQN、策略梯度法和 Actor-Critic 方法，展示了數學推導在實際程式實現中的應用。