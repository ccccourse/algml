### 3.2 貝爾曼方程

貝爾曼方程是強化學習中的核心概念，它描述了在某一狀態下，如何基於當前策略或最優策略計算回報。這一方程揭示了狀態的價值（或行為的價值）如何依賴於未來狀態的價值，是動態規劃方法的基礎。在強化學習中，貝爾曼方程不僅用於計算價值函數，還能幫助我們推導出最優策略。

本節將介紹貝爾曼方程的兩種主要形式：值函數和行為價值函數，並探討貝爾曼最佳化原理。

#### 3.2.1 值函數與Q函數

在強化學習中，**值函數**\(V(s)\)表示在某一狀態\(s\)下，遵循某一策略\(\pi\)所能獲得的期望回報。對於一個狀態\(s\)，值函數\(V^\pi(s)\)定義為從狀態\(s\)開始，根據策略\(\pi\)選擇行為所獲得的總回報的期望值：

\[
V^\pi(s) = \mathbb{E}^\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s \right]
\]

其中，\(\gamma\)是折扣因子，\(R_{t+1}\)是時間步\(t+1\)的即時回報。

**行為價值函數**\(Q(s, a)\)表示在狀態\(s\)下選擇行為\(a\)，並遵循策略\(\pi\)之後的期望回報。它的定義是從狀態\(s\)開始選擇行為\(a\)，然後根據策略\(\pi\)繼續進行，所獲得的回報期望值：

\[
Q^\pi(s, a) = \mathbb{E}^\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
\]

貝爾曼方程是對這些函數的遞推定義，它將一個狀態或狀態-行為對的價值與未來的狀態或狀態-行為對的價值聯繫起來。

對於值函數\(V^\pi(s)\)，貝爾曼方程為：

\[
V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right]
\]

其中，\(\pi(a|s)\)是策略\(\pi\)在狀態\(s\)下選擇行為\(a\)的概率，\(P(s'|s, a)\)是從狀態\(s\)出發，執行行為\(a\)並轉移到狀態\(s'\)的概率，\(R(s, a, s')\)是即時回報。

對於行為價值函數\(Q^\pi(s, a)\)，其貝爾曼方程為：

\[
Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right]
\]

這裡，\(Q^\pi(s, a)\)的值取決於選擇行為\(a\)之後的未來回報以及未來狀態\(s'\)的價值。

這些貝爾曼方程通過遞推的方式計算價值函數，使得我們可以逐步推導出最優策略或價值。

#### 3.2.2 貝爾曼最佳化原理

貝爾曼最佳化原理（Bellman Optimality Principle）是強化學習中的一個關鍵概念，它描述了如何通過最優策略來最大化期望回報。根據該原理，最優策略的價值函數\(V^*(s)\)和行為價值函數\(Q^*(s, a)\)滿足以下的貝爾曼方程。

1. **最優值函數**：

   最優值函數\(V^*(s)\)表示從狀態\(s\)開始，選擇最優行為所能獲得的最大期望回報。根據貝爾曼最佳化原理，最優值函數的定義為：

   \[
   V^*(s) = \max_a \left( \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right] \right)
   \]

   這意味著，從狀態\(s\)開始，最優策略將選擇能夠最大化未來期望回報的行為\(a\)。

2. **最優行為價值函數**：

   最優行為價值函數\(Q^*(s, a)\)表示在狀態\(s\)下選擇行為\(a\)，並採取最優策略之後的期望回報。根據貝爾曼最佳化原理，最優行為價值函數的定義為：

   \[
   Q^*(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \max_{a'} Q^*(s', a') \right]
   \]

   這表示，最優行為價值函數在每個狀態-行為對上，選擇能夠最大化回報的行為。

最優策略\(\pi^*\)對應於最大化最優行為價值函數的選擇：

\[
\pi^*(s) = \arg\max_a Q^*(s, a)
\]

貝爾曼最佳化原理的核心是根據當前狀態下的行為選擇，最大化未來所有可能狀態的回報期望。這一原理是強化學習中最優策略迭代的基礎，並且支持動態規劃中的值迭代和策略迭代方法。

#### 小結

貝爾曼方程是強化學習中的基石，通過描述狀態價值與行為價值如何遞歸依賴於未來狀態的價值，它幫助我們理解最優策略如何選擇最適合的行為。值函數和行為價值函數是描述策略的兩種方式，而貝爾曼最佳化原理則提供了推導最優策略和最優回報的理論框架。這些概念是動態規劃、策略迭代和值迭代等強化學習方法的理論基礎。