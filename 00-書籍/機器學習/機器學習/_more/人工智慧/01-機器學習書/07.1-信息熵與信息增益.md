### **信息熵與信息增益**

在決策樹算法中，信息熵（Entropy）和信息增益（Information Gain）是衡量特徵重要性以及決策樹分裂的關鍵指標。這些概念源自於信息理論，用來量化不確定性和從數據中獲得的有用信息。

#### **1. 信息熵（Entropy）**

信息熵是衡量一個隨機變量不確定性的指標，通常應用於分類問題。在決策樹中，熵用來度量某個節點上數據的混亂程度。當節點中有很多不同類別的樣本時，熵的值較高，表示系統的混亂；反之，當節點中大多數樣本來自同一類時，熵的值較低。

信息熵的定義公式為：

\[
H(D) = - \sum_{i=1}^{k} p_i \log_2 p_i
\]

其中：
- \(H(D)\) 是數據集 \(D\) 的熵，
- \(p_i\) 是第 \(i\) 類在數據集中的概率，
- \(k\) 是類別的總數。

熵的值範圍為 0 到 \( \log_2 k \)，其中 \(k\) 是類別數目。當數據集完全均勻（即每個類別的樣本數目相同）時，熵達到最大值；當數據集完全純淨（即所有樣本屬於同一類別）時，熵為 0。

#### **2. 信息增益（Information Gain）**

信息增益衡量的是從某個特徵中得到的資訊量，換句話說，就是分裂某個節點後，數據集的熵減少了多少。對於每個特徵，決策樹算法會計算它所帶來的信息增益，並選擇信息增益最大的特徵來進行分裂。

信息增益的計算公式為：

\[
\text{IG}(D, A) = H(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} H(D_v)
\]

其中：
- \( \text{IG}(D, A) \) 是特徵 \(A\) 的信息增益，
- \(H(D)\) 是數據集 \(D\) 的熵，
- \( \text{Values}(A) \) 是特徵 \(A\) 的所有可能取值，
- \( D_v \) 是對應於特徵 \(A\) 取值為 \(v\) 的子集，
- \( |D_v| / |D| \) 是特徵 \(A\) 取值為 \(v\) 的子集的比例，
- \( H(D_v) \) 是子集 \(D_v\) 的熵。

#### **3. 計算範例**

假設有一個簡單的二分類問題，數據集包含了 6 個樣本，其中 4 個屬於類別 1，2 個屬於類別 0。我們想根據特徵 "天氣" （`晴天`、`雨天`）來分裂數據集。首先，我們計算整個數據集的熵，然後計算基於 "天氣" 特徵分裂後的子集熵，再根據公式計算信息增益。

**步驟 1: 計算整個數據集的熵**

數據集共有 6 個樣本，其中 4 個屬於類別 1，2 個屬於類別 0。

\[
H(D) = -\left( \frac{4}{6} \log_2 \frac{4}{6} + \frac{2}{6} \log_2 \frac{2}{6} \right) = 0.918
\]

**步驟 2: 根據 "天氣" 特徵分裂數據集**

假設根據 "天氣" 特徵分裂後，數據集被分成兩個子集：`晴天` 和 `雨天`。
- `晴天` 子集：包含 4 個樣本，其中 3 個屬於類別 1，1 個屬於類別 0。
- `雨天` 子集：包含 2 個樣本，兩個都屬於類別 0。

計算每個子集的熵：
- `晴天` 子集的熵：
\[
H(\text{晴天}) = -\left( \frac{3}{4} \log_2 \frac{3}{4} + \frac{1}{4} \log_2 \frac{1}{4} \right) = 0.811
\]
- `雨天` 子集的熵：
\[
H(\text{雨天}) = -\left( \frac{2}{2} \log_2 \frac{2}{2} \right) = 0
\]

**步驟 3: 計算信息增益**

信息增益計算公式為：
\[
\text{IG}(D, \text{天氣}) = H(D) - \left( \frac{4}{6} H(\text{晴天}) + \frac{2}{6} H(\text{雨天}) \right)
\]
\[
\text{IG}(D, \text{天氣}) = 0.918 - \left( \frac{4}{6} \times 0.811 + \frac{2}{6} \times 0 \right) = 0.918 - 0.541 = 0.377
\]

因此，基於 "天氣" 特徵的分裂帶來的信息增益為 0.377。

#### **4. 小結**

- **信息熵** 用於衡量數據集的純度或不確定性，越高的熵表示數據越混亂，越低的熵表示數據越純淨。
- **信息增益** 是根據特徵進行分裂時，數據集熵的減少量。當信息增益越大，表示該特徵對分類的貢獻越大。
- 在決策樹中，算法會選擇具有最大信息增益的特徵來進行分裂，這樣可以最有效地減少數據的不確定性，從而達到最佳的分類效果。