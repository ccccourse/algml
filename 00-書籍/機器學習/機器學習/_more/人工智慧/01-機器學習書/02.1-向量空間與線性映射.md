### 線性代數與矩陣運算

#### 向量空間與線性映射

線性代數是機器學習中至關重要的一部分，尤其在數據處理、模型訓練和特徵提取中，線性代數提供了強大的數學基礎。兩個基本概念：**向量空間**和**線性映射**，是理解矩陣運算和算法的關鍵。

1. **向量空間（Vector Space）**

   向量空間是由一組向量組成的集合，這些向量遵循兩個基本運算：加法和數量乘法。向量空間的定義要求滿足以下幾個條件：
   
   - **加法封閉性**：對於任意的向量 \( \mathbf{v}, \mathbf{u} \)，它們的和 \( \mathbf{v} + \mathbf{u} \) 仍然是該空間中的向量。
   - **數量乘法封閉性**：對於任意的向量 \( \mathbf{v} \) 和標量 \( c \)，\( c\mathbf{v} \) 仍然是該空間中的向量。
   - 向量空間還要求存在**零向量**，即加法的單位元，並且對每個向量都存在一個**負向量**，使得兩者相加得到零向量。

   例如， \( \mathbb{R}^n \)（n維實數空間）是一個常見的向量空間。這個空間中的向量通常表示為 \( \mathbf{v} = [v_1, v_2, \dots, v_n]^\top \)，其中每個 \( v_i \) 是實數。

2. **線性映射（Linear Mapping）**

   線性映射（或稱為線性變換）是將一個向量空間映射到另一個向量空間的函數，它保持加法和數量乘法運算。即對於兩個向量 \( \mathbf{v} \) 和 \( \mathbf{u} \)，以及標量 \( c \)，映射 \( T \) 必須滿足以下兩個條件：
   
   - \( T(\mathbf{v} + \mathbf{u}) = T(\mathbf{v}) + T(\mathbf{u}) \)（加法的保持）
   - \( T(c\mathbf{v}) = cT(\mathbf{v}) \)（數量乘法的保持）

   在線性代數中，線性映射通常用矩陣表示。假設有一個線性映射 \( T: \mathbb{R}^n \to \mathbb{R}^m \)，對應的矩陣為 \( A \)，則映射可以寫為：

   \[
   T(\mathbf{v}) = A \mathbf{v}
   \]

   這裡 \( \mathbf{v} \) 是一個 \( n \)-維向量，\( A \) 是一個 \( m \times n \) 的矩陣，並且 \( T(\mathbf{v}) \) 是一個 \( m \)-維向量。矩陣 \( A \) 被稱為映射的係數矩陣，它描述了向量空間中的變換規則。

3. **矩陣與向量的關係**

   矩陣作為線性映射的表現，對於計算和優化來說至關重要。例如，將一組特徵表示為向量，然後用矩陣將這些特徵映射到新的空間中。在機器學習中，這樣的操作可以出現於多種情境，如：
   
   - **數據轉換**：特徵縮放、主成分分析（PCA）等技術本質上是通過矩陣來映射數據。
   - **線性回歸**：在線性回歸中，假設模型可以表示為 \( \mathbf{y} = A \mathbf{x} + \mathbf{b} \)，其中 \( A \) 是回歸係數矩陣，\( \mathbf{x} \) 是特徵向量，\( \mathbf{b} \) 是偏置。

4. **基與維度**

   向量空間還有一個關鍵的概念——**基（Basis）**。基是該空間中一組線性無關的向量，任何其他向量都可以寫成這些基向量的線性組合。維度（Dimension）則是基向量的數目，表示空間的“大小”或“自由度”。在 \( \mathbb{R}^n \) 中，基是由 \( n \) 個線性無關的向量組成，維度為 \( n \)。

總結來說，向量空間和線性映射是線性代數的核心概念，它們不僅在數學理論中具有深遠意義，在機器學習中也起到了至關重要的作用。理解這些概念，有助於深入掌握數據處理、特徵變換、模型訓練和優化過程中的矩陣運算。