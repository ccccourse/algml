### **14. 多目標學習與 Ensemble 方法**

Ensemble（集成）方法是將多個基模型組合成一個強大的模型，從而提高預測的準確性和穩定性。這些方法通常是解決過擬合問題、提高模型的泛化能力的重要工具。主要的集成方法包括 **Bagging**、**Boosting** 和 **Stacking**，它們各自有不同的工作原理和應用場景。

#### **1. Bagging (Bootstrap Aggregating)**

Bagging 是一種減少模型方差的集成方法，通常通過訓練多個基學習器來提高預測穩定性。每個基學習器都是在原始數據集的不同隨機子集上訓練的，這些子集是通過有放回抽樣（Bootstrap）得到的。最終的預測是對所有基學習器預測結果的平均（回歸問題）或投票（分類問題）。

**特點**：
- 通過多次隨機取樣和基學習器訓練來減少過擬合。
- 增加模型穩定性，適用於高方差的模型。

**常見算法**：
- 隨機森林（Random Forest）是最著名的 Bagging 方法。

**數學原理**：
假設有一個訓練集 \( D = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \)，通過有放回抽樣生成多個子集 \( D_1, D_2, \dots, D_k \)，每個基學習器 \( h_k \) 都在不同的 \( D_k \) 上訓練。最終預測為：
\[ \hat{y} = \frac{1}{k} \sum_{k=1}^{K} h_k(x) \quad \text{(回歸)} \]
\[ \hat{y} = \text{vote}(\{h_1(x), h_2(x), \dots, h_K(x)\}) \quad \text{(分類)} \]

**實例：** 隨機森林（Random Forest）
```python
from sklearn.ensemble import RandomForestClassifier

# 定義隨機森林模型
model = RandomForestClassifier(n_estimators=100, random_state=42)

# 訓練模型
model.fit(X_train, y_train)

# 預測
y_pred = model.predict(X_test)
```

#### **2. Boosting**

Boosting 是一種將多個弱學習器轉化為強學習器的集成方法。它通過加權的方式逐步修正前一輪學習器的錯誤。每個基學習器都對先前學習器的錯誤樣本進行強化，最終的預測是所有學習器預測結果的加權和。Boosting 特別適合處理高偏差問題，並能顯著提高預測準確性。

**特點**：
- 通過增強錯誤樣本的學習來減少偏差。
- 每次訓練都會關注錯誤樣本，從而提高模型的準確性。

**常見算法**：
- **AdaBoost**（Adaptive Boosting）：每個基學習器對錯誤樣本加權，並最終加權所有學習器的預測結果。
- **Gradient Boosting**：每次在前一輪預測的基礎上修正錯誤，通過梯度下降法來最小化誤差。

**數學原理**：
每次將上一輪的預測錯誤樣本加權，假設我們有一個基學習器 \( h_1, h_2, \dots, h_k \)，每一輪的預測權重由上一輪的錯誤率決定。最終預測為：
\[ \hat{y} = \sum_{k=1}^{K} \alpha_k h_k(x) \]
其中， \( \alpha_k \) 是每個學習器的權重，根據每個基學習器的誤差來計算。

**實例：** Gradient Boosting
```python
from sklearn.ensemble import GradientBoostingClassifier

# 定義梯度提升模型
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)

# 訓練模型
model.fit(X_train, y_train)

# 預測
y_pred = model.predict(X_test)
```

#### **3. Stacking**

Stacking 是一種更高級的集成方法，通過將多個基學習器的預測結果作為新特徵，並用一個最終的學習器來進行訓練。這種方法有助於挖掘不同模型間的互補性，提高預測的準確性。Stacking 通常會用到一個 **元學習器（Meta-Learner）**，來學習如何組合基學習器的預測結果。

**特點**：
- 融合多個不同的學習算法來增加模型的多樣性。
- 使用一個元學習器來進行二次學習，融合基學習器的預測結果。

**數學原理**：
假設有多個基學習器 \( h_1, h_2, \dots, h_K \)，它們的預測結果作為新的特徵 \( \hat{y}_k = h_k(x) \)，這些預測結果被用來訓練一個元學習器 \( h_{meta} \)：
\[ \hat{y} = h_{meta}(\hat{y}_1, \hat{y}_2, \dots, \hat{y}_K) \]

**實例：** 使用 `stacking` 方法
```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# 定義基學習器
estimators = [
    ('dt', DecisionTreeClassifier()),
    ('svc', SVC(probability=True))
]

# 定義元學習器
meta_model = LogisticRegression()

# 定義 Stacking 模型
stacking_model = StackingClassifier(estimators=estimators, final_estimator=meta_model)

# 訓練模型
stacking_model.fit(X_train, y_train)

# 預測
y_pred = stacking_model.predict(X_test)
```

#### **比較：Bagging、Boosting 和 Stacking**

| 方法       | 特點                                           | 優勢                                           | 缺點                                           |
|------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|
| **Bagging** | 通過有放回抽樣生成多個訓練集並訓練基學習器   | 減少過擬合，提高穩定性，適合高方差模型       | 無法有效減少偏差，對於簡單模型效果較差         |
| **Boosting** | 增強錯誤樣本的學習，迭代式修正錯誤           | 顯著減少偏差，適合高偏差模型，提升準確性     | 可能會過擬合，對噪聲敏感                      |
| **Stacking** | 使用多個基學習器和一個元學習器組合預測結果 | 能夠融合多個模型的優勢，減少過擬合，提升性能 | 較為複雜，需要更多的計算資源，可能過度依賴元學習器 |

#### **總結**

- **Bagging** 適合用於減少模型方差，尤其是對於高方差的基學習器（如決策樹）效果顯著，隨機森林是其代表。
- **Boosting** 專注於減少偏差，逐步修正錯誤樣本，適合用於需要提高精確度的任務（如 AdaBoost、Gradient Boosting）。
- **Stacking** 提供了一個靈活且強大的框架來融合多個基學習器，從而充分挖掘不同模型的優勢，特別適用於多樣性較強的數據。

這些集成方法在多數實際應用中都能顯著提升模型性能，選擇合適的方法可以根據具體的問題和數據來決定。