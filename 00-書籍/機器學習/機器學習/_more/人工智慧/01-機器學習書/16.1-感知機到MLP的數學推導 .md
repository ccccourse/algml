### 16. **神經網路的數學視角**  
#### **感知機到 MLP 的數學推導**

感知機（Perceptron）是最基本的神經網路模型之一，起源於1960年代，由Frank Rosenblatt提出。它是線性分類器的基礎，用於二元分類問題。而多層感知機（MLP, Multi-Layer Perceptron）則是感知機的擴展，可以處理更複雜的非線性問題，並且可以有多個隱藏層。

以下是從感知機到 MLP 的數學推導過程，通過這些推導可以幫助理解神經網絡是如何逐步從簡單的線性分類器發展成為強大的深度學習模型的。

---

### **1. 感知機（Perceptron）的數學推導**

感知機是一種單層神經網路（即只有輸入層和輸出層），每個神經元進行線性加權輸入的計算，然後通過激活函數進行非線性變換。數學表達式如下：

#### **(1) 線性組合：**
給定一個輸入向量 \( \mathbf{x} = (x_1, x_2, \dots, x_n) \) 和相應的權重向量 \( \mathbf{w} = (w_1, w_2, \dots, w_n) \)，感知機的輸出是這些輸入的線性加權和：

\[
z = \mathbf{w}^T \mathbf{x} + b = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
\]

其中，\( b \) 是偏置項。

#### **(2) 激活函數：**
感知機的輸出會經過一個激活函數 \( f(z) \)，通常是階躍函數（step function）：

\[
y = f(z) = \begin{cases} 
1, & \text{if } z \geq 0 \\
0, & \text{if } z < 0 
\end{cases}
\]

這樣，感知機的最終輸出 \( y \) 要么是 1（正類別），要么是 0（負類別）。

#### **(3) 訓練與學習規則：**
感知機使用感知機學習規則來更新權重。對於每個訓練樣本 \( (\mathbf{x}_i, y_i) \)，根據預測誤差來調整權重：

\[
\mathbf{w} \leftarrow \mathbf{w} + \eta (y_i - \hat{y_i}) \mathbf{x}_i
\]
\[
b \leftarrow b + \eta (y_i - \hat{y_i})
\]

其中，\( \eta \) 是學習率，\( \hat{y_i} \) 是模型的預測輸出。

---

### **2. 多層感知機（MLP）的數學推導**

多層感知機（MLP）是由多層感知機構成的神經網路，通常包含一個輸入層、一個或多個隱藏層以及一個輸出層。每一層的每個神經元與上一層的所有神經元都有連接，並通過加權求和後進行激活函數處理。MLP 使用反向傳播（backpropagation）算法來訓練模型。

#### **(1) 前向傳播（Forward Propagation）：**

對於 MLP，假設有一個包含兩個隱藏層的網路，輸入層為 \( \mathbf{x} \)，隱藏層的輸出為 \( \mathbf{h}^{(1)}, \mathbf{h}^{(2)} \)，最終輸出層為 \( \mathbf{y} \)，那麼前向傳播的計算過程如下：

##### **輸入層到第一隱藏層：**
\[
\mathbf{h}^{(1)} = f^{(1)}(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
\]
其中，\( \mathbf{W}^{(1)} \) 是第一隱藏層的權重矩陣，\( \mathbf{b}^{(1)} \) 是偏置項，\( f^{(1)} \) 是激活函數（如 ReLU、Sigmoid 或 Tanh）。

##### **第一隱藏層到第二隱藏層：**
\[
\mathbf{h}^{(2)} = f^{(2)}(\mathbf{W}^{(2)} \mathbf{h}^{(1)} + \mathbf{b}^{(2)})
\]

##### **隱藏層到輸出層：**
\[
\mathbf{y} = f^{(3)}(\mathbf{W}^{(3)} \mathbf{h}^{(2)} + \mathbf{b}^{(3)})
\]

其中，\( \mathbf{W}^{(2)} \) 和 \( \mathbf{W}^{(3)} \) 分別是第二隱藏層和輸出層的權重矩陣，\( \mathbf{b}^{(2)} \) 和 \( \mathbf{b}^{(3)} \) 是對應的偏置項，激活函數 \( f^{(3)} \) 取決於問題類型（例如，對於回歸問題，通常不使用激活函數，對於分類問題，可能使用 Softmax 函數）。

#### **(2) 反向傳播（Backpropagation）：**

反向傳播是用於訓練 MLP 的算法，其目的是通過梯度下降來更新權重。首先，我們需要計算損失函數對權重的梯度，然後使用鏈式法則將誤差從輸出層反向傳播到每一層，逐層更新權重。

##### **損失函數：**
通常使用均方誤差（MSE）或交叉熵（cross-entropy）作為損失函數，對於二分類問題，交叉熵損失函數為：

\[
L = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
\]
其中，\( \hat{y} \) 是預測輸出，\( y \) 是真實標籤。

##### **梯度計算與權重更新：**
反向傳播的目標是計算每一層的梯度並進行權重更新，對於權重 \( \mathbf{W}^{(k)} \) 和偏置 \( \mathbf{b}^{(k)} \)，更新公式為：

\[
\mathbf{W}^{(k)} \leftarrow \mathbf{W}^{(k)} - \eta \frac{\partial L}{\partial \mathbf{W}^{(k)}}
\]
\[
\mathbf{b}^{(k)} \leftarrow \mathbf{b}^{(k)} - \eta \frac{\partial L}{\partial \mathbf{b}^{(k)}}
\]

其中，\( \eta \) 是學習率，\( \frac{\partial L}{\partial \mathbf{W}^{(k)}} \) 和 \( \frac{\partial L}{\partial \mathbf{b}^{(k)}} \) 是分別對權重和偏置的梯度。

反向傳播的具體過程包括以下幾個步驟：

1. **計算輸出層的梯度：**
   \[
   \delta^{(3)} = \frac{\partial L}{\partial \mathbf{y}} \cdot f'^{(3)}(\mathbf{y})
   \]

2. **計算隱藏層的梯度：**
   \[
   \delta^{(2)} = \left( \delta^{(3)} \cdot \mathbf{W}^{(3)} \right) \cdot f'^{(2)}(\mathbf{h}^{(2)})
   \]

3. **繼續反向傳播，直到第一層：**
   \[
   \delta^{(1)} = \left( \delta^{(2)} \cdot \mathbf{W}^{(2)} \right) \cdot f'^{(1)}(\mathbf{h}^{(1)})
   \]

4. **更新權重和偏置：**
   \[
   \mathbf{W}^{(k)} \leftarrow \mathbf{W}^{(k)} - \eta \delta^{(k)} \mathbf{h}^{(k-1)}
   \]
   \[
   \mathbf{b}^{(k)} \leftarrow \mathbf{b}^{(k)} - \eta \delta^{(k)}
   \]

這樣，通過多次前向傳播和反向傳播，MLP 可以學習到適合於特定任務的權重和偏置。

---

### **小結：**

- **感知機** 是一個單層神經網路，通過加權和後用階躍函數進行分類，並使用學習規則進行訓練。
- **多層感知機（MLP）** 是感知機的擴展，通過多層隱藏層能夠處理更複雜的問題，並使用反向傳播來訓練。
- **數學推導過程** 展示了如何從感知機的線性模型進化到更強大的 MLP 模型，這些數學基礎是理解神經網絡運作的關鍵。

這些基礎的數學推導有助於深入理解神經網路的內部運作，並在實際應用中更好地調整和優化模型。