### **scikit-learn 中的優化實現**

`scikit-learn` 是 Python 中非常流行的機器學習庫，它提供了多種常用的優化方法來幫助用戶有效地訓練機器學習模型。這些優化方法大多數基於梯度下降法，並在此基礎上進行了不同的優化與調整。以下將介紹幾種在 `scikit-learn` 中常見的優化實現，包括 **梯度下降**、**隨機梯度下降（SGD）** 以及 **自動超參數調優工具**。

---

#### **1. 梯度下降與隨機梯度下降（SGD）**

在 `scikit-learn` 中，兩個常見的使用梯度下降法的類是 `SGDClassifier` 和 `SGDRegressor`。這些類可以用來執行分類和回歸任務，並且能夠自動進行優化。

**SGDClassifier** 和 **SGDRegressor** 都使用 **隨機梯度下降（SGD）** 算法進行優化，適用於大規模數據集，尤其是在樣本數量非常大的情況下，這些方法能夠大幅提高訓練速度。

##### **範例：SGDRegressor 實現線性回歸**

```python
from sklearn.linear_model import SGDRegressor
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt

# 生成回歸數據
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# 創建並訓練模型
sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.01)
sgd_regressor.fit(X, y)

# 預測結果
y_pred = sgd_regressor.predict(X)

# 繪製結果
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, y_pred, color='red', label='SGD Fit')
plt.legend()
plt.show()

# 顯示最終的參數
print("Final coefficients:", sgd_regressor.coef_)
print("Final intercept:", sgd_regressor.intercept_)
```

在這段代碼中，`SGDRegressor` 用來對回歸問題進行建模。`max_iter` 參數控制最大迭代次數，`tol` 參數則是容許的容忍誤差。

##### **SGDClassifier 實現分類問題**

```python
from sklearn.linear_model import SGDClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 生成分類數據
X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)

# 分割數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 創建並訓練模型
sgd_classifier = SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge', eta0=0.01)
sgd_classifier.fit(X_train, y_train)

# 預測並評估模型
y_pred = sgd_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test accuracy:", accuracy)
```

在這段代碼中，`SGDClassifier` 用於解決二分類問題。這裡使用了 `hinge` 損失函數，這是支持向量機（SVM）中常見的損失函數之一。

---

#### **2. 自動超參數調優：GridSearchCV 和 RandomizedSearchCV**

在 `scikit-learn` 中，模型的優化不僅僅依賴於選擇合適的算法，還需要選擇合適的超參數。`GridSearchCV` 和 `RandomizedSearchCV` 是兩個非常有用的工具，用於自動化地搜索最佳的超參數組合。

##### **GridSearchCV**

`GridSearchCV` 通過遍歷所有超參數的網格來進行模型的超參數調優。這意味著，它會對給定的超參數範圍進行穷舉式搜尋，並選擇使模型表現最佳的參數組合。

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 生成分類數據
X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)

# 分割數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 定義模型
svc = SVC()

# 定義參數網格
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# 定義 GridSearchCV
grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')

# 執行網格搜索
grid_search.fit(X_train, y_train)

# 顯示最佳參數
print("Best parameters:", grid_search.best_params_)

# 預測並評估模型
y_pred = grid_search.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test accuracy:", accuracy)
```

在這段程式碼中，`GridSearchCV` 對 `SVC` 模型的超參數 `C`、`kernel` 和 `gamma` 進行了遍歷搜索，並選擇最佳的組合。

##### **RandomizedSearchCV**

相比於 `GridSearchCV`，`RandomizedSearchCV` 隨機選擇超參數組合來進行搜索。這種方法能夠在更短的時間內搜尋到近似的最佳結果，特別適用於超參數範圍較大時。

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# 生成分類數據
X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)

# 分割數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 定義模型
svc = SVC()

# 定義超參數分佈
param_dist = {
    'C': np.logspace(-3, 3, 7),
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# 定義 RandomizedSearchCV
random_search = RandomizedSearchCV(svc, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)

# 執行隨機搜索
random_search.fit(X_train, y_train)

# 顯示最佳參數
print("Best parameters:", random_search.best_params_)

# 預測並評估模型
y_pred = random_search.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test accuracy:", accuracy)
```

在這段程式碼中，`RandomizedSearchCV` 隨機選擇超參數進行多次試驗，並選擇表現最佳的參數組合。

---

#### **3. 小結**

- **SGDRegressor / SGDClassifier**：`scikit-learn` 中提供了基於隨機梯度下降的回歸和分類算法，適用於大規模數據集。這些模型使用梯度下降方法來優化損失函數，並能夠處理線性回歸、分類等任務。
  
- **GridSearchCV**：`GridSearchCV` 會遍歷所有給定的超參數組合進行搜索，找到最佳的超參數配置。這是一種穷舉式的搜索方法，對於較小的超參數範圍較為有效。

- **RandomizedSearchCV**：`RandomizedSearchCV` 隨機選擇超參數組合進行搜索，比起 `GridSearchCV`，它能在較短的時間內搜尋到較為接近的最佳參數，特別適用於超參數範圍較大的情況。

這些優化工具使得模型的訓練和超參數調優變得更加高效，從而提高了機器學習模型的性能和可擴展性。