### **回歸模型的數學解析**

回歸模型是機器學習中最常用的模型之一，主要用於預測一個連續型目標變數。這些模型通過學習輸入特徵與目標變數之間的關係來進行預測。在回歸模型中，**線性回歸** 和 **廣義線性模型**（GLM）是最基礎和常見的兩種形式。

---

#### **1. 線性回歸**

**線性回歸** 是最簡單且常見的回歸模型之一。它假設目標變數 \( y \) 和特徵變數 \( x_1, x_2, \dots, x_n \) 之間存在線性關係。具體而言，線性回歸模型表示為：

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon
\]

其中：
- \( y \) 是預測目標變數（連續型）。
- \( x_1, x_2, \dots, x_n \) 是輸入特徵。
- \( \beta_0 \) 是截距項（bias），表示當所有特徵為 0 時的預測值。
- \( \beta_1, \beta_2, \dots, \beta_n \) 是模型的權重（係數），表示各個特徵對預測結果的貢獻。
- \( \epsilon \) 是誤差項，通常假設誤差服從均值為 0 的正態分佈。

**目標：** 根據訓練數據集來學習出最佳的係數 \( \beta_0, \beta_1, \dots, \beta_n \)。

#### **2. 最小二乘法（Ordinary Least Squares, OLS）**

在進行線性回歸的訓練時，我們希望最小化預測值與實際值之間的誤差。常用的方法是 **最小二乘法**（OLS），其目標是最小化所有樣本的預測誤差的平方和。具體的損失函數為：

\[
L(\beta_0, \beta_1, \dots, \beta_n) = \sum_{i=1}^{m} (y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_n x_{in}))^2
\]

這裡：
- \( m \) 是樣本數量。
- \( y_i \) 是第 \( i \) 個樣本的真實目標變數值。
- \( x_{i1}, x_{i2}, \dots, x_{in} \) 是第 \( i \) 個樣本的特徵。

我們通過最小化這個損失函數來求解模型的最佳係數。

##### **範例：線性回歸實現**

```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt

# 生成回歸數據
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# 創建線性回歸模型
model = LinearRegression()

# 訓練模型
model.fit(X, y)

# 預測結果
y_pred = model.predict(X)

# 繪製結果
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, y_pred, color='red', label='Linear Fit')
plt.legend()
plt.show()

# 顯示模型係數
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)
```

在這段代碼中，我們使用 `LinearRegression` 模型來擬合一個簡單的回歸問題，並且利用 `fit()` 方法進行訓練，最後畫出擬合曲線。

---

#### **3. 廣義線性模型（GLM）**

**廣義線性模型（Generalized Linear Models, GLM）** 是一個擴展了線性回歸的統計模型，它不僅包含線性回歸的形式，還允許目標變數的分佈不再是正態分佈，並且允許輸入變數通過某些變換（如對數變換、指數變換等）來影響預測。

GLM 由以下三個組成部分構成：
1. **隨機成分**：假設目標變數 \( y \) 服從某個特定的概率分佈（如正態分佈、伯努利分佈等）。
2. **系統成分**：這部分是輸入特徵的線性組合，通常表示為 \( \eta = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n \)。
3. **連接函數**：定義了如何將線性預測 \( \eta \) 與預測的期望值 \( E(y) \) 聯繫起來。常見的連接函數包括對數連接函數（對於泊松回歸）、邏輯斯回歸（對於二分類）等。

GLM 的形式為：

\[
g(E(y)) = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
\]

其中 \( g(\cdot) \) 是連接函數，\( E(y) \) 是目標變數的期望。

#### **GLM 的應用：邏輯回歸（Logistic Regression）**

邏輯回歸是一個廣義線性模型，其中目標變數 \( y \) 是二元變量，通常使用邏輯斯函數（Logistic Function）作為連接函數：

\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}}
\]

這裡，\( p \) 是預測為正類的概率，並且目標變數是 0 或 1。

##### **範例：邏輯回歸實現**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 生成分類數據
X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)

# 分割數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 創建邏輯回歸模型
log_reg = LogisticRegression()

# 訓練模型
log_reg.fit(X_train, y_train)

# 預測結果
y_pred = log_reg.predict(X_test)

# 計算準確率
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

在這段代碼中，我們使用 `LogisticRegression` 來解決一個二分類問題。模型通過最大化對數似然函數來估計參數。

---

#### **小結**

- **線性回歸** 假設目標變數與特徵之間存在線性關係，並通過最小化誤差平方和來擬合模型。
  
- **廣義線性模型（GLM）** 是線性回歸的擴展，允許目標變數具有不同的分佈，並使用連接函數將線性預測與目標變數的期望值聯繫起來。GLM 包含了多種回歸模型，如邏輯回歸、泊松回歸等。

- 線性回歸和廣義線性模型都可以利用 `scikit-learn` 提供的接口輕鬆實現，並且廣泛應用於各種回歸和分類任務中。