### **正則化方法：Lasso 與 Ridge**

正則化是機器學習中的一種技術，用於防止模型過擬合，尤其是在特徵較多或訓練數據較少的情況下。通過在損失函數中加入額外的懲罰項來控制模型的複雜度，從而促使模型選擇較少的關鍵特徵或較小的參數值。常見的兩種正則化方法是 **Lasso** 和 **Ridge**。

---

#### **1. Ridge 正則化（L2 正則化）**

**Ridge 回歸**（也稱為 **L2 正則化**）在線性回歸的損失函數中加入了權重的平方和，這樣可以抑制模型中權重過大的問題。其目標是最小化以下損失函數：

\[
L(\beta_0, \beta_1, \dots, \beta_n) = \sum_{i=1}^{m} (y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_n x_{in}))^2 + \lambda \sum_{j=1}^{n} \beta_j^2
\]

其中：
- 第一部分是普通的最小二乘損失。
- 第二部分是 Ridge 正則化項，其中 \( \lambda \) 是正則化強度的超參數（越大則正則化越強）。
- \( \beta_j^2 \) 是模型參數的平方。

Ridge 正則化的作用是將所有的模型係數推向較小的值，但不會使它們完全為零。這意味著 Ridge 正則化偏向於保留所有特徵，但將它們的權重縮小。

---

#### **2. Lasso 正則化（L1 正則化）**

**Lasso 回歸**（也稱為 **L1 正則化**）在損失函數中加入了權重的絕對值和，從而促使某些特徵的權重變為零，實現特徵選擇。其損失函數如下：

\[
L(\beta_0, \beta_1, \dots, \beta_n) = \sum_{i=1}^{m} (y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_n x_{in}))^2 + \lambda \sum_{j=1}^{n} |\beta_j|
\]

其中：
- 第一部分是最小二乘損失。
- 第二部分是 Lasso 正則化項，其中 \( \lambda \) 是正則化強度，\( \sum_{j=1}^{n} |\beta_j| \) 是權重的絕對值和。

Lasso 正則化的特點是它會使得一些不重要的特徵的權重變為零，從而進行特徵選擇，減少模型的複雜度。因此，Lasso 非常適用於需要進行特徵選擇的情況。

---

#### **3. Ridge vs Lasso**

- **Ridge**：適用於特徵之間有多重共線性的情況，並且希望保留所有的特徵，但使它們的影響力減小。它能夠防止模型權重過大，但不會將權重完全歸零。
  
- **Lasso**：適用於特徵數量較多，且希望進行特徵選擇的情況。它不僅能夠防止過擬合，還能將一些特徵的權重壓縮為零，實現自動特徵選擇。

- **Elastic Net**：結合了 Ridge 和 Lasso 的優勢，它在損失函數中同時包含 L1 和 L2 正則化項，因此適合於特徵多且具有高度相關性的情況。

---

#### **4. 範例：Ridge 和 Lasso 實現**

在 `scikit-learn` 中，Ridge 和 Lasso 回歸模型可以通過 `Ridge` 和 `Lasso` 類來實現。以下是兩種正則化方法的簡單示例：

##### **Ridge 回歸範例**

```python
from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt

# 生成回歸數據
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# 創建 Ridge 模型並設置正則化強度
ridge_model = Ridge(alpha=1.0)

# 訓練模型
ridge_model.fit(X, y)

# 預測結果
y_pred = ridge_model.predict(X)

# 繪製結果
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, y_pred, color='red', label='Ridge Fit')
plt.legend()
plt.show()

# 顯示模型係數
print("Ridge Coefficients:", ridge_model.coef_)
```

在這段代碼中，我們使用 `Ridge` 類創建 Ridge 回歸模型，並使用 `alpha` 參數設置正則化強度。

##### **Lasso 回歸範例**

```python
from sklearn.linear_model import Lasso
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt

# 生成回歸數據
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# 創建 Lasso 模型並設置正則化強度
lasso_model = Lasso(alpha=0.1)

# 訓練模型
lasso_model.fit(X, y)

# 預測結果
y_pred = lasso_model.predict(X)

# 繪製結果
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, y_pred, color='red', label='Lasso Fit')
plt.legend()
plt.show()

# 顯示模型係數
print("Lasso Coefficients:", lasso_model.coef_)
```

在這段代碼中，我們使用 `Lasso` 類來創建 Lasso 回歸模型，並使用 `alpha` 參數來設置正則化強度。

---

#### **5. 小結**

- **Ridge 正則化** 主要通過平方項對權重進行懲罰，適合於多重共線性或所有特徵都具有某些預測能力的情況。
  
- **Lasso 正則化** 主要通過絕對值項對權重進行懲罰，適合於特徵選擇並能夠使某些特徵的權重為零，實現自動選擇重要特徵。

- 在 `scikit-learn` 中，可以通過 `Ridge` 和 `Lasso` 類實現這些方法，並使用 `alpha` 參數來控制正則化的強度。