### **scikit-learn 的實現：KMeans 與 SpectralClustering**

在機器學習中，K-means 和 Spectral Clustering 都是常用的聚類算法。這兩種方法都能夠有效地將數據劃分為幾個簇，但它們的工作原理和適用的情境有所不同。

---

### **1. K-Means 聚類**

K-means 是最常見且高效的聚類算法之一。它的目標是將數據分為 \( K \) 個簇，使得每個簇內的點盡可能相似，而簇間的點則儘量不同。

#### **K-means 算法的基本步驟**
1. 隨機選擇 \( K \) 個初始質心。
2. 為每個數據點分配一個最近的質心，形成 \( K \) 個簇。
3. 根據當前簇的點更新質心，即計算每個簇內所有點的平均值，並將此均值設為新的質心。
4. 重複步驟 2 和 3，直到質心不再改變（或改變非常小），算法收斂。

#### **數學背景**
K-means 的目標是最小化簇內點到質心的平方和誤差（SSE，Sum of Squared Errors），即：
\[
J = \sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2
\]
其中：
- \( J \) 是誤差度量（目標函數）。
- \( x_i \) 是屬於簇 \( C_k \) 的數據點。
- \( \mu_k \) 是簇 \( C_k \) 的質心。
- \( \|x_i - \mu_k\|^2 \) 是點 \( x_i \) 與其質心 \( \mu_k \) 的平方歐氏距離。

---

#### **K-Means 在 scikit-learn 中的實現**

在 `scikit-learn` 中，使用 `KMeans` 類可以輕鬆地實現 K-means 聚類。

```python
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# 創建一個示例數據集
X = np.random.randn(300, 2)  # 300 個隨機生成的 2D 點

# 設置 K 值，這裡選擇 3 個簇
kmeans = KMeans(n_clusters=3, random_state=42)

# 擬合模型並預測簇標籤
labels = kmeans.fit_predict(X)

# 可視化結果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100, label='Centroids')
plt.title("K-Means Clustering")
plt.xlabel("X1")
plt.ylabel("X2")
plt.legend()
plt.show()
```

在這個例子中：
- `n_clusters=3`：指定將數據分為 3 個簇。
- `fit_predict(X)`：首先擬合數據，然後預測每個點的簇標籤。

---

### **2. Spectral Clustering 聚類**

Spectral Clustering 是一種基於圖論的聚類算法，它根據數據的相似度矩陣來進行聚類。這種方法通常用於處理非凸形狀的數據，並且對於 K-means 難以處理的數據結構（如環狀或不規則形狀的簇）非常有效。

#### **Spectral Clustering 的基本步驟**
1. **構建相似度矩陣**：計算數據集的相似度矩陣，通常使用高斯核函數（例如，基於歐氏距離的高斯核）來計算數據點之間的相似度。
2. **計算拉普拉斯矩陣**：根據相似度矩陣計算拉普拉斯矩陣 \( L \)。有不同的拉普拉斯矩陣定義，其中最常用的是對稱拉普拉斯矩陣：
   \[
   L = D - W
   \]
   其中，\( D \) 是度矩陣（對角矩陣，對角線元素是每個節點的度數），\( W \) 是相似度矩陣。
3. **特徵分解**：計算拉普拉斯矩陣的前 \( K \) 個最小特徵值對應的特徵向量，並將這些特徵向量作為新特徵表示。
4. **K-means 聚類**：將這些特徵向量作為輸入，使用 K-means 算法進行聚類，得到最終的簇標籤。

#### **數學背景**
Spectral Clustering 利用圖的拉普拉斯矩陣來捕捉數據點的結構。特徵分解的目的是將數據映射到一個低維空間，然後在這個空間中進行聚類。這樣可以更有效地發現數據中隱藏的結構。

---

#### **Spectral Clustering 在 scikit-learn 中的實現**

在 `scikit-learn` 中，`SpectralClustering` 類提供了這一算法的實現。

```python
from sklearn.cluster import SpectralClustering
import numpy as np
import matplotlib.pyplot as plt

# 創建一個示例數據集
X = np.random.randn(300, 2)  # 300 個隨機生成的 2D 點

# 設置 K 值，這裡選擇 3 個簇
spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)

# 擬合模型並預測簇標籤
labels = spectral.fit_predict(X)

# 可視化結果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.title("Spectral Clustering")
plt.xlabel("X1")
plt.ylabel("X2")
plt.show()
```

在這個例子中：
- `n_clusters=3`：指定將數據分為 3 個簇。
- `affinity='nearest_neighbors'`：指定計算相似度的方式，這裡使用最近鄰來計算相似度矩陣。

---

### **3. 小結**

- **K-means 聚類**：一種基於質心的簡單且高效的聚類算法，適合於處理具有凸形狀的數據。
- **Spectral Clustering**：基於圖論的聚類方法，適用於處理非凸形狀的數據，並且能夠捕捉複雜的數據結構。

這兩種聚類方法各有優勢，K-means 更適用於簡單且均勻的數據集，而 Spectral Clustering 更適用於數據具有複雜結構的情境。