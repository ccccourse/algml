### **AdaBoost 與梯度提升算法（GBDT）**

AdaBoost（Adaptive Boosting）和梯度提升算法（Gradient Boosting Decision Trees, GBDT）都是強化學習算法中的重要方法，旨在通過多個弱學習器（通常是簡單的決策樹）組合成一個強學習器，以提高模型的預測能力。這些算法都是「提升」（boosting）方法的代表，並且具有非常好的分類性能，廣泛應用於各種機器學習任務中。

#### **1. AdaBoost（Adaptive Boosting）**

AdaBoost 是一種增強型的集成學習方法，它通過加強先前弱分類器錯誤分類樣本的權重來實現學習。具體來說，AdaBoost 將多個弱學習器（如決策樹）加以組合，每個弱學習器的權重會根據其錯誤率進行調整。後續的分類器將更加關注錯誤分類的樣本。

**工作原理**：
- 首先，給每個訓練樣本分配一個初始權重，通常所有樣本的權重都設為相等。
- 訓練第一個弱分類器，計算其分類錯誤率。
- 增加錯誤分類樣本的權重，並減少正確分類樣本的權重。
- 訓練第二個弱分類器，對這些加權後的樣本進行學習。
- 重複這一過程，直到達到預定的分類器數量或錯誤率滿足停止條件。
- 最後，通過加權投票的方式將所有弱分類器的預測結果結合起來，得到最終預測。

**數學形式**：
每個弱分類器的預測會根據其錯誤率加權：

\[
\text{最終預測} = \sum_{t=1}^{T} \alpha_t h_t(x)
\]

其中：
- \( h_t(x) \) 是第 \(t\) 個弱分類器對樣本 \(x\) 的預測，
- \( \alpha_t \) 是第 \(t\) 個分類器的權重，根據其錯誤率計算：

\[
\alpha_t = \frac{1}{2} \log \left( \frac{1 - \epsilon_t}{\epsilon_t} \right)
\]

其中 \( \epsilon_t \) 是第 \(t\) 個分類器的錯誤率。

#### **2. 梯度提升算法（GBDT）**

梯度提升算法（GBDT）是一種基於梯度下降方法的集成學習算法。與 AdaBoost 依賴於加權錯誤樣本的不同，GBDT 將每一棵新樹視為對前一棵樹的錯誤進行修正的過程。GBDT 在每一輪中訓練一棵新的樹，這棵樹的目標是對上一輪樹的預測誤差進行擬合。

GBDT 的核心思想是：每一棵新樹都學習上一輪模型的殘差（即預測錯誤）。因此，每一棵樹都是對上一棵樹的錯誤進行修正的。

**工作原理**：
- 首先，初始化模型，通常使用樣本的平均值。
- 在每一步，計算當前模型的預測誤差（殘差），然後用一棵新的決策樹來擬合這些誤差。
- 通過對殘差進行擬合，來改進模型的預測。
- 重複多次這個過程，每次生成一棵新樹，並將其加到現有模型中。
- 最後，通過將多棵樹的預測加權平均（通常是加權和）來得到最終的預測。

**數學形式**：
GBDT 的更新公式如下：

\[
f_{m+1}(x) = f_m(x) + \eta \cdot h_{m+1}(x)
\]

其中：
- \( f_m(x) \) 是第 \(m\) 輪模型的預測結果，
- \( h_{m+1}(x) \) 是在第 \(m+1\) 輪擬合的模型（即樹）的預測，
- \( \eta \) 是學習率，用於控制每棵樹的影響程度。

每次新樹的擬合是對當前殘差的擬合，目標是最小化損失函數的梯度。對於回歸問題，通常使用均方誤差（MSE）作為損失函數，而對於分類問題，則使用對數損失函數。

#### **3. AdaBoost 與 GBDT 的比較**

| 方面                | AdaBoost                               | GBDT                              |
|---------------------|----------------------------------------|-----------------------------------|
| 基本原理            | 通過加強錯誤樣本的權重來增強分類器     | 每棵樹擬合前一棵樹的預測誤差     |
| 每輪迭代學習目標    | 根據錯誤率調整樣本權重並訓練新分類器   | 對殘差進行擬合，修正前一輪的誤差 |
| 模型組合方式        | 加權投票結合弱分類器                   | 加權平均多棵樹的預測             |
| 適用範圍            | 二分類問題、回歸問題                  | 二分類問題、回歸問題             |
| 算法特點            | 訓練過程較快，但對離群點較為敏感       | 可以處理複雜的非線性關係，較為強大 |
| 超參數控制          | 主要通過迭代次數與學習率進行調整       | 訓練過程中需要控制樹的深度、學習率等超參數 |

#### **4. 程式範例**

以下是使用 `scikit-learn` 實現 AdaBoost 和 GBDT 的簡單範例。

**AdaBoost 實現：**

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 載入資料集
data = load_iris()
X = data.data
y = data.target

# 分割資料集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 建立基學習器（決策樹）
base_estimator = DecisionTreeClassifier(max_depth=1)

# 建立 AdaBoost 模型
ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50)

# 訓練模型
ada_boost.fit(X_train, y_train)

# 預測
y_pred = ada_boost.predict(X_test)

# 評估模型
print(f"AdaBoost Accuracy: {accuracy_score(y_test, y_pred)}")
```

**GBDT 實現：**

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 載入資料集
data = load_iris()
X = data.data
y = data.target

# 分割資料集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 建立 GBDT 模型
gbdt = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=3)

# 訓練模型
gbdt.fit(X_train, y_train)

# 預測
y_pred = gbdt.predict(X_test)

# 評估模型
print(f"GBDT Accuracy: {accuracy_score(y_test, y_pred)}")
```

#### **5. 小結**

- **AdaBoost** 通過加強錯誤分類的樣本來增強模型的分類能力，較為簡單且直觀，適用於較為線性的問題。
- **梯度提升算法（GBDT）** 通過擬合殘差來修正模型的誤差，對非線性問題具有較強的擬合能力，並且能夠處理複雜的數據關係。
- 兩者都屬於提升方法，但其核心思想和實現方式有所不同，AdaBoost 更注重樣本權重的調整，而 GBDT 則通過梯度下降來優化模型。