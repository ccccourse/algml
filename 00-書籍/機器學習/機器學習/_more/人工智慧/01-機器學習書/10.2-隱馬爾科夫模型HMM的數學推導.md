### **隱馬爾科夫模型（HMM）的數學推導**

#### **隱馬爾科夫模型（HMM）概述**

隱馬爾科夫模型（Hidden Markov Model, HMM）是一種統計模型，用來描述具有隱藏狀態的隨機過程。HMM 假設系統的狀態在每個時間步都是隱藏的，並且這些隱藏的狀態按照馬爾科夫性質進行轉移，且每個狀態會生成觀察數據。HMM 特別適用於序列數據的建模，如語音識別、時間序列分析、基因序列分析等。

HMM 由以下三個主要成分組成：
1. **隱藏狀態集合（State space）**：假設系統在每個時間步都有一個隱藏狀態，這些狀態不可直接觀察，且遵循馬爾科夫性質。這些隱藏狀態常記為 \( S = \{S_1, S_2, \dots, S_N\} \)，其中 \( N \) 是隱藏狀態的數量。
2. **觀察符號集合（Observation space）**：系統的每個隱藏狀態會對應一個觀察符號。觀察符號的集合記為 \( O = \{O_1, O_2, \dots, O_M\} \)，其中 \( M \) 是觀察符號的數量。
3. **模型參數**：
   - \( A = \{a_{ij}\} \)：轉移概率矩陣，描述從狀態 \( S_i \) 轉移到狀態 \( S_j \) 的概率，滿足 \( a_{ij} = P(S_j | S_i) \)。
   - \( B = \{b_j(k)\} \)：發射概率矩陣，描述在隱藏狀態 \( S_j \) 下觀察到符號 \( O_k \) 的概率，滿足 \( b_j(k) = P(O_k | S_j) \)。
   - \( \pi = \{\pi_i\} \)：初始狀態分佈，表示在初始時間步 \( t = 1 \) 時系統處於狀態 \( S_i \) 的概率，滿足 \( \pi_i = P(S_i) \)。

#### **HMM 的數學推導**

隱馬爾科夫模型的目標是根據觀察數據序列來推斷隱藏狀態的序列。常見的問題有三類：
1. **評估問題（Evaluation problem）**：給定觀察數據序列，計算其在模型下的概率 \( P(O | \lambda) \)，其中 \( \lambda = (A, B, \pi) \) 是模型參數。
2. **解碼問題（Decoding problem）**：給定觀察數據序列，推斷最可能的隱藏狀態序列 \( Q = (q_1, q_2, \dots, q_T) \)。
3. **學習問題（Learning problem）**：根據觀察數據序列 \( O = (O_1, O_2, \dots, O_T) \) 和模型的結構，估計最適合的模型參數 \( A, B, \pi \)。

#### **評估問題：計算觀察序列的概率**

計算觀察序列 \( O = (O_1, O_2, \dots, O_T) \) 的概率 \( P(O | \lambda) \)，可以使用動態規劃的方法，具體是前向算法（Forward Algorithm）。

1. **前向變量（Forward variable）**：
   定義 \( \alpha_t(i) \) 為在時間 \( t \) 時，系統處於狀態 \( S_i \) 且觀察序列為 \( O_1, O_2, \dots, O_t \) 的概率，即：
   \[
   \alpha_t(i) = P(O_1, O_2, \dots, O_t, S_t = S_i | \lambda)
   \]
   
   初始步：
   \[
   \alpha_1(i) = \pi_i b_i(O_1)
   \]
   
   遞歸步：
   \[
   \alpha_t(i) = \left[ \sum_{j=1}^{N} \alpha_{t-1}(j) a_{ji} \right] b_i(O_t)
   \]

   最終步：
   \[
   P(O | \lambda) = \sum_{i=1}^{N} \alpha_T(i)
   \]

   這樣，前向算法可以計算給定模型和觀察序列的總概率。

#### **解碼問題：推斷最可能的隱藏狀態序列**

解碼問題是指，根據觀察序列 \( O \)，找到最可能的隱藏狀態序列 \( Q = (q_1, q_2, \dots, q_T) \)。這可以通過維特比算法（Viterbi Algorithm）來實現。

1. **維特比變量（Viterbi variable）**：
   定義 \( \delta_t(i) \) 為在時間 \( t \) 時，系統處於狀態 \( S_i \) 且觀察序列為 \( O_1, O_2, \dots, O_t \) 的最大概率：
   \[
   \delta_t(i) = \max_{q_1, \dots, q_{t-1}} P(O_1, O_2, \dots, O_t, q_t = S_i | \lambda)
   \]
   
   初始步：
   \[
   \delta_1(i) = \pi_i b_i(O_1)
   \]
   
   遞歸步：
   \[
   \delta_t(i) = \max_{j=1}^{N} \left( \delta_{t-1}(j) a_{ji} \right) b_i(O_t)
   \]
   
   最終步：
   \[
   P^*(O | \lambda) = \max_{i=1}^{N} \delta_T(i)
   \]
   
   這樣可以計算出最可能的隱藏狀態序列。

#### **學習問題：估計模型參數**

學習問題是指，在已知觀察數據 \( O \) 的情況下，如何估計模型的參數 \( A, B, \pi \)。這可以使用 Baum-Welch 算法（EM 算法的一種特例）來解決。

1. **E 步驟：** 計算每個隱藏狀態在每個時間步的責任度（期望），即 \( \gamma_t(i) = P(S_t = S_i | O, \lambda) \) 和 \( \xi_t(i, j) = P(S_t = S_i, S_{t+1} = S_j | O, \lambda) \)。
   
2. **M 步驟：** 根據 E 步驟中的期望值來更新參數：
   \[
   \pi_i = \gamma_1(i)
   \]
   \[
   a_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i, j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
   \]
   \[
   b_j(k) = \frac{\sum_{t=1}^{T} \gamma_t(j) \mathbf{1}(O_t = O_k)}{\sum_{t=1}^{T} \gamma_t(j)}
   \]

   重複進行 E 步驟和 M 步驟，直到參數收斂。

### **總結**

隱馬爾科夫模型是一個強大的工具，能夠用來建模隱藏狀態和觀察序列之間的關係。其核心數學推導包括前向算法來計算觀察序列的概率、維特比算法來推斷最可能的隱藏狀態序列，以及使用 Baum-Welch 算法來估計模型參數。HMM 在許多應用中表現出色，特別是在序列數據的建模和預測中。