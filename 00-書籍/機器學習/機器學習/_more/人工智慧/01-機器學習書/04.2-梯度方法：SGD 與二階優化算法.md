### **梯度方法：SGD 與二階優化算法**

在機器學習中，優化算法的目標是調整模型的參數，以最小化損失函數或達到最佳的預測效果。梯度方法是最常用的優化技術，它利用損失函數對參數的梯度來更新參數，直到找到最小值或收斂到理想解。根據梯度更新規則的不同，梯度方法可分為一階和二階優化方法。以下將介紹常見的一階優化方法：隨機梯度下降（SGD），以及二階優化算法。

---

#### **1. 隨機梯度下降（SGD）**

隨機梯度下降（Stochastic Gradient Descent, SGD）是一種基於梯度的優化方法，它在每一步中利用訓練數據中的一個樣本來計算損失函數的梯度，然後更新模型的參數。與批量梯度下降（BGD）不同，SGD 每次更新的參數是基於單個樣本或小批量數據，這樣的更新會更加頻繁，但會導致更新具有較大的波動。

**SGD 公式：**

\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t; x^{(i)}, y^{(i)})
\]

其中：
- \( \theta_t \) 是第 \( t \) 次迭代中的參數。
- \( \eta \) 是學習率（步長），決定每次更新的幅度。
- \( \nabla_\theta J(\theta_t; x^{(i)}, y^{(i)}) \) 是損失函數 \( J \) 對參數 \( \theta \) 的梯度，基於樣本 \( (x^{(i)}, y^{(i)}) \)。

**SGD 的優缺點：**
- **優點**：
  - 計算量小，每次迭代只需計算一個樣本的梯度，速度較快。
  - 適合大規模數據集，能夠在線學習。
- **缺點**：
  - 更新過程中的波動較大，可能會導致參數無法精確收斂。
  - 需要合理的學習率設置來避免過大或過小的步長。

---

#### **2. 二階優化算法**

二階優化算法在每次參數更新時，不僅使用梯度信息，還會使用損失函數的二階導數（即Hessian矩陣）來改進更新過程。這些方法可以在某些情況下更有效率地收斂，尤其是在凸問題中。然而，計算二階導數需要更多的計算資源，因此在大規模數據集上使用較少。

常見的二階優化方法有 **牛頓法（Newton's Method）** 和 **擴展牛頓法（Quasi-Newton Methods）**。

---

#### **2.1 牛頓法（Newton's Method）**

牛頓法是一種使用二階導數信息來加速優化過程的算法。其更新規則為：

\[
\theta_{t+1} = \theta_t - \eta H^{-1} \nabla_\theta J(\theta_t)
\]

其中：
- \( \nabla_\theta J(\theta_t) \) 是梯度。
- \( H \) 是損失函數 \( J(\theta) \) 的 Hessian 矩陣（即二階導數矩陣），可以表示為：

\[
H = \frac{\partial^2 J(\theta)}{\partial \theta^2}
\]

牛頓法使用梯度和 Hessian 矩陣來進行參數更新，這樣可以更精確地調整參數。

**牛頓法的優缺點：**
- **優點**：
  - 收斂速度較快，特別是對於凸優化問題。
  - 適合於高維參數空間，能夠在少數幾次迭代中找到最優解。
- **缺點**：
  - 計算 Hessian 矩陣的成本非常高，尤其是在高維空間。
  - 需要正確的初始化，且不適用於非凸問題。

---

#### **2.2 擴展牛頓法（Quasi-Newton Methods）**

為了避免計算完整的 Hessian 矩陣，擴展牛頓法提出了近似的方式。常見的擴展牛頓法包括 **BFGS（Broyden–Fletcher–Goldfarb–Shanno）** 和 **L-BFGS（Limited-memory BFGS）**。

BFGS 會利用前幾次迭代的梯度和參數信息來近似 Hessian 矩陣，進行高效更新。L-BFGS 是 BFGS 的一種改進，專門針對大規模問題，通過儲存有限的歷史信息來減少內存消耗。

**BFGS 和 L-BFGS 的優缺點：**
- **優點**：
  - 不需要計算完整的 Hessian 矩陣，降低計算複雜度。
  - 相對較快的收斂速度，特別是在大部分問題中。
  - L-BFGS 更適合大規模數據集。
- **缺點**：
  - 需要額外的內存和計算資源來儲存過往的梯度信息。
  - 還是需要一定的計算成本，尤其是在高維問題中。

---

#### **3. Python 程式範例：隨機梯度下降（SGD）**

以下是使用 `scikit-learn` 實現隨機梯度下降來進行線性回歸的一個簡單例子：

```python
import numpy as np
from sklearn.linear_model import SGDRegressor
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt

# 生成簡單的回歸數據
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# 初始化 SGDRegressor，並設置學習率
sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.01)

# 擬合模型
sgd_regressor.fit(X, y)

# 預測結果
y_pred = sgd_regressor.predict(X)

# 繪製結果
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, y_pred, color='red', label='SGD Fit')
plt.legend()
plt.show()

# 顯示最終的參數
print("Final coefficients:", sgd_regressor.coef_)
print("Final intercept:", sgd_regressor.intercept_)
```

在這段程式碼中，我們使用 `SGDRegressor` 模型來進行線性回歸。使用隨機梯度下降法來擬合簡單的回歸數據集。

---

#### **4. 小結**

- **SGD**：隨機梯度下降是一種一階優化方法，適用於大規模數據集，能夠快速計算並進行更新。其主要缺點是更新過程中的波動性，可能會使得參數無法精確收斂。
- **二階優化算法**：牛頓法和擴展牛頓法利用二階導數（Hessian 矩陣）來改善更新步驟，從而提高收斂速度，尤其在凸優化問題中更為有效。然而，這些方法的計算成本較高，尤其在處理高維數據時。

這些優化方法廣泛應用於機器學習中的各類模型，尤其在深度學習和支持向量機（SVM）等領域。