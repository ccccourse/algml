### **Logistic 回歸的損失函數與梯度推導**

#### **1. Logistic 回歸概述**
Logistic 回歸是一種常見的分類算法，用於預測二分類問題中的樣本屬於某一類的概率。與線性回歸不同，Logistic 回歸的目的是估計某個樣本屬於某一類的概率，而不是預測連續的數值。

假設我們有一組特徵 \( \mathbf{x} = (x_1, x_2, \dots, x_n) \)，並且我們的目標是預測這些特徵對應的樣本屬於類別 1 的概率。Logistic 回歸的模型可以寫為：

\[
p(y = 1 | \mathbf{x}) = \sigma(w^T \mathbf{x} + b)
\]

其中：
- \( p(y = 1 | \mathbf{x}) \) 是樣本屬於類別 1 的概率，
- \( \sigma(z) = \frac{1}{1 + e^{-z}} \) 是 **Sigmoid 函數**，用來將輸入的線性組合轉換為概率值，
- \( w \) 是權重向量，\( b \) 是偏置項。

因此，對於每個樣本 \( \mathbf{x} \)，Logistic 回歸預測其屬於類別 1 的概率，並且 \( p(y = 0 | \mathbf{x}) = 1 - p(y = 1 | \mathbf{x}) \)，即類別 0 的概率。

---

#### **2. 損失函數**
在二分類問題中，常用的損失函數是 **對數損失函數**（Log-Loss），又稱為 **交叉熵損失函數**。對於每個樣本，其損失可以定義為：

\[
L(y, \hat{y}) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
\]

其中：
- \( y \in \{0, 1\} \) 是實際標籤，
- \( \hat{y} = \sigma(w^T \mathbf{x} + b) \) 是模型預測的概率。

對於所有訓練樣本，總損失函數（即平均損失）可以寫為：

\[
J(w, b) = - \frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right)
\]

其中：
- \( m \) 是樣本數量，
- \( y^{(i)} \) 和 \( \hat{y}^{(i)} \) 分別是第 \( i \) 個樣本的實際標籤和預測概率。

這個損失函數度量了預測的概率 \( \hat{y} \) 與實際標籤 \( y \) 之間的差異，目的是最小化這個損失函數，使得預測值與實際值之間的差異最小。

---

#### **3. 梯度推導**
為了最小化損失函數 \( J(w, b) \)，我們需要計算關於權重 \( w \) 和偏置 \( b \) 的梯度。這樣，我們可以使用梯度下降法來更新參數。首先，我們需要對 \( J(w, b) \) 分別對 \( w \) 和 \( b \) 求導。

##### **對 \( w_j \) 的偏導數**：
對於第 \( j \) 個特徵的權重 \( w_j \)，其偏導數為：

\[
\frac{\partial J(w, b)}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right) x_j^{(i)}
\]

這個導數表示模型對於每個特徵的權重調整方向，依賴於預測值 \( \hat{y}^{(i)} \) 和實際標籤 \( y^{(i)} \) 之間的誤差，以及該特徵 \( x_j^{(i)} \) 的值。

##### **對 \( b \) 的偏導數**：
對於偏置項 \( b \)，其偏導數為：

\[
\frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)
\]

偏置項的梯度是所有樣本誤差的平均值，代表了模型整體預測偏差的調整。

---

#### **4. 梯度下降更新規則**
利用上面推導的梯度，我們可以使用梯度下降法來更新模型參數。對於每個參數，我們根據其對應的梯度進行更新：

\[
w_j := w_j - \alpha \cdot \frac{\partial J(w, b)}{\partial w_j}
\]
\[
b := b - \alpha \cdot \frac{\partial J(w, b)}{\partial b}
\]

其中：
- \( \alpha \) 是學習率，決定每次更新的步長。

這樣的更新規則將使損失函數 \( J(w, b) \) 在每次迭代中逐步減少，直到收斂到最小值，從而得到最優的模型參數。

---

#### **5. 小結**
- **Logistic 回歸** 是一種用於二分類問題的回歸模型，其核心是使用 Sigmoid 函數將線性輸入映射到概率。
- **損失函數** 使用對數損失（或交叉熵損失）來衡量預測概率與實際標籤之間的差異。
- 梯度推導過程幫助我們計算權重和偏置項的更新規則，並利用 **梯度下降** 優化模型。

通過這些數學推導，我們可以理解 Logistic 回歸的學習過程，並能夠更好地掌握如何應用該方法進行分類問題的建模。