### **scikit-learn 的性能度量工具**

在機器學習中，性能度量是評估模型預測效果的關鍵。scikit-learn 提供了多種工具來計算和評估模型的性能，這些工具涵蓋了分類、回歸、多類別問題以及更多評估指標。以下是一些常用的性能度量工具及其使用方式。

#### **1. 常見分類評估指標**

- **混淆矩陣（Confusion Matrix）**
  
  混淆矩陣是用來顯示模型的預測結果與真實標籤之間的對比，適用於分類問題。每行表示實際類別，每列表示預測類別。

  ```python
  from sklearn.metrics import confusion_matrix
  cm = confusion_matrix(y_true, y_pred)
  ```

  其中，`y_true` 是真實標籤，`y_pred` 是模型的預測標籤。

- **精確率、召回率、F1 分數（Precision, Recall, F1 Score）**
  
  這些指標用來評估分類模型在不同類別上的表現，`precision_recall_fscore_support` 可以計算這些指標。

  ```python
  from sklearn.metrics import precision_recall_fscore_support
  precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)
  ```

  - **精確率（Precision）**: 預測為正的樣本中有多少是正確的。
  - **召回率（Recall）**: 實際為正的樣本中有多少被正確預測為正。
  - **F1 分數**: 精確率和召回率的調和平均值。

- **分類報告（Classification Report）**
  
  `classification_report` 提供了每一類別的精確率、召回率、F1 分數，以及宏觀、微觀和加權平均值。

  ```python
  from sklearn.metrics import classification_report
  print(classification_report(y_true, y_pred))
  ```

- **ROC 曲線與 AUC（ROC Curve and AUC）**
  
  ROC 曲線顯示了不同閾值下分類模型的性能，AUC（Area Under the Curve）是 ROC 曲線下的面積，越大表示模型性能越好。

  ```python
  from sklearn.metrics import roc_curve, auc
  fpr, tpr, thresholds = roc_curve(y_true, y_prob)
  roc_auc = auc(fpr, tpr)
  ```

  其中，`y_prob` 是模型對每個樣本屬於正類的概率，通常對於二分類模型，`y_prob` 是正類的預測概率。

#### **2. 常見回歸評估指標**

- **均方誤差（Mean Squared Error, MSE）**
  
  MSE 衡量預測值與實際值之間的差異，計算公式為所有誤差的平方和的平均。

  ```python
  from sklearn.metrics import mean_squared_error
  mse = mean_squared_error(y_true, y_pred)
  ```

- **均方根誤差（Root Mean Squared Error, RMSE）**
  
  RMSE 是 MSE 的平方根，與原始數據的單位一致，便於理解。

  ```python
  rmse = mean_squared_error(y_true, y_pred, squared=False)
  ```

- **決定係數（R²）**
  
  R² 衡量回歸模型的解釋能力，值越接近 1 表示模型越好。

  ```python
  from sklearn.metrics import r2_score
  r2 = r2_score(y_true, y_pred)
  ```

- **平均絕對誤差（Mean Absolute Error, MAE）**
  
  MAE 衡量預測值與實際值之間的平均絕對差異。

  ```python
  from sklearn.metrics import mean_absolute_error
  mae = mean_absolute_error(y_true, y_pred)
  ```

#### **3. 多類別評估指標**

- **精確率、召回率、F1 分數的多類別評估**

  在多類別分類中，這些指標可以使用 `precision_recall_fscore_support` 進行計算。

  ```python
  from sklearn.metrics import precision_recall_fscore_support
  precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='macro')
  ```

  - `average='macro'`: 計算所有類別的平均值，不考慮類別的不平衡。
  - `average='micro'`: 計算所有類別的總體精確率和召回率，再進行平均。
  - `average='weighted'`: 計算每個類別的指標，再加權平均，權重為該類別的樣本數。

- **加權平均（Weighted Average）**

  如上所述，加權平均考慮了每個類別的樣本數，使得類別不均衡時能提供更真實的評估。

  ```python
  from sklearn.metrics import classification_report
  print(classification_report(y_true, y_pred, average='weighted'))
  ```

#### **4. 複雜度與時間性能**

- **計算時間**

  在評估模型的性能時，計算模型的訓練時間和預測時間也是必要的，尤其是在處理大數據集或複雜模型時。

  ```python
  import time
  start_time = time.time()
  model.fit(X_train, y_train)
  print(f"Training time: {time.time() - start_time} seconds")

  start_time = time.time()
  model.predict(X_test)
  print(f"Prediction time: {time.time() - start_time} seconds")
  ```

- **計算複雜度**

  可以使用 `sklearn.model_selection.cross_val_score` 進行交叉驗證，評估模型的泛化能力和穩定性。

  ```python
  from sklearn.model_selection import cross_val_score
  scores = cross_val_score(model, X, y, cv=5)
  print(f"Cross-validation scores: {scores}")
  ```

#### **5. scikit-learn 的性能度量工具總結**

- `confusion_matrix`：計算混淆矩陣，適用於分類問題。
- `precision_recall_fscore_support`：計算精確率、召回率、F1 分數等指標，支持多類別。
- `classification_report`：顯示分類模型的精確率、召回率、F1 分數等指標。
- `roc_curve` 和 `auc`：計算 ROC 曲線和 AUC 值，適用於二分類問題。
- `mean_squared_error`、`r2_score` 等：評估回歸模型的誤差和解釋能力。
- `cross_val_score`：交叉驗證，評估模型的泛化能力。

這些工具不僅能幫助我們理解模型的預測效果，還能提供對不同模型的比較，幫助選擇最適合的算法。