### **混淆矩陣與 AUC 的數學定義**

在分類模型的評估中，混淆矩陣和 AUC（Area Under the Curve）是兩個重要的指標，幫助我們理解模型的預測性能。這些指標提供了更多的信息，比單純的準確度更能反映模型在不同情況下的表現。

#### **1. 混淆矩陣（Confusion Matrix）**

混淆矩陣是一種用來描述分類模型預測結果的工具，將模型的預測結果與實際標籤進行比較，並將其以矩陣的形式呈現。對於二分類問題，混淆矩陣通常是 2x2 的矩陣，分別表示預測為正類（positive）和負類（negative）以及實際的正類和負類。

假設我們有一個二分類問題，其中：
- **True Positive (TP)**: 實際為正類且預測為正類。
- **True Negative (TN)**: 實際為負類且預測為負類。
- **False Positive (FP)**: 實際為負類但預測為正類。
- **False Negative (FN)**: 實際為正類但預測為負類。

則混淆矩陣如下所示：

|               | Predicted Positive | Predicted Negative |
|---------------|-------------------|-------------------|
| **Actual Positive** | TP                | FN                |
| **Actual Negative** | FP                | TN                |

#### **2. 混淆矩陣中的評估指標**

從混淆矩陣中，我們可以計算出幾個重要的評估指標：

- **準確率（Accuracy）**：模型正確預測的比例。
  \[
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  \]
  
- **精確率（Precision）**：預測為正類的樣本中，實際為正類的比例。
  \[
  \text{Precision} = \frac{TP}{TP + FP}
  \]
  
- **召回率（Recall）**（也稱為靈敏度或真陽性率）：實際為正類的樣本中，預測為正類的比例。
  \[
  \text{Recall} = \frac{TP}{TP + FN}
  \]
  
- **F1 分數（F1-Score）**：精確率和召回率的調和平均數，當精確率和召回率不平衡時使用，綜合考量兩者。
  \[
  \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  \]

#### **3. AUC（Area Under the Curve）**

AUC 是 ROC 曲線下的面積，它用來衡量分類模型區分正類與負類的能力。ROC（Receiver Operating Characteristic）曲線是以假陽性率（False Positive Rate, FPR）為橫軸，真陽性率（True Positive Rate, TPR）為縱軸所繪製的曲線。

- **假陽性率（FPR）**：
  \[
  \text{FPR} = \frac{FP}{FP + TN}
  \]
  
- **真陽性率（TPR）**（即召回率）：
  \[
  \text{TPR} = \frac{TP}{TP + FN}
  \]

AUC 的值範圍從 0 到 1，其中：
- **AUC = 0.5**：模型沒有區分能力，相當於隨機猜測。
- **AUC = 1**：模型能完美區分正類和負類。
- **AUC < 0.5**：模型表現比隨機猜測還差，通常是因為模型的預測類別被反轉。

AUC 通常用來比較不同模型的性能，AUC 值越高，表示模型的區分能力越強。

#### **4. 混淆矩陣與 AUC 的聯繫**

- **混淆矩陣** 主要用來分析模型在不同分類情況下的預測表現。它能夠給出具體的預測結果數字，從而計算出精確率、召回率、F1 分數等指標。
  
- **AUC** 則是通過考慮不同的分類閾值來評估模型的區分能力，通常用來評估模型在各種閾值下的表現，而不是單一的準確度。AUC 是基於 ROC 曲線的，能夠直觀地顯示模型在不同假陽性率下的真陽性率，並給出一個綜合評價。

#### **程式範例：混淆矩陣與 AUC 的計算**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 加載數據集
data = load_iris()
X = data.data
y = data.target

# 只選擇二分類的數據
X = X[y != 0]
y = y[y != 0]

# 分割數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 創建隨機森林分類器
model = RandomForestClassifier()

# 訓練模型
model.fit(X_train, y_train)

# 預測結果
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # 取得正類的預測概率

# 計算混淆矩陣
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# 計算 AUC
auc = roc_auc_score(y_test, y_prob)
print(f"AUC: {auc:.3f}")

# 計算 ROC 曲線
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# 畫出 ROC 曲線
plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

**程式解釋：**
- `confusion_matrix` 用來計算混淆矩陣。
- `roc_auc_score` 用來計算 AUC 值。
- `roc_curve` 用來計算 ROC 曲線的各個點，並繪製 ROC 曲線。

### **總結**

- **混淆矩陣** 提供了分類結果的詳細信息，包括真正例（TP）、假正例（FP）、真反例（TN）和假反例（FN）。這些值可用來計算精確率、召回率、F1 分數等指標。
- **AUC** 是評估模型區分能力的指標，通過 ROC 曲線下的面積來衡量模型在不同閾值下的表現，AUC 越高，表示模型的區分能力越強。