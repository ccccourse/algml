### 貝爾曼方程（Bellman Equation）

貝爾曼方程是強化學習和動態規劃中的一個核心概念，它提供了一種描述最優策略或最優價值函數的遞歸關係。貝爾曼方程的基本思想是將未來的回報拆解為當前的回報和未來的回報之和，從而通過這個關係遞迴計算最優解。

### 1. 貝爾曼方程的形式

#### 狀態價值函數（Value Function）

狀態價值函數 \( V^\pi(s) \) 代表從狀態 \(s\) 開始，遵循策略 \( \pi \) 的智能體能夠期望獲得的回報。貝爾曼方程可以用來描述這一價值函數。

對於給定策略 \( \pi \)，狀態價值函數 \( V^\pi(s) \) 的貝爾曼方程為：

\[
V^\pi(s) = \mathbb{E}\left[ R(s, \pi(s)) + \gamma V^\pi(s') \right]
\]

其中：
- \( R(s, \pi(s)) \) 是在狀態 \(s\) 下執行策略 \( \pi \) 所得到的即時回報，
- \( \gamma \) 是折扣因子，
- \( s' \) 是從狀態 \(s\) 執行 \( \pi(s) \) 行為後轉移到的下一狀態。

貝爾曼方程將當前的價值 \( V^\pi(s) \) 拆解為當前回報和未來回報的加權總和，從而建立了價值函數的遞歸關係。

#### 最優狀態價值函數（Optimal Value Function）

最優狀態價值函數 \( V^*(s) \) 表示在每個狀態下，遵循最優策略 \( \pi^* \) 所能獲得的最大期望回報。最優價值函數的貝爾曼方程為：

\[
V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V^*(s') \right]
\]

這裡：
- \( R(s, a) \) 是在狀態 \(s\) 下執行行為 \(a\) 所得到的回報，
- \( P(s' | s, a) \) 是從狀態 \(s\) 執行行為 \(a\) 後轉移到狀態 \(s'\) 的概率。

最優狀態價值函數的貝爾曼方程描述了如何從當前狀態 \(s\) 開始，選擇一個最優行為 \(a\) 以最大化未來回報。

#### Q函數（Action-Value Function）

Q函數 \( Q^\pi(s, a) \) 代表在狀態 \(s\) 下執行行為 \(a\) 並遵循策略 \( \pi \) 所能夠期望獲得的回報。Q函數的貝爾曼方程為：

\[
Q^\pi(s, a) = \mathbb{E}\left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V^\pi(s') \right]
\]

這是狀態-行為價值函數的遞歸形式，描述了在給定狀態 \(s\) 和行為 \(a\) 的情況下，智能體期望獲得的回報。

#### 最優Q函數（Optimal Q-function）

最優Q函數 \( Q^*(s, a) \) 描述了在狀態 \(s\) 下選擇行為 \(a\) 會獲得的最大期望回報。最優Q函數的貝爾曼方程為：

\[
Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) \max_{a'} Q^*(s', a')
\]

最優Q函數的貝爾曼方程展示了從狀態 \(s\) 執行行為 \(a\) 後，獲得的即時回報加上從新狀態 \(s'\) 開始執行最優策略的最大期望回報。

### 2. 貝爾曼方程的應用

貝爾曼方程是求解MDP的核心工具。在強化學習中，算法如價值迭代（Value Iteration）和策略迭代（Policy Iteration）都利用貝爾曼方程來計算最優策略和最優價值函數。

#### 價值迭代（Value Iteration）

價值迭代通過反覆計算最優價值函數，最終收斂到最優解。每次迭代基於貝爾曼方程更新價值函數：

\[
V_{t+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V_t(s') \right]
\]

#### 策略迭代（Policy Iteration）

策略迭代包含兩個主要步驟：策略評估和策略改善。在策略評估階段，根據當前策略計算價值函數，然後用貝爾曼方程更新價值函數。

### 3. 貝爾曼方程的簡單範例

假設有一個簡單的MDP，狀態空間 \( S = \{s_0, s_1\} \)，行為空間 \( A = \{a_0, a_1\} \)，轉移函數和回報如下：

- \( R(s_0, a_0) = 1 \), \( R(s_1, a_1) = 2 \)
- 轉移概率：\( P(s_0|s_0, a_0) = 0.8 \), \( P(s_1|s_0, a_0) = 0.2 \), \( P(s_1|s_1, a_1) = 1.0 \)
- 折扣因子 \( \gamma = 0.9 \)

我們可以使用貝爾曼方程來計算最優狀態價值函數。

```python
import numpy as np

# 定義轉移概率和回報
R = np.array([[1, 0], [0, 2]])  # 兩個狀態和兩個行為的回報
P = np.array([[[0.8, 0.2], [0, 1]], [[0, 0], [1, 0]]])  # 轉移概率

gamma = 0.9  # 折扣因子
states = [0, 1]  # 狀態空間

# 初始化價值函數
V = np.zeros(len(states))

# 貝爾曼方程的迭代計算
for _ in range(100):
    V_new = np.copy(V)
    for s in states:
        V_new[s] = max(sum(P[s][a] * (R[s][a] + gamma * V[s_]) for s_, a in zip(states, range(len(P[s])))) for a in range(len(P[s])))
    V = V_new

print("最優價值函數 V*: ", V)
```

### 4. 小結

貝爾曼方程是強化學習和動態規劃中的一個關鍵工具，通過遞歸方式描述最優價值函數和最優策略的計算。在強化學習中，通過利用貝爾曼方程的遞歸性質，可以在多次交互過程中學習到最優的行為策略。