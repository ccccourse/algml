### 馬可夫決策過程（MDP）理論

馬可夫決策過程（Markov Decision Process, MDP）是強化學習中用來描述一個環境的數學模型。它提供了一種結構化的方式來描述智能體與環境之間的交互過程。在強化學習中，智能體的目標是最大化它在與環境交互過程中獲得的累積回報（reward）。

### 1. MDP的基本組成

馬可夫決策過程由以下五個基本組成部分構成：

1. **狀態空間（State Space, S）**：系統可能的所有狀態的集合。每個狀態代表了環境的某一種可能配置。

2. **行為空間（Action Space, A）**：智能體在每個狀態下可以選擇的所有行為的集合。

3. **轉移函數（Transition Function, P）**：描述智能體選擇某一行為後，從當前狀態轉移到下一狀態的概率。通常用 \(P(s' | s, a)\) 表示，其中 \(s'\) 是下一狀態，\(s\) 是當前狀態，\(a\) 是當前行為。這個轉移概率必須符合馬可夫性質，即未來狀態的分佈僅依賴於當前狀態和當前行為，而與之前的歷史無關。

4. **獎勳函數（Reward Function, R）**：描述智能體在某一狀態下執行某一行為後所獲得的即時獎勳。通常表示為 \(R(s, a)\)，即智能體在狀態 \(s\) 下執行行為 \(a\) 所獲得的獎勳。

5. **折扣因子（Discount Factor, γ）**：表示未來回報相對於當前回報的重要性。折扣因子 \(γ\) 的範圍是 [0, 1]。當 \(γ = 1\) 時，智能體將未來的回報視為與當前回報同等重要；當 \(γ = 0\) 時，智能體只關注當前回報，忽略未來的回報。

### 2. MDP的數學表達

馬可夫決策過程可以用一組五元組來表示：
\[
MDP = (S, A, P, R, \gamma)
\]
其中：
- \(S\) 是狀態空間，
- \(A\) 是行為空間，
- \(P(s' | s, a)\) 是轉移函數，
- \(R(s, a)\) 是獎勳函數，
- \(\gamma\) 是折扣因子。

### 3. MDP中的目標

在強化學習中，智能體的目標是通過選擇最優的行為策略 \(π: S \to A\)，來最大化它的期望回報。具體來說，智能體希望最大化在整個過程中獲得的折扣累積獎勳，這通常表達為：

\[
V^\pi(s) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s, \pi \right]
\]
其中 \(V^\pi(s)\) 表示在狀態 \(s\) 下，根據策略 \(\pi\) 執行行為的期望回報。

### 4. 策略與價值函數

- **策略（Policy, \(π\))**：策略是智能體選擇行為的規則。它可以是確定性的或隨機的，並且通常表示為一個映射 \(π: S \to A\)，即給定一個狀態 \(s\)，策略 \(π\) 決定智能體選擇的行為 \(a\)。
  
- **價值函數（Value Function, \(V^\pi\))**：價值函數衡量的是在給定策略 \(π\) 下，從狀態 \(s\) 開始，智能體可以獲得的預期回報。這是智能體的“預期利益”，即從某一狀態出發，依照策略 \(π\) 執行後的回報總和。

- **Q函數（Q-Function, \(Q^\pi\))**：Q函數則是衡量智能體在某一狀態 \(s\) 下執行某一行為 \(a\) 並根據策略 \(π\) 執行後，能夠獲得的預期回報。Q函數定義為：
  \[
  Q^\pi(s, a) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s, a_0 = a, \pi \right]
  \]

### 5. 優化目標：最大化回報

在強化學習中，智能體的目標是通過學習最優策略 \(π^*\)，來最大化累積的回報。這可以通過以下方式來描述：

\[
V^*(s) = \max_{\pi} V^\pi(s)
\]
這裡 \(V^*(s)\) 是在狀態 \(s\) 下，根據最優策略 \(π^*\) 可達到的最大期望回報。

### 6. 強化學習中的MDP求解

要解決MDP，強化學習算法通常依賴於以下幾個方法：

1. **價值迭代（Value Iteration）**：價值迭代是一種動態規劃方法，用來迭代計算最優價值函數 \(V^*(s)\)。一旦計算出最優價值函數，就可以推導出最優策略。

2. **策略迭代（Policy Iteration）**：策略迭代方法交替執行兩個步驟：策略評估（根據當前策略計算價值函數）和策略改善（根據價值函數更新策略）。

3. **Q學習（Q-Learning）**：Q學習是一種無模型的強化學習算法，它通過與環境交互來學習最優的行為價值函數 \(Q^*(s, a)\)，並進而推導出最優策略。

### 7. MDP的實現範例：Q學習

以下是使用 Python 實現的簡單 Q-learning 算法範例：

```python
import numpy as np

# 定義環境（例如，迷宮問題）
states = [0, 1, 2, 3, 4, 5]  # 狀態空間
actions = [0, 1]  # 行為空間，0表示左移，1表示右移
rewards = [-1, -1, -1, 10, -1, -1]  # 每個狀態的獎勳
gamma = 0.9  # 折扣因子
alpha = 0.1  # 學習率

# Q表初始化
Q = np.zeros((len(states), len(actions)))

# 環境轉移
def next_state(state, action):
    if action == 0:  # 左移
        return max(0, state - 1)
    elif action == 1:  # 右移
        return min(len(states) - 1, state + 1)

# Q學習
num_episodes = 1000
for episode in range(num_episodes):
    state = np.random.choice(states)  # 隨機選擇初始狀態
    while state != 4:  # 目標狀態為4
        # epsilon-greedy策略選擇行為
        if np.random.rand() < 0.1:
            action = np.random.choice(actions)  # 隨機選擇行為
        else:
            action = np.argmax(Q[state])  # 選擇最優行為
        
        next_s = next_state(state, action)
        reward = rewards[next_s]
        
        # 更新Q表
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_s]) - Q[state, action])
        
        state = next_s

# 顯示學到的Q表
print("Q-table:")
print(Q)
```

### 小結

馬可夫決策過程（MDP）是強化學習中一個非常重要的理論框架，它提供了描述環境狀態、行為選擇及其回報的結構。在MDP的基

礎上，智能體的目標是通過選擇最優策略來最大化長期回報。常見的求解方法包括價值迭代、策略迭代和Q學習等。