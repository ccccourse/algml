### 貝爾曼方程（Bellman Equation）

貝爾曼方程是強化學習中的核心工具，它描述了在一個馬可夫決策過程（MDP）中，狀態價值或行動價值函數如何遞歸地計算。這些方程提供了一種有效的方式來計算最優策略，並在動態規劃方法中扮演著重要角色。

貝爾曼方程分為兩類：**狀態價值方程**和**行動價值方程**。它們分別描述了狀態價值函數和行動價值函數的遞歸性質。

#### 1. 狀態價值方程（Bellman Equation for Value Function）

狀態價值函數 \( V^\pi(s) \) 描述了在狀態 \( s \) 下，遵循策略 \( \pi \) 所能獲得的期望回報。根據貝爾曼方程，狀態價值函數可以表示為：

\[
V^\pi(s) = \mathbb{E}[R_{t+1} + \gamma V^\pi(s_{t+1}) | s_t = s]
\]

這裡：
- \( R_{t+1} \) 是在當前狀態 \( s_t = s \) 下執行行動 \( a_t \) 後，轉移到下一個狀態 \( s_{t+1} \) 並獲得的回報。
- \( \gamma \) 是折扣因子，表示未來回報的權重。
- \( \mathbb{E}[\cdot] \) 表示期望值，即對於所有可能的後續狀態 \( s_{t+1} \) 的期望。

這個方程式的含義是，狀態 \( s \) 下的價值等於當前回報加上未來狀態的價值（經過折扣後）。換句話說，從狀態 \( s \) 開始的價值，等於執行當前行動後的即時回報，加上從新狀態開始的價值的期望。

#### 2. 行動價值方程（Bellman Equation for Q-function）

行動價值函數 \( Q^\pi(s, a) \) 描述了在狀態 \( s \) 下，選擇行動 \( a \) 後，遵循策略 \( \pi \) 所能獲得的期望回報。根據貝爾曼方程，行動價值函數可以表示為：

\[
Q^\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma V^\pi(s_{t+1}) | s_t = s, a_t = a]
\]

這裡：
- \( R_{t+1} \) 是在狀態 \( s_t = s \) 下選擇行動 \( a_t = a \) 後，轉移到下一個狀態 \( s_{t+1} \) 並獲得的回報。
- \( \gamma \) 是折扣因子，表示未來回報的權重。
- \( V^\pi(s_{t+1}) \) 是狀態 \( s_{t+1} \) 下的價值函數。

這個方程式的含義是，行動 \( a \) 的價值等於當前回報加上選擇該行動後的後續狀態的期望價值（經過折扣）。換句話說，從狀態 \( s \) 開始並選擇行動 \( a \)，其價值是當前的回報加上未來狀態價值的期望。

#### 3. 最優貝爾曼方程（Optimal Bellman Equation）

最優策略是能夠使得智能體在每個狀態下獲得最大的期望回報。對於最優策略 \( \pi^* \)，其狀態價值函數 \( V^*(s) \) 和行動價值函數 \( Q^*(s, a) \) 的貝爾曼方程如下：

- **最優狀態價值方程**：
  \[
  V^*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma V^*(s_{t+1}) | s_t = s, a_t = a]
  \]
  最優狀態價值等於在狀態 \( s \) 下選擇所有可能行動的最大期望回報。

- **最優行動價值方程**：
  \[
  Q^*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q^*(s_{t+1}, a') | s_t = s, a_t = a]
  \]
  最優行動價值等於執行某個行動後，當前回報加上未來的最大行動價值。

#### 4. 貝爾曼方程的應用

貝爾曼方程是許多強化學習算法的基礎，尤其是在**動態規劃**中。最常見的應用包括：

- **值迭代（Value Iteration）**：通過反覆更新狀態價值函數，直到其收斂，從而獲得最優策略。
- **策略迭代（Policy Iteration）**：通過交替執行策略評估（使用貝爾曼方程）和策略改進（選擇最佳行動）來獲得最優策略。
- **Q學習（Q-learning）**：這是一種無模型的強化學習算法，通過學習行動價值函數 \( Q(s, a) \)，最終收斂於最優策略。

#### 5. 結論

貝爾曼方程提供了強化學習中價值函數和行動價值函數的遞歸計算方式，並為許多基於動態規劃的方法（如值迭代、策略迭代）奠定了基礎。它的應用在最優決策和策略學習中具有重要意義。