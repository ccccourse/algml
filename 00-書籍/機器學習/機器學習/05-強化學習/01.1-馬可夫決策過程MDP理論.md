### 馬可夫決策過程（MDP）理論

馬可夫決策過程（Markov Decision Process，MDP）是強化學習中的核心數學模型，描述了一個智能體（agent）如何在環境中進行決策。MDP提供了一種數學框架，幫助描述在一個隨時間發展的隨機過程中，如何根據當前的狀態選擇行動並獲得獎勳。

MDP由五個元素組成：狀態空間、行動空間、轉移概率、回報函數和折扣因子。這些元素描述了智能體與環境之間的互動規則。

#### 1. MDP的五個基本元素

1. **狀態空間（State space）**：表示環境中所有可能的狀態。每個狀態都描述了環境在某一時間點的情況。通常，狀態空間用 \( S \) 表示，其中 \( s \in S \) 是一個具體的狀態。

2. **行動空間（Action space）**：表示智能體在每個狀態下可以選擇的所有可能行動的集合。行動空間用 \( A \) 表示，其中 \( a \in A \) 是一個具體的行動。

3. **轉移概率（Transition probability）**：描述了在當前狀態 \( s_t \) 下，選擇某一行動 \( a_t \) 後，系統轉移到下個狀態 \( s_{t+1} \) 的概率。轉移概率通常用 \( P(s_{t+1} | s_t, a_t) \) 表示，表示在狀態 \( s_t \) 執行行動 \( a_t \) 之後，轉移到狀態 \( s_{t+1} \) 的概率。

4. **回報函數（Reward function）**：回報函數 \( R(s_t, a_t, s_{t+1}) \) 定義了在狀態 \( s_t \) 下執行行動 \( a_t \) 並轉移到狀態 \( s_{t+1} \) 之後，智能體所獲得的即時回報。這個回報反映了該行動的好壞，智能體會根據回報來調整策略。

5. **折扣因子（Discount factor）**：折扣因子 \( \gamma \) 是一個介於 0 和 1 之間的數字，用來決定未來回報的重要性。較高的折扣因子意味著智能體更重視未來的回報，而較低的折扣因子則意味著智能體更多關注當前的回報。折扣因子影響智能體的長期策略選擇。

#### 2. MDP的數學模型

MDP的數學形式可以如下表示：

- **狀態空間** \( S = \{s_1, s_2, ..., s_n\} \)：所有可能的環境狀態。
- **行動空間** \( A = \{a_1, a_2, ..., a_m\} \)：智能體可選擇的所有行動。
- **轉移概率** \( P(s'|s, a) \)：在狀態 \( s \) 下選擇行動 \( a \) 之後轉移到狀態 \( s' \) 的概率。
- **回報函數** \( R(s, a, s') \)：在狀態 \( s \) 下選擇行動 \( a \) 並轉移到狀態 \( s' \) 之後獲得的回報。
- **折扣因子** \( \gamma \)：未來回報的重要性，範圍為 \( 0 \leq \gamma < 1 \)。

#### 3. 策略（Policy）

在MDP中，智能體需要決定在每個狀態下選擇哪個行動來最大化長期回報。這是通過策略（Policy）來實現的。策略 \( \pi \) 是一個從狀態到行動的映射，它告訴智能體在每個狀態下應該選擇哪個行動。策略可以是確定性的（在每個狀態下只有一個行動）或隨機的（在每個狀態下有一組可能的行動，每個行動都有一個選擇概率）。

- **確定性策略** \( \pi(s) = a \)：在狀態 \( s \) 下選擇行動 \( a \)。
- **隨機策略** \( \pi(a|s) \)：在狀態 \( s \) 下，選擇行動 \( a \) 的概率是 \( \pi(a|s) \)。

#### 4. 回報與價值函數

在強化學習中，智能體的目標是最大化回報的期望。為此，我們引入了兩個重要的概念：回報和價值函數。

- **總回報（Return）**：智能體在當前狀態下採取策略 \( \pi \) 所能獲得的回報總和。總回報通常表示為從某一時間步開始的回報之和。對於在狀態 \( s_t \) 開始的總回報 \( G_t \)，可以表示為：
  \[
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
  \]
  
- **價值函數（Value function）**：在狀態 \( s \) 下，智能體選擇行動後能夠獲得的期望總回報。價值函數分為兩種：
  - **狀態價值函數** \( V^\pi(s) \)：在狀態 \( s \) 下，遵循策略 \( \pi \) 所能獲得的期望回報。
    \[
    V^\pi(s) = \mathbb{E}[G_t | s_t = s]
    \]
  - **行動價值函數** \( Q^\pi(s, a) \)：在狀態 \( s \) 下選擇行動 \( a \) 並按照策略 \( \pi \) 行事後所能獲得的期望回報。
    \[
    Q^\pi(s, a) = \mathbb{E}[G_t | s_t = s, a_t = a]
    \]

#### 5. 最優策略與最優價值函數

MDP的目標是找到一個最優策略，該策略可以使得智能體獲得最大的期望回報。最優策略 \( \pi^* \) 是一個使得狀態價值函數 \( V^{\pi^*}(s) \) 或行動價值函數 \( Q^{\pi^*}(s, a) \) 最大化的策略。

- **最優狀態價值函數**：\( V^*(s) = \max_{\pi} V^\pi(s) \)
- **最優行動價值函數**：\( Q^*(s, a) = \max_{\pi} Q^\pi(s, a) \)

最優策略可以通過以下方法來求解：
- **動態規劃**（如值迭代或策略迭代）
- **蒙特卡羅方法**
- **時序差分學習**（Temporal Difference Learning）

#### 6. MDP的應用

馬可夫決策過程被廣泛應用於許多強化學習任務中，包括但不限於：

- **機器人控制**：在機器人控制中，MDP用來描述機器人的狀態（位置、速度等）、行動（移動、旋轉等）以及回報（目標達成、避障等）。

- **遊戲 AI**：在博弈中，MDP用來描述遊戲的狀態（棋盤、牌面等）、玩家的行動（下棋、出牌等）以及根據行動獲得的回報。

- **自動駕駛**：自動駕駛系統可以被建模為MDP，其中車輛的狀態包括速度、位置等，行動包括加速、刹車等，而回報則是車輛是否安全行駛。

#### 7. 結論

馬可夫決策過程為強化學習提供了一個強大的數學框架，幫助我們理解智能體如何在隨機環境中進行決策並最大化回報。通過設計適當的策略和利用價值函數，MDP幫助我們分析和解決各種實際問題。