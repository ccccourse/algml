### 梯度下降法 (Gradient Descent)

梯度下降法是一種基本的一階優化方法，廣泛應用於無約束優化問題中，特別是在機器學習和深度學習中的參數調整。該方法基於目標函數的梯度信息，沿著梯度的反方向進行逐步迭代，以逼近局部最小值。

#### 1. **基本原理**

給定一個可微的目標函數 \( f(x) \)，梯度下降法的核心思想是沿著目標函數的梯度反方向進行迭代，因為梯度反方向是函數下降最快的方向。

每次迭代的更新規則為：
\[
x_{k+1} = x_k - \alpha \nabla f(x_k),
\]
其中：
- \( x_k \) 是第 \( k \) 次迭代的參數向量，
- \( \alpha \) 是學習率或步長，控制每次迭代的步幅大小，
- \( \nabla f(x_k) \) 是目標函數在 \( x_k \) 處的梯度。

#### 2. **收斂性分析**

- **選擇適當的步長 \( \alpha \)**：
  - 如果步長太小，算法會收斂得非常慢；
  - 如果步長太大，可能會導致算法不收斂或在最小值附近震盪。
  
- **收斂條件**：
  - 若 \( f(x) \) 是強凸函數，則梯度下降法具有全局收斂性，且收斂速度可以達到線性。
  - 對於一般凸函數，梯度下降法仍然能夠收斂，但速度較慢。
  
- **減少目標函數值的性質**：
  每次迭代都能夠保證 \( f(x_{k+1}) \leq f(x_k) \) ，只要步長選擇得當。

#### 3. **學習率選擇策略**

- **固定步長**：最常見的設置，使用固定的學習率 \( \alpha \)。然而，選擇適當的固定步長是一個挑戰。
  
- **自適應步長**：動態調整步長，如學習率逐漸減小或使用線搜索技術來選擇最佳步長。

- **逐步減小步長**：採用 \( \alpha_k = \frac{\alpha_0}{k} \) 的策略，隨著迭代次數的增長，步長逐漸減小，可以保證收斂性。

#### 4. **應用與局限性**

- **應用**：梯度下降法應用於各種無約束優化問題，如線性迴歸、邏輯迴歸等機器學習模型的參數估計。
  
- **局限性**：
  - 容易陷入局部極小值或平坦區域，特別是在非凸函數中。
  - 對於高維空間的目標函數，收斂速度可能較慢，特別是在陡峭或扁平的區域。

梯度下降法是優化算法中的基礎和起點，許多改進算法（如隨機梯度下降、動量法、自適應學習率方法）都是基於梯度下降法的變體或擴展。