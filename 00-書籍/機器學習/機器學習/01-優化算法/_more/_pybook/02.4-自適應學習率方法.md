### 自適應學習率方法（Adaptive Learning Rate Methods）

**概念解釋：**

自適應學習率方法旨在根據每個參數的梯度信息動態地調整學習率。與傳統的固定學習率方法不同，自適應學習率方法可以針對不同的參數設定不同的學習率，並根據每次梯度更新的特性進行調整。這些方法能夠在參數空間中有效地找到合適的步長，從而加速收斂並減少手動選擇學習率的繁瑣。

常見的自適應學習率方法包括：
- **Adagrad**：根據每個參數的歷史梯度自適應調整學習率。
- **RMSprop**：通過使用指數衰減平均來解決Adagrad的問題。
- **Adam**：結合了動量法和RMSprop的思想，進一步提高了性能。

### 1. **Adagrad**

Adagrad（Adaptive Gradient Algorithm）是一種通過對每個參數單獨調整學習率的方式來優化訓練過程的方法。Adagrad會對每個參數的歷史梯度平方進行累積，使得每個參數的學習率根據其歷史梯度的頻繁程度來進行調整。對於頻繁更新的參數，學習率會逐步減小，而對於較少更新的參數，學習率會較大。

**數學表示：**

\[
g_{t,i} = \nabla f_i(x_t) \quad \text{(梯度)}
\]
\[
G_{t,i} = \sum_{k=1}^{t} g_{k,i}^2 \quad \text{(歷史梯度平方和)}
\]
\[
\eta_i = \frac{\eta}{\sqrt{G_{t,i} + \epsilon}} \quad \text{(自適應學習率)}
\]
\[
x_{t+1,i} = x_{t,i} - \eta_i \cdot g_{t,i} \quad \text{(參數更新)}
\]

其中：
- \( g_{t,i} \) 是在第 \( t \) 步的梯度。
- \( G_{t,i} \) 是第 \( t \) 步之前所有梯度的平方和。
- \( \epsilon \) 是為避免除零錯誤的小常數（通常取 \( 1e-8 \)）。

### 2. **RMSprop**

RMSprop（Root Mean Square Propagation）是對Adagrad的改進。Adagrad可能會在迭代過程中過快地減小學習率，導致算法過早停止學習。為了解決這個問題，RMSprop引入了指數衰減平均來控制歷史梯度的影響，使得較新的梯度信息更加重要。

**數學表示：**

\[
g_{t,i} = \nabla f_i(x_t)
\]
\[
E[g^2]_{t,i} = \beta \cdot E[g^2]_{t-1,i} + (1 - \beta) \cdot g_{t,i}^2
\]
\[
\eta_i = \frac{\eta}{\sqrt{E[g^2]_{t,i} + \epsilon}}
\]
\[
x_{t+1,i} = x_{t,i} - \eta_i \cdot g_{t,i}
\]

其中：
- \( \beta \) 是指數衰減因子，通常選擇 \( \beta = 0.9 \)。
- \( E[g^2]_{t,i} \) 是梯度平方的加權平均。

### 3. **Adam**

Adam（Adaptive Moment Estimation）是一個結合了動量法和RMSprop的自適應學習率方法。Adam不僅考慮了梯度的平方和（類似於RMSprop），還考慮了梯度的平均值，這樣可以在訓練過程中更好地捕捉到梯度的趨勢。

Adam的主要優勢在於其能夠高效地處理稀疏梯度（如在自然語言處理中經常出現的情況）。

**數學表示：**

\[
m_{t,i} = \beta_1 \cdot m_{t-1,i} + (1 - \beta_1) \cdot g_{t,i} \quad \text{(梯度的移動平均)}
\]
\[
v_{t,i} = \beta_2 \cdot v_{t-1,i} + (1 - \beta_2) \cdot g_{t,i}^2 \quad \text{(梯度平方的移動平均)}
\]
\[
\hat{m}_{t,i} = \frac{m_{t,i}}{1 - \beta_1^t} \quad \text{(偏差修正後的動量)}
\]
\[
\hat{v}_{t,i} = \frac{v_{t,i}}{1 - \beta_2^t} \quad \text{(偏差修正後的梯度平方)}
\]
\[
x_{t+1,i} = x_{t,i} - \frac{\eta}{\sqrt{\hat{v}_{t,i}} + \epsilon} \cdot \hat{m}_{t,i}
\]

其中：
- \( m_{t,i} \) 是梯度的移動平均。
- \( v_{t,i} \) 是梯度平方的移動平均。
- \( \beta_1 \) 和 \( \beta_2 \) 分別是動量和梯度平方的衰減率，通常取 \( \beta_1 = 0.9 \) 和 \( \beta_2 = 0.999 \)。
- \( \epsilon \) 是防止除以零的小常數（通常取 \( 1e-8 \)）。

### Python 範例：自適應學習率（Adam）

以下是使用Adam算法訓練簡單的函數 \( f(x) = x^2 \) 的範例：

```python
import numpy as np
import matplotlib.pyplot as plt

# 目標函數 f(x) = x^2
def objective_function(x):
    return x ** 2

# 目標函數的梯度 f'(x) = 2x
def gradient(x):
    return 2 * x

# Adam優化算法
def adam(learning_rate, beta1, beta2, max_iters, initial_x):
    x = initial_x
    m = 0  # 梯度的移動平均
    v = 0  # 梯度平方的移動平均
    epsilon = 1e-8  # 防止除以零的小常數
    history = []
    for t in range(1, max_iters + 1):
        grad = gradient(x)
        m = beta1 * m + (1 - beta1) * grad  # 更新動量
        v = beta2 * v + (1 - beta2) * grad**2  # 更新梯度平方
        m_hat = m / (1 - beta1**t)  # 偏差修正
        v_hat = v / (1 - beta2**t)  # 偏差修正
        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)  # 更新參數
        history.append(x)
    return x, history

# 設定超參數
learning_rate = 0.1
beta1 = 0.9
beta2 = 0.999
max_iters = 50
initial_x = 10

# 執行Adam算法
final_x, history = adam(learning_rate, beta1, beta2, max_iters, initial_x)

# 顯示結果
print(f"最優解: {final_x}")
print(f"最小函數值: {objective_function(final_x)}")

# 可視化Adam算法過程
x_values = np.linspace(-10, 10, 100)
y_values = objective_function(x_values)

plt.plot(x_values, y_values, label='Objective function f(x) = x^2')
plt.scatter(history, [objective_function(x) for x in history], color='red', label='Adam Iterations')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Adam Optimization on f(x) = x^2')
plt.legend()
plt.show()
```

### 程式解析：
1. **objective_function(x)**：定義了目標函數 \( f(x) = x^2 \)。
2. **gradient(x)**：定義了目標函數的梯度 \( f'(x) = 2x \)。
3. **adam()**：實現了Adam算法，並根據每次梯度更新進行參數更新。
4. **可視化**：繪製了目標函數及Adam算法過程中的參數更新步驟。

### 小結：
自適應學習率方法，如Adagrad、R

MSprop和Adam，可以根據梯度信息動態調整學習率，從而在訓練過程中提高收斂速度並減少震盪。這些方法在許多現實問題中都表現出色，特別是在處理稀疏梯度或複雜優化問題時。