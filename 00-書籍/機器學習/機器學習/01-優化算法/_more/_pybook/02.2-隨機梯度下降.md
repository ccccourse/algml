### 隨機梯度下降（Stochastic Gradient Descent, SGD）

**概念解釋：**

隨機梯度下降（SGD）是一種梯度下降的變體，在這種方法中，每次參數更新都基於一個隨機選擇的訓練樣本（而不是所有樣本的平均梯度）。這樣可以加速學習過程，尤其是對於大規模數據集，因為每次更新不需要計算整個數據集的梯度，從而減少了計算開銷。

**數學表示：**

對於一個目標函數 \( f(x) \)，給定一個訓練樣本 \( x_i \)，隨機梯度下降法的更新規則為：

\[
x_{t+1} = x_t - \eta \nabla f(x_t; x_i)
\]

其中：
- \( x_t \) 是當前點，
- \( \eta \) 是學習率（步長），
- \( \nabla f(x_t; x_i) \) 是基於樣本 \( x_i \) 計算的梯度。

**SGD的優勢和挑戰：**

優勢：
1. 計算效率高：每次更新只需要一個樣本，因此每次更新非常快速。
2. 可以跳出局部最小值：由於每次更新都有隨機性，這樣可以幫助優化算法跳出局部最小值（在非凸問題中尤其有效）。

挑戰：
1. 更新方向會有波動：由於每次只使用一個樣本，更新方向會相對不穩定，可能會在最優解附近震盪。
2. 需要設置適當的學習率：過大的學習率可能導致不穩定，過小的學習率可能導致收斂緩慢。

### Python 範例：隨機梯度下降法

以下是使用隨機梯度下降法來最小化二次函數 \( f(x) = x^2 \) 的範例。這個函數的最小值為 \( x = 0 \)。

#### 例子設定：
- 目標函數：\( f(x) = x^2 \)
- 目標：最小化 \( f(x) \)

#### Python 實現：

```python
import numpy as np
import matplotlib.pyplot as plt

# 定義目標函數 f(x) = x^2
def objective_function(x):
    return x ** 2

# 定義目標函數的梯度（導數）f'(x) = 2x
def gradient(x):
    return 2 * x

# 隨機梯度下降法
def stochastic_gradient_descent(learning_rate, max_iters, initial_x, batch_size=1):
    x = initial_x
    history = []  # 記錄每次迭代的結果
    for _ in range(max_iters):
        # 隨機選擇一個樣本，這裡使用的只是單個樣本
        grad = gradient(x)  # 計算當前點的梯度
        x = x - learning_rate * grad  # 更新參數
        history.append(x)
    return x, history

# 設定超參數
learning_rate = 0.1  # 學習率
max_iters = 50  # 最大迭代次數
initial_x = 10  # 初始猜測值

# 執行隨機梯度下降法
final_x, history = stochastic_gradient_descent(learning_rate, max_iters, initial_x)

# 顯示結果
print(f"最優解: {final_x}")
print(f"最小函數值: {objective_function(final_x)}")

# 可視化隨機梯度下降過程
x_values = np.linspace(-10, 10, 100)
y_values = objective_function(x_values)

plt.plot(x_values, y_values, label='Objective function f(x) = x^2')
plt.scatter(history, [objective_function(x) for x in history], color='red', label='SGD Iterations')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Stochastic Gradient Descent on f(x) = x^2')
plt.legend()
plt.show()
```

#### 程式解析：
1. **objective_function(x)**：定義了目標函數 \( f(x) = x^2 \)。
2. **gradient(x)**：定義了目標函數的梯度（即 \( f'(x) = 2x \)）。
3. **stochastic_gradient_descent()**：實現了隨機梯度下降法，這裡每次迭代隨機選擇一個樣本（對於簡單例子，實際上這裡使用的是當前點的梯度，但在真實情況中可以使用隨機選擇的樣本）。
4. **可視化**：使用 `matplotlib` 來繪製目標函數的曲線，並顯示隨機梯度下降過程中參數更新的步驟。

#### 結果：
- 程式會顯示最優解，並且展示了隨機梯度下降過程中參數更新的步驟。

### 收斂問題：
由於每次只使用單個樣本，隨機梯度下降的更新方向會有波動，這意味著它的收斂過程可能會比批量梯度下降更不穩定。但這種隨機性有助於在非凸問題中逃脫局部最小值。為了減少收斂波動，通常會使用學習率衰減等技術。

### 小結：
隨機梯度下降法（SGD）是處理大規模數據集時的一種有效方法。它具有計算效率高、能夠跳出局部最小值等優點，但也需要小心學習率的選擇，並且通常會在最優解附近震盪。