### 梯度下降法（Gradient Descent）

**概念解釋：**

梯度下降法是一種常用的一階優化方法，主要用來尋找一個函數的局部最小值（或者最大值，通過反向梯度）。在多數的機器學習和深度學習問題中，梯度下降法通常用來優化損失函數。這種方法基於這樣的想法：如果我們知道當前點的梯度（函數的導數），我們就可以朝著梯度的反方向更新參數，這樣可以逐步減少函數值，進而達到最小值。

**數學表示：**

對於一個目標函數 \( f(x) \)，梯度下降法的更新規則為：

\[
x_{t+1} = x_t - \eta \nabla f(x_t)
\]

其中：
- \( x_t \) 是當前點，
- \( \eta \) 是學習率（步長），
- \( \nabla f(x_t) \) 是在當前點 \( x_t \) 處的梯度。

**梯度下降法的步驟：**
1. 初始化參數 \( x_0 \)（起始點）。
2. 計算當前參數 \( x_t \) 處的梯度 \( \nabla f(x_t) \)。
3. 使用梯度下降公式更新參數 \( x_{t+1} = x_t - \eta \nabla f(x_t) \)。
4. 重複步驟 2 和 3，直到收斂（即梯度變得非常小）。

**梯度下降的類型**：
1. **批量梯度下降（Batch Gradient Descent）**：使用全部訓練數據來計算梯度。每一步更新參數的方向和大小是基於整個訓練集的平均梯度。
2. **隨機梯度下降（Stochastic Gradient Descent, SGD）**：每次更新參數時，只使用一個樣本來計算梯度，更新速度較快，但可能會有較大的波動。
3. **小批量梯度下降（Mini-batch Gradient Descent）**：每次使用一小部分數據來計算梯度，兼具批量梯度下降和隨機梯度下降的優點。

### Python 範例：梯度下降法

以下是使用梯度下降法來最小化一個簡單的二次函數 \( f(x) = x^2 \) 的範例。這個函數的最小值明顯是 \( x = 0 \)。

#### 例子設定：
- 目標函數：\( f(x) = x^2 \)
- 目標：最小化 \( f(x) \)

#### Python 實現：

```python
import numpy as np
import matplotlib.pyplot as plt

# 定義目標函數 f(x) = x^2
def objective_function(x):
    return x ** 2

# 定義目標函數的梯度（導數）f'(x) = 2x
def gradient(x):
    return 2 * x

# 梯度下降法
def gradient_descent(learning_rate, max_iters, initial_x):
    x = initial_x
    history = []  # 記錄每次迭代的結果
    for _ in range(max_iters):
        grad = gradient(x)
        x = x - learning_rate * grad  # 更新參數
        history.append(x)
    return x, history

# 設定超參數
learning_rate = 0.1  # 學習率
max_iters = 50  # 最大迭代次數
initial_x = 10  # 初始猜測值

# 執行梯度下降法
final_x, history = gradient_descent(learning_rate, max_iters, initial_x)

# 顯示結果
print(f"最優解: {final_x}")
print(f"最小函數值: {objective_function(final_x)}")

# 可視化梯度下降過程
x_values = np.linspace(-10, 10, 100)
y_values = objective_function(x_values)

plt.plot(x_values, y_values, label='Objective function f(x) = x^2')
plt.scatter(history, [objective_function(x) for x in history], color='red', label='Gradient Descent Iterations')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent on f(x) = x^2')
plt.legend()
plt.show()
```

#### 程式解析：
1. **objective_function(x)**：定義了目標函數 \( f(x) = x^2 \)。
2. **gradient(x)**：定義了目標函數的梯度（即 \( f'(x) = 2x \)）。
3. **gradient_descent()**：實現了梯度下降法，根據梯度更新參數 \( x \)。
4. **可視化**：使用 `matplotlib` 來繪製目標函數的曲線和梯度下降過程中參數更新的軌跡。

#### 結果：
- 程式會顯示出最優解，並且展示了梯度下降過程中參數更新的步驟。

### 梯度下降的收斂條件：
- 梯度下降法會在函數的梯度接近零時停止收斂。
- 收斂速度受到學習率的影響，過大的學習率可能導致不穩定，過小的學習率可能使收斂過慢。

### 小結：
梯度下降法是一個非常基礎且有效的優化方法，適用於各種機器學習和深度學習的問題。了解其原理和實現，對於學習更高級的優化算法非常有幫助。