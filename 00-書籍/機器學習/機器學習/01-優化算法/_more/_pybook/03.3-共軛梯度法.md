### 共軛梯度法（Conjugate Gradient Method）

**概念解釋：**

共軛梯度法是一種用於求解大規模無約束優化問題的迭代方法，特別適用於二次型目標函數的情況。它的主要特點是通過選擇“共軛方向”來避免在每一步中計算Hessian矩陣，從而比普通的梯度下降法更快地收斂。共軛梯度法的核心思想是對每個迭代步進行一個新的方向搜索，這個方向與之前的搜索方向是**共軛的**。

**數學原理：**

假設我們要最小化一個二次型目標函數：

\[
f(x) = \frac{1}{2} x^T A x - b^T x
\]

其中，\( A \) 是對稱正定矩陣，\( b \) 是常數向量。對於這樣的函數，梯度是：

\[
\nabla f(x) = A x - b
\]

共軛梯度法的核心步驟如下：

1. **初始化**：
   - 設定初始點 \( x_0 \) 和梯度 \( r_0 = \nabla f(x_0) \)。
   - 設定初始搜索方向為 \( p_0 = r_0 \)。

2. **迭代**：
   - 在每一步中計算沿當前方向的步長 \( \alpha_k \)，這樣使得在該方向上達到最小值：
   
   \[
   \alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}
   \]
   
   - 更新解向量：
   
   \[
   x_{k+1} = x_k + \alpha_k p_k
   \]
   
   - 計算新的殘差：
   
   \[
   r_{k+1} = r_k - \alpha_k A p_k
   \]
   
   - 計算新的共軛方向：
   
   \[
   \beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}
   \]
   
   \[
   p_{k+1} = r_{k+1} + \beta_k p_k
   \]
   
   這裡的 \( \beta_k \) 用來調整方向，使得新方向與之前的方向共軛。

3. **停止條件**：當殘差 \( r_k \) 足夠小時停止迭代，通常是當 \( \|r_k\| \) 小於某個閾值。

**共軛梯度法的特點：**
- **高效性**：由於不需要計算Hessian矩陣，共軛梯度法的每次迭代都比牛頓法等二階方法更加高效。
- **適用於大規模問題**：共軛梯度法尤其適合於維度非常高的情況（如在有限元素法中求解大型線性方程組）。
- **收斂速度**：在最優條件下，共軛梯度法可以在 \( n \) 步內收斂，其中 \( n \) 是變數的數量。

### Python範例：共軛梯度法

以下是使用共軛梯度法求解二次型目標函數 \( f(x) = \frac{1}{2} x^T A x - b^T x \) 的Python範例，並進行優化。

```python
import numpy as np
import matplotlib.pyplot as plt

# 目標函數 f(x) = (1/2) * x^T A x - b^T x
# 梯度 f'(x) = A x - b

# 定義A和b
A = np.array([[4, 1], [1, 3]])  # 這是對稱正定矩陣
b = np.array([1, 2])

# 目標函數
def objective_function(x):
    return 0.5 * np.dot(x.T, np.dot(A, x)) - np.dot(b.T, x)

# 梯度
def gradient(x):
    return np.dot(A, x) - b

# 共軛梯度法
def conjugate_gradient(A, b, x0, tol=1e-6, max_iter=100):
    x = x0
    r = gradient(x)
    p = -r
    rsold = np.dot(r.T, r)
    
    for i in range(max_iter):
        Ap = np.dot(A, p)
        alpha = rsold / np.dot(p.T, Ap)
        x = x + alpha * p
        r = r + alpha * Ap
        rsnew = np.dot(r.T, r)
        
        if np.sqrt(rsnew) < tol:
            break
        
        p = -r + (rsnew / rsold) * p
        rsold = rsnew
    
    return x

# 初始猜測
x0 = np.array([0, 0])

# 執行共軛梯度法
x_opt = conjugate_gradient(A, b, x0)

# 顯示結果
print(f"最優解：{x_opt}")
print(f"最小函數值：{objective_function(x_opt)}")

# 可視化優化過程
x_vals = np.linspace(-2, 2, 100)
y_vals = np.linspace(-2, 2, 100)
X, Y = np.meshgrid(x_vals, y_vals)
Z = 0.5 * (A[0, 0] * X**2 + 2 * A[0, 1] * X * Y + A[1, 1] * Y**2) - b[0] * X - b[1] * Y

plt.contour(X, Y, Z, levels=50)
plt.plot(x_opt[0], x_opt[1], 'ro', label='Optimized Solution')
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Conjugate Gradient Optimization')
plt.show()
```

### 程式解析：
1. **目標函數**：`objective_function` 定義了二次型函數 \( f(x) = \frac{1}{2} x^T A x - b^T x \)。
2. **梯度**：`gradient` 函數返回目標函數的梯度 \( f'(x) = A x - b \)。
3. **共軛梯度法**：`conjugate_gradient` 函數實現了共軛梯度法的迭代過程。每一步更新新的解向量 \( x \)，並根據前一迭代的殘差計算新的共軛方向。
4. **可視化**：繪製了目標函數的等高線，並標註了最優解。

### 小結：
共軛梯度法是一種高效的無約束優化算法，特別適用於求解大型線性系統和二次型目標函數。它通過選擇共軛方向進行搜尋，避免了直接計算Hessian矩陣的需要，適合於大規模的優化問題。