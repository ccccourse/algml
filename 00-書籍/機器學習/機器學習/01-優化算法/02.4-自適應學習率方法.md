### 自適應學習率方法 (Adaptive Learning Rate Methods)

自適應學習率方法旨在根據每個參數的更新歷史來自動調整學習率，以提高優化效率並減少手動調整學習率的需求。這些方法的核心思想是：對於不同的參數，根據其歷史梯度的大小來調整學習率，使得梯度較大或變化較劇烈的參數能夠有較小的學習率，而梯度較小或變化較平穩的參數則可以使用較大的學習率。

自適應學習率方法通常比傳統的固定學習率梯度下降法具有更高的收斂速度，並且在處理複雜的非凸問題時更為有效。

#### 1. **Adagrad (Adaptive Gradient Algorithm)**

Adagrad 是最早提出的自適應學習率方法之一，它通過累積梯度的平方來調整每個參數的學習率。

- **更新規則**：
  對每個參數 \( x_k \) 的學習率進行調整：
  \[
  g_{k+1} = g_k + \nabla f(x_k)^2,
  \]
  \[
  x_{k+1} = x_k - \frac{\alpha}{\sqrt{g_{k+1}}} \nabla f(x_k),
  \]
  其中：
  - \( g_k \) 是參數 \( x_k \) 之前所有梯度的平方累積，
  - \( \alpha \) 是全局學習率。

- **優點**：
  - Adagrad 自動根據每個參數的歷史梯度調整學習率，對於稀疏的梯度（例如在自然語言處理中的詞向量更新）效果較好。
  - 不需要手動調整學習率。

- **缺點**：
  - 由於累積的梯度平方會隨著訓練過程不斷增大，這會導致學習率在訓練過程中逐漸減小，最終可能導致學習速率過低，從而使得訓練過程停滯。

#### 2. **RMSprop (Root Mean Square Propagation)**

RMSprop 是一種解決 Adagrad 退化問題的改進算法。它引入了指數衰減平均，從而使累積梯度平方的過快增長得以控制。

- **更新規則**：
  \[
  v_{k+1} = \beta v_k + (1 - \beta) \nabla f(x_k)^2,
  \]
  \[
  x_{k+1} = x_k - \frac{\alpha}{\sqrt{v_{k+1} + \epsilon}} \nabla f(x_k),
  \]
  其中：
  - \( v_k \) 是過去梯度平方的加權平均，
  - \( \beta \) 是衰減因子，通常設置為 0.9，
  - \( \epsilon \) 是為了避免除零錯誤的常數。

- **優點**：
  - 解決了 Adagrad 隨著訓練進行學習率過小的問題，能夠保持穩定的學習率。
  - 更適用於非平穩的問題，如深度學習中非常高維的參數空間。

- **缺點**：
  - 雖然減少了學習率逐步衰減的問題，但仍需要適當調整學習率 \( \alpha \)。

#### 3. **Adam (Adaptive Moment Estimation)**

Adam 是一種結合了 Adagrad 和 RMSprop 特點的算法，通過使用動量（即一階矩估計）和自適應學習率（即二階矩估計）來改進參數更新。

- **更新規則**：
  \[
  m_{k+1} = \beta_1 m_k + (1 - \beta_1) \nabla f(x_k),
  \]
  \[
  v_{k+1} = \beta_2 v_k + (1 - \beta_2) \nabla f(x_k)^2,
  \]
  \[
  \hat{m}_{k+1} = \frac{m_{k+1}}{1 - \beta_1^{k+1}}, \quad \hat{v}_{k+1} = \frac{v_{k+1}}{1 - \beta_2^{k+1}},
  \]
  \[
  x_{k+1} = x_k - \frac{\alpha}{\sqrt{\hat{v}_{k+1}} + \epsilon} \hat{m}_{k+1},
  \]
  其中：
  - \( m_k \) 和 \( v_k \) 分別是梯度的一階矩和二階矩估計，
  - \( \beta_1 \) 和 \( \beta_2 \) 是動量參數，通常設置為 0.9 和 0.999，
  - \( \hat{m}_{k+1} \) 和 \( \hat{v}_{k+1} \) 是修正過的動量和二階矩估計。

- **優點**：
  - 結合了動量法和自適應學習率方法，能夠有效加速收斂並處理稀疏或高噪聲的梯度。
  - 可以自動調整學習率，減少手動調參的需求。

- **缺點**：
  - 雖然 Adam 通常表現良好，但在某些情況下（如訓練過程過長或對超參數非常敏感的問題），可能需要仔細調整 \( \beta_1 \) 和 \( \beta_2 \) 來避免過度震盪。

#### 4. **Adadelta**

Adadelta 是對 Adagrad 的一種改進方法，它解決了 Adagrad 學習率衰減過快的問題。Adadelta 的核心思想是使用一個動態的學習率調整方式，而不再需要全局學習率 \( \alpha \)。

- **更新規則**：
  \[
  E[g^2]_{k+1} = \beta E[g^2]_k + (1 - \beta) \nabla f(x_k)^2,
  \]
  \[
  \Delta x_{k+1} = -\frac{\sqrt{E[\Delta x^2]_k + \epsilon}}{\sqrt{E[g^2]_{k+1} + \epsilon}} \nabla f(x_k),
  \]
  \[
  x_{k+1} = x_k + \Delta x_{k+1},
  \]
  其中：
  - \( E[g^2]_k \) 是梯度平方的滑動平均，
  - \( E[\Delta x^2]_k \) 是參數更新的平方平均。

- **優點**：
  - 不需要事先設置學習率 \( \alpha \)，自動調整學習率。
  - 可以克服 Adagrad 學習率過早衰減的問題。

- **缺點**：
  - 較為複雜，可能需要更多的計算資源，並且在某些情況下可能不如 Adam 或 RMSprop 表現穩定。

#### 5. **比較與選擇**

- **Adagrad**：適用於稀疏數據或特徵，但學習率過快衰減可能不利於長期訓練。
- **RMSprop**：適合處理非平穩的問題，並保持穩定的學習率。
- **Adam**：最常用的自適應學習率算法，結合了動量和自適應學習率的優點，適用於大多數問題。
- **Adadelta**：解決了 Adagrad 的學習率衰減問題，適合長期訓練，尤其在無法選擇全局學習率時。

總結來說，自適應學習率方法能夠根據不同的梯度信息自動調整學習率，從而加速收斂並提高優化效率，尤其在複雜的深度學習模型中具有優異的性能。