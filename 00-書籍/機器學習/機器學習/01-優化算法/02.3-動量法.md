### 動量法 (Momentum)

動量法是一種對梯度下降法的改進，旨在加速收斂並減少震盪。其基本思想借鑒了物理學中動量的概念，即對當前梯度的更新添加上一些歷史梯度的「慣性」，從而在更新時考慮過去的梯度信息，讓參數的更新過程更為平滑。

#### 1. **基本原理**

在標準的梯度下降法中，參數的更新僅依賴於當前的梯度：
\[
x_{k+1} = x_k - \alpha \nabla f(x_k),
\]
其中 \( x_k \) 是第 \( k \) 次迭代的參數，\( \alpha \) 是學習率，\( \nabla f(x_k) \) 是當前步驟的梯度。

而在動量法中，對每次迭代的更新引入一個「動量」項，這樣參數的更新就不僅僅依賴於當前的梯度，還考慮了前幾次的梯度方向。具體而言，動量法使用一個累積的梯度來調整參數更新：

\[
v_{k+1} = \beta v_k + (1 - \beta) \nabla f(x_k),
\]
\[
x_{k+1} = x_k - \alpha v_{k+1},
\]
其中：
- \( v_k \) 是第 \( k \) 次迭代的「動量」，是過去梯度的加權平均，
- \( \beta \) 是動量參數（通常在 0 到 1 之間），控制歷史梯度的影響程度，
- \( \nabla f(x_k) \) 是當前步驟的梯度。

#### 2. **收斂性分析**

- **加速收斂**：
  動量法能夠有效加速收斂，尤其在梯度下降法面臨平坦區域或極小值附近時。動量的作用是幫助梯度下降跳過這些平坦區域，從而提高效率。

- **減少震盪**：
  在高曲率方向（梯度變化較大的地方），梯度下降法的更新會受到較大波動；而動量法則通過引入過去的梯度來減少這些波動，從而在收斂過程中減少震盪，特別是在窄長的最小值區域。

- **穩定性**：
  雖然動量法能減少震盪，但仍然可能因為過大的動量參數 \( \beta \) 而導致過度「慣性」，使得更新過度跳躍，從而偏離最小值。因此，選擇合適的 \( \beta \) 是很重要的。

#### 3. **學習率與動量的選擇**

- **學習率 \( \alpha \)**：動量法的學習率選擇和標準梯度下降相似，過大的學習率可能會使算法無法收斂，而過小的學習率會導致收斂速度過慢。
  
- **動量參數 \( \beta \)**：動量參數 \( \beta \) 通常設置在 0.9 左右（也有些變種會設置為更小的值，如 0.99），這表示新梯度的更新相對於舊動量的影響程度。一般來說，較大的 \( \beta \) 能夠強化慣性效應，適用於長時間的優化過程。

#### 4. **動量法的優缺點**

- **優點**：
  - **加速收斂**：在窄長的極小值區域，動量法能夠有效減少梯度下降的震盪，從而加速收斂過程。
  - **減少震盪**：在高曲率區域，動量法能夠使參數更新變得更加穩定，避免在平坦區域過度波動。

- **缺點**：
  - **需要選擇合適的動量參數 \( \beta \)**：選擇不當的動量參數可能會導致算法不穩定，甚至無法收斂。
  - **無法處理非平穩梯度**：對於梯度變化劇烈的問題，動量法有時會過度「慣性」，造成參數更新過大，導致算法不穩定。

#### 5. **動量法的變種**

- **Nesterov加速梯度 (NAG)**：
  Nesterov加速梯度法是動量法的一個改進，它提前對參數進行預測，然後基於這個預測值來計算梯度更新。這樣可以在某些情況下進一步提高收斂速度。

  具體更新規則為：
  \[
  v_{k+1} = \beta v_k + (1 - \beta) \nabla f(x_k - \alpha \beta v_k),
  \]
  \[
  x_{k+1} = x_k - \alpha v_{k+1}.
  \]
  這種方法利用了預測梯度的「未來」信息，對於某些問題可以進一步加速收斂。

#### 6. **應用**

- **機器學習和深度學習**：動量法在神經網絡的訓練中廣泛應用，尤其是在面對大規模數據和複雜模型時。其在提高訓練速度和穩定性方面發揮了重要作用。
- **大規模優化問題**：在很多需要優化的問題中（如圖像識別、自然語言處理等），動量法經常作為改進梯度下降的常見方法之一。

---

動量法通過引入過去梯度的累積信息，不僅減少了梯度下降中的震盪，還能加速在平坦區域的收斂，是優化過程中常用的技術。