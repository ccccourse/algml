### 支持向量機（Support Vector Machine, SVM）

支持向量機（SVM）是一種強大的機器學習算法，主要用於分類和回歸分析。SVM的核心思想是將數據映射到高維空間，在這個高維空間中，尋找一個最優的超平面來進行分類。這個超平面能夠最大化分類邊界，從而提高模型的泛化能力。

SVM主要有兩個版本：
1. **分類支持向量機**（SVC）
2. **回歸支持向量機**（SVR）

### 1. **基本概念**

給定一組訓練樣本 \( \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \)，其中 \( x_i \in \mathbb{R}^d \) 是樣本的特徵向量，\( y_i \in \{-1, +1\} \) 是對應的標籤。在SVM的分類問題中，我們的目標是找到一個超平面 \( w^T x + b = 0 \)，將兩個類別的樣本分開，並且保證分類邊界的最大化。

### 2. **最優超平面與間隔**

在SVM中，我們希望找到一個超平面，使得不同類別的樣本點被正確分類，並且兩個類別之間的間隔（margin）最大。間隔是指最接近該超平面的正樣本和負樣本到該超平面的距離。這個最優的超平面可以由以下條件確定：

1. 對於所有的正樣本 \( x_i \)（標籤為 \( y_i = +1 \)），有：
   \[
   w^T x_i + b \geq 1
   \]

2. 對於所有的負樣本 \( x_i \)（標籤為 \( y_i = -1 \)），有：
   \[
   w^T x_i + b \leq -1
   \]

最優超平面應該能夠最大化兩類樣本到這個超平面的間隔。該間隔的大小是：
\[
\frac{2}{\| w \|}
\]
因此，我們的目標是最大化間隔，也就是最小化 \( \| w \|^2 \)，並且確保上述的分類約束條件滿足。

### 3. **優化問題**

SVM的最終目標是解決以下的凸優化問題：

\[
\min_{w, b} \frac{1}{2} \| w \|^2
\]
subject to
\[
y_i (w^T x_i + b) \geq 1, \quad i = 1, 2, \dots, n
\]

這是一個典型的約束優化問題，其中約束條件確保所有的樣本都被正確分類，且分類邊界最大化。

### 4. **拉格朗日對偶與KKT條件**

為了求解這個最優化問題，我們可以使用拉格朗日乘子法。引入拉格朗日乘子 \( \alpha_i \) 對約束條件進行加權，得到拉格朗日函數：

\[
L(w, b, \alpha) = \frac{1}{2} \| w \|^2 - \sum_{i=1}^{n} \alpha_i \left[ y_i (w^T x_i + b) - 1 \right]
\]

其中，\( \alpha_i \geq 0 \) 是拉格朗日乘子，對應於每個約束條件。

對拉格朗日函數求偏導數並令其為零，得到：
- \( \frac{\partial L}{\partial w} = 0 \) 以及
- \( \frac{\partial L}{\partial b} = 0 \)

這兩個條件最終會推導出對偶問題。對偶問題的解會使得最優超平面 \( w \) 和偏置 \( b \) 被確定。

此外，SVM模型的解必須滿足KKT（Karush-Kuhn-Tucker）條件，即每個拉格朗日乘子 \( \alpha_i \) 與相應的約束條件之間的關係。

### 5. **非線性SVM與核方法**

對於線性不可分的情況，SVM可以通過核技巧（Kernel Trick）來實現非線性分類。核函數 \( k(x, x') \) 是一種隱式地將樣本映射到高維特徵空間的方式。在這個空間中，SVM仍然可以找到一個最優超平面來進行分類，而無需顯式地計算高維映射。

常見的核函數包括：
- **線性核** \( k(x, x') = x^T x' \)
- **多項式核** \( k(x, x') = (x^T x' + c)^d \)
- **高斯徑向基核**（RBF核） \( k(x, x') = \exp\left(-\frac{\| x - x' \|^2}{2\sigma^2}\right) \)

通過核技巧，SVM可以在高維特徵空間中找到超平面，並且僅通過計算內積來避免顯式的特徵映射。這樣，SVM就能夠處理線性不可分的情況。

### 6. **支持向量**

在SVM的最優超平面中，有一部分訓練樣本點位於超平面的邊界上，這些樣本稱為支持向量（Support Vectors）。支持向量是對超平面位置有決定性影響的樣本，因為它們恰好位於間隔的邊界上（即 \( y_i (w^T x_i + b) = 1 \)）。這些樣本在最終的分類決策中起到至關重要的作用。

### 7. **支持向量回歸（SVR）**

支持向量機不僅可以用於分類問題，還可以用於回歸問題。支持向量回歸（SVR）旨在尋找一個最適合數據的回歸函數，並且要求這個函數的誤差小於某個容忍範圍，同時保持最大化支持向量的間隔。

SVR的目標是找到一個函數，使得對於大多數數據點，回歸誤差小於一個預設的容忍範圍 \( \epsilon \)，並且盡可能地平滑。

### 8. **SVM的優勢與挑戰**

#### 優勢：
- **高效性**：SVM通常能夠在高維特徵空間中運行良好，且具有較強的泛化能力。
- **理論保證**：SVM基於結構風險最小化原則，具有堅實的理論基礎，可以在有限樣本情況下提供良好的分類性能。
- **適用性廣泛**：SVM可以處理線性與非線性問題，並且對於小樣本、高維度數據有較好的表現。

#### 挑戰：
- **計算開銷**：當訓練樣本數量非常大時，SVM的計算開銷會較高，尤其是在求解對偶問題的過程中。
- **核函數選擇**：選擇合適的核函數是SVM性能的關鍵，錯誤的核函數可能導致模型過擬合或欠擬合。

### 9. **結論**

支持向量機是一個強大的分類和回歸工具，具有堅實的數學理論基礎。通過選擇合適的核函數，SVM可以處理複雜的非線性問題。儘管在大規模數據集上可能存在計算挑戰，但SVM仍然是許多機器學習任務中非常有效且受歡迎的方法之一。