### 線性迴歸 (Linear Regression)

線性迴歸是一種最基本且廣泛使用的回歸分析方法，用於預測數值型的目標變數。其基本假設是，目標變數（或稱應變數）與一組自變數之間存在線性關係，即目標變數可以由自變數的線性組合來表達。

#### 1. **模型假設**

線性迴歸的基本假設是，對於每一個輸入特徵向量 \( \mathbf{x} = (x_1, x_2, \dots, x_n)^\top \)，目標變數 \( y \) 與自變數之間存在一個線性關係：

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon
\]

其中：
- \( y \) 是目標變數，通常是我們要預測的變量。
- \( x_1, x_2, \dots, x_n \) 是自變數或特徵變數，通常是觀察到的數據。
- \( \beta_0, \beta_1, \dots, \beta_n \) 是模型的係數，稱為回歸係數或權重，這些係數需要通過訓練來學習。
- \( \epsilon \) 是隨機誤差項，表示模型未能捕捉到的隨機因素或噪音。

在這個模型中，假設誤差項 \( \epsilon \) 服從零均值、常數方差且相互獨立的隨機分佈。

#### 2. **目標函數**

在線性迴歸中，我們的目標是通過最小化目標函數來估計回歸係數 \( \beta_0, \beta_1, \dots, \beta_n \)。通常，這個目標函數是訓練數據點的**均方誤差** (Mean Squared Error, MSE)：

\[
J(\beta_0, \beta_1, \dots, \beta_n) = \frac{1}{m} \sum_{i=1}^m \left( y_i - \hat{y}_i \right)^2
\]

其中：
- \( m \) 是訓練樣本的數量。
- \( y_i \) 是第 \( i \) 個訓練樣本的真實值。
- \( \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_n x_{in} \) 是模型對第 \( i \) 個樣本的預測值。

這個均方誤差衡量了預測值與真實值之間的差異，目標是最小化這個誤差。

#### 3. **解析解：最小二乘法**

對於線性迴歸，最常見的求解方法是最小二乘法。這個方法的基本思想是，通過最小化均方誤差，來獲得最佳的回歸係數。假設訓練數據集有 \( m \) 個樣本，並且有 \( n \) 個特徵，則線性模型的預測結果可以表示為：

\[
\hat{y} = X \beta
\]

其中：
- \( \hat{y} \) 是 \( m \times 1 \) 的預測向量。
- \( X \) 是 \( m \times (n+1) \) 的設計矩陣，其中每行對應一個樣本，第一列為全1（對應偏置項 \( \beta_0 \)）。
- \( \beta \) 是 \( (n+1) \times 1 \) 的回歸係數向量，包含了 \( \beta_0, \beta_1, \dots, \beta_n \)。

最小化均方誤差等價於求解下列最小化問題：

\[
\beta = \arg \min_{\beta} \left( \frac{1}{m} \| X \beta - y \|^2 \right)
\]

這是一個凸二次優化問題，通過求解下面的正常方程（Normal Equation）來得到回歸係數的解析解：

\[
\beta = (X^\top X)^{-1} X^\top y
\]

其中，\( X^\top \) 是 \( X \) 的轉置，\( (X^\top X)^{-1} \) 是 \( X^\top X \) 的逆矩陣，且要求 \( X^\top X \) 可逆。

#### 4. **梯度下降法**

對於較大規模的數據集，解析解計算可能會非常昂貴，因此可以使用梯度下降法來進行參數的迭代更新。梯度下降法的基本思想是，從某一隨機初始化的回歸係數 \( \beta^{(0)} \) 開始，通過不斷地更新回歸係數來最小化均方誤差。具體的更新公式為：

\[
\beta^{(k+1)} = \beta^{(k)} - \alpha \nabla J(\beta^{(k)})
\]

其中：
- \( \alpha \) 是學習率，控制每次更新的步長。
- \( \nabla J(\beta^{(k)}) \) 是均方誤差對回歸係數的梯度，計算公式為：

\[
\nabla J(\beta) = -\frac{2}{m} X^\top (y - X \beta)
\]

通過重複這個更新過程，回歸係數將逐步逼近最小均方誤差。

#### 5. **正則化**

在實際應用中，線性迴歸的模型可能會受到過擬合的影響，特別是在特徵數量很多的情況下。為了防止過擬合，可以引入正則化技術來限制回歸係數的大小。

- **L2正則化**（Ridge回歸）：通過加入L2正則化項 \( \lambda \| \beta \|^2 \) 來控制係數的大小，其中 \( \lambda \) 是正則化參數。正則化後的目標函數為：

\[
J(\beta) = \frac{1}{m} \sum_{i=1}^m \left( y_i - \hat{y}_i \right)^2 + \lambda \| \beta \|^2
\]

- **L1正則化**（Lasso回歸）：通過加入L1正則化項 \( \lambda \| \beta \|_1 \) 來使得某些係數變為零，實現特徵選擇。正則化後的目標函數為：

\[
J(\beta) = \frac{1}{m} \sum_{i=1}^m \left( y_i - \hat{y}_i \right)^2 + \lambda \| \beta \|_1
\]

#### 6. **總結**

線性迴歸是一種簡單且強大的回歸方法，適用於處理自變數與應變數之間的線性關係。通過最小二乘法可以獲得其解析解，或者使用梯度下降法進行數值優化。在實際應用中，正則化技術可用於提高模型的泛化能力並防止過擬合。