### 邏輯迴歸 (Logistic Regression)

**概念解釋：**
邏輯迴歸是一種用於二分類問題的統計學方法，目的是預測事件發生的概率。與線性迴歸不同，邏輯迴歸的目標是將預測值映射到0和1之間，這樣可以表示二分類問題中的類別概率。其基本思想是使用邏輯斯迴歸（Logistic Function，或稱為 Sigmoid 函數）來進行分類。

邏輯斯函數的公式為：

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

其中，\( z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n \) 是一個線性組合。這個函數的輸出範圍為 (0, 1)，可以解釋為某事件發生的概率。

邏輯迴歸模型的預測公式為：

\[
P(y = 1 | x) = \sigma(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)
\]

這裡，\( P(y = 1 | x) \) 表示給定輸入特徵 \( x \)，事件 \( y = 1 \) 發生的概率。

**目標：**
邏輯迴歸的目標是最小化損失函數，通常使用 **對數損失函數**（Log-Loss）來衡量模型的預測誤差。對數損失函數的形式為：

\[
L(\beta) = -\frac{1}{N} \sum_{i=1}^{N} \left[y_i \log(p_i) + (1 - y_i) \log(1 - p_i)\right]
\]

其中：
- \( N \) 是樣本數量。
- \( y_i \) 是樣本的實際類別（0 或 1）。
- \( p_i = P(y = 1 | x_i) \) 是模型預測為1的概率。

最小化這個損失函數就可以得到最佳的回歸係數 \( \beta_0, \beta_1, \dots, \beta_n \)。

### Python範例：邏輯迴歸模型

以下範例展示了如何使用 `scikit-learn` 訓練邏輯迴歸模型並進行預測：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# 生成二分類數據
X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)

# 分割數據為訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 創建邏輯迴歸模型
model = LogisticRegression()

# 訓練模型
model.fit(X_train, y_train)

# 預測測試集
y_pred = model.predict(X_test)

# 計算準確率
accuracy = accuracy_score(y_test, y_pred)
print(f"準確率: {accuracy}")

# 混淆矩陣
cm = confusion_matrix(y_test, y_pred)
print(f"混淆矩陣:\n{cm}")

# 可視化邏輯迴歸決策邊界
xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),
                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.RdYlBu)
plt.title("邏輯迴歸決策邊界")
plt.xlabel("特徵 1")
plt.ylabel("特徵 2")
plt.show()
```

### 程式解釋：
1. **數據生成**：使用 `make_classification` 函數生成二分類的隨機數據，這裡有兩個特徵。
2. **數據分割**：使用 `train_test_split` 函數將數據分割為訓練集和測試集。
3. **創建和訓練模型**：使用 `LogisticRegression` 創建邏輯迴歸模型，並使用訓練集來訓練模型。
4. **預測與評估**：使用訓練好的模型對測試集進行預測，並計算準確率（Accuracy）和混淆矩陣（Confusion Matrix）來評估模型的預測能力。
5. **可視化決策邊界**：使用 `matplotlib` 繪製決策邊界，展示邏輯迴歸模型的分類邊界。

### 結果：
- **準確率**：輸出模型的準確率，表示模型預測正確的樣本比例。
- **混淆矩陣**：展示模型的分類結果，顯示真陽性（TP）、假陽性（FP）、真陰性（TN）和假陰性（FN）的數量。
- **決策邊界**：通過可視化決策邊界，觀察邏輯迴歸模型如何區分兩個類別。

### 優點：
- 邏輯迴歸是一個簡單而高效的分類模型，尤其適用於線性可分的數據。
- 模型訓練和推理速度較快，適合用於大規模數據集。
- 它的概率輸出可以進一步用於概率分析和不確定性量化。

### 缺點：
- 僅適用於線性可分的問題，對於非線性邊界的問題，模型表現較差。
- 可能對於特徵間高度相關的情況（多重共線性）表現不佳。

### 小結：
邏輯迴歸是一個廣泛使用的二分類模型，簡單、有效且易於解釋。當面臨非線性邊界的問題時，可以考慮使用核方法或其他更複雜的模型（如支持向量機、神經網絡等）。