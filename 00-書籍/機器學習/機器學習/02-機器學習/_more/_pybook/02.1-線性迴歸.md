### 線性迴歸 (Linear Regression)

**概念解釋：**
線性迴歸是一種基於數據點之間的線性關係來進行預測的統計方法。這種方法假設自變量（輸入特徵）和因變量（目標變量）之間存在線性關係。其目的是找到一條最佳擬合線，使得預測值和實際觀察值之間的誤差最小。

對於一個簡單的單變量線性迴歸，模型可以表示為：

\[
y = \beta_0 + \beta_1 x + \epsilon
\]

其中：
- \( y \) 是目標變量（預測值）。
- \( x \) 是自變量（特徵）。
- \( \beta_0 \) 是截距項（常數）。
- \( \beta_1 \) 是斜率（回歸係數），表示每單位 \( x \) 變動對 \( y \) 的影響。
- \( \epsilon \) 是誤差項，表示觀察值和預測值之間的偏差。

對於多變量線性迴歸，模型可以擴展為：

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon
\]

這裡，\( x_1, x_2, \dots, x_n \) 是多個特徵，\( \beta_1, \beta_2, \dots, \beta_n \) 是每個特徵的回歸係數。

**目標：**
我們的目標是最小化損失函數，通常使用均方誤差（MSE，Mean Squared Error）來衡量模型的預測誤差。均方誤差定義為：

\[
MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y_i})^2
\]

其中：
- \( N \) 是樣本數量。
- \( y_i \) 是實際值。
- \( \hat{y_i} \) 是預測值。

**最小化 MSE：**
為了求解最佳的回歸係數（\( \beta_0, \beta_1, \dots, \beta_n \)），我們可以使用最小二乘法（Ordinary Least Squares, OLS），即最小化均方誤差來找到回歸係數。

### Python範例：線性迴歸模型

以下範例展示了如何使用 `scikit-learn` 訓練線性迴歸模型並進行預測：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成模擬數據
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # 特徵數據
y = 2.5 * X + np.random.randn(100, 1) * 2  # 目標變量，加入噪聲

# 分割數據為訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 創建線性回歸模型
model = LinearRegression()

# 訓練模型
model.fit(X_train, y_train)

# 預測測試集
y_pred = model.predict(X_test)

# 計算均方誤差
mse = mean_squared_error(y_test, y_pred)
print(f"均方誤差 (MSE): {mse}")

# 繪製結果
plt.scatter(X_test, y_test, color='blue', label='實際值')
plt.plot(X_test, y_pred, color='red', label='預測值', linewidth=2)
plt.xlabel('特徵 (X)')
plt.ylabel('目標變量 (y)')
plt.legend()
plt.title('線性回歸預測')
plt.show()
```

### 程式解釋：
1. **數據生成**：我們首先生成一些隨機數據，使用 `np.random.rand` 生成特徵 \( X \)，並使用 \( y = 2.5X + \text{噪聲} \) 來創建目標變量 \( y \)（其中噪聲是正態分佈）。
2. **數據分割**：使用 `train_test_split` 函數將數據分為訓練集和測試集，這裡將 20% 的數據作為測試集。
3. **創建和訓練模型**：創建 `LinearRegression` 模型並訓練它，學習回歸係數 \( \beta_0, \beta_1 \)。
4. **預測與評估**：使用訓練好的模型對測試集進行預測，並計算均方誤差（MSE）來評估模型的預測性能。
5. **結果可視化**：使用 `matplotlib` 繪製測試集的實際值和模型的預測值，以可視化回歸效果。

### 結果：
- **均方誤差 (MSE)** 是用來衡量預測誤差的指標，越小表示模型的預測越準確。
- **預測曲線**：模型的預測線與實際數據點之間的擬合情況會顯示在圖表中，理想情況下，預測線應該盡量接近實際數據點。

### 優點：
- 線性迴歸模型直觀且計算簡單，適合於對線性關係建模。
- 可解釋性強，回歸係數 \( \beta_1, \beta_2, \dots \) 可以直接解釋每個特徵對預測結果的影響。

### 缺點：
- 僅適用於線性關係，對於非線性關係可能表現不佳。
- 對於特徵間的多重共線性（Multicollinearity）非常敏感。

### 小結：
線性迴歸是一個強大而簡單的模型，適用於許多基本的預測任務。它可以很好地捕捉線性關係，但對於更複雜的非線性關係則需要其他方法（如多項式回歸、支持向量回歸等）。