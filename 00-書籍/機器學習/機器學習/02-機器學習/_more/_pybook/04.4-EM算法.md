### EM算法（Expectation-Maximization Algorithm）

EM（Expectation-Maximization）算法是一種迭代算法，用於最大化含有隱變量的概率模型的似然函數。當我們的模型包含隱變量（例如，混合高斯模型中的潛在類別變量）且觀察到的數據存在缺失或不完全時，EM算法是一種常用的方法。

EM算法的核心思想是利用期望步驟（E步驟）和最大化步驟（M步驟）來迭代地更新參數，直到達到收斂。

#### 1. 基本概念

EM算法假設觀察數據 \( \mathbf{X} \) 由隱變量 \( \mathbf{Z} \) 決定，且我們對隱變量的後驗分佈不可得，這使得直接最大化觀察數據的對數似然函數變得困難。EM算法通過兩個步驟來解決這個問題：

- **E步驟（Expectation Step）**：根據當前的模型參數 \( \theta^{(t)} \)，計算隱變量的條件期望，即計算隱變量 \( \mathbf{Z} \) 的後驗分佈。
  
- **M步驟（Maximization Step）**：根據E步驟計算的期望，最大化似然函數，更新模型參數。

#### 2. 數學推導

假設我們有觀察數據 \( \mathbf{X} \) 和隱變量 \( \mathbf{Z} \)，並且希望最大化似然函數 \( p(\mathbf{X} | \theta) \)，但由於隱變量 \( \mathbf{Z} \) 的存在，這個問題變得難以直接處理。EM算法的目標是最大化以下的對數似然函數：

\[
\log p(\mathbf{X} | \theta) = \log \int p(\mathbf{X}, \mathbf{Z} | \theta) \, d\mathbf{Z}
\]

這裡，直接計算積分是非常困難的，因此 EM 算法引入了隱變量 \( \mathbf{Z} \) 的條件期望：

\[
\mathbb{Q}(\theta | \theta^{(t)}) = \mathbb{E}_{\mathbf{Z} | \mathbf{X}, \theta^{(t)}} [\log p(\mathbf{X}, \mathbf{Z} | \theta)]
\]

這樣，我們就將原來的最大化問題轉化為最大化期望步驟的結果。EM算法的兩步過程如下：

1. **E步驟**：計算隱變量的後驗分佈：

\[
Q(\theta | \theta^{(t)}) = \mathbb{E}_{\mathbf{Z} | \mathbf{X}, \theta^{(t)}} [\log p(\mathbf{X}, \mathbf{Z} | \theta)]
\]

2. **M步驟**：最大化期望：

\[
\theta^{(t+1)} = \arg\max_{\theta} Q(\theta | \theta^{(t)})
\]

這樣，通過交替執行E步驟和M步驟，EM算法會逐步逼近最大似然估計。

#### 3. EM算法的迭代過程

EM算法的每次迭代包括以下兩個步驟：

- **E步驟（Expectation）**：根據當前參數 \( \theta^{(t)} \) 計算隱變量的後驗分佈或期望。
  
- **M步驟（Maximization）**：基於E步驟計算出的期望，更新參數 \( \theta^{(t+1)} \)，使得似然函數最大化。

這樣交替進行，直到收斂（即參數不再更新或改變非常小）。

#### 4. Python 實現（混合高斯模型的 EM 算法）

以下是一個基於 EM 算法的簡單示例，演示如何使用 EM 算法來擬合一個高斯混合模型（GMM）。

```python
import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# 生成混合高斯數據
np.random.seed(42)
n_samples = 300
X1 = np.random.normal(loc=-5, scale=1, size=(n_samples // 2, 2))
X2 = np.random.normal(loc=5, scale=1, size=(n_samples // 2, 2))
X = np.vstack([X1, X2])

# 使用GaussianMixture進行EM算法
gmm = GaussianMixture(n_components=2, covariance_type='full')
gmm.fit(X)

# 顯示結果
print(f"均值：\n{gmm.means_}")
print(f"協方差矩陣：\n{gmm.covariances_}")
print(f"混合權重：\n{gmm.weights_}")

# 預測樣本屬於哪個高斯成分
labels = gmm.predict(X)

# 可視化結果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.title("EM Algorithm - Gaussian Mixture Model")
plt.show()
```

#### 5. 程式解釋

- **數據生成**：我們首先生成兩個高斯分佈的數據集 `X1` 和 `X2`，然後將它們合併為一個數據集 `X`。這代表了我們要用 EM 算法擬合的混合高斯模型的觀察數據。
  
- **高斯混合模型**：我們使用 `sklearn.mixture.GaussianMixture` 來擬合一個具有兩個成分的高斯混合模型。`fit` 方法會自動執行 EM 算法。

- **結果顯示**：輸出高斯混合模型的均值、協方差矩陣和混合權重，並根據擬合結果進行分類，將每個數據點分配到某個高斯成分。

- **可視化**：使用 Matplotlib 顯示每個數據點的顏色，根據其所屬的高斯成分進行區分。

#### 6. 小結

EM算法是一種強大的迭代算法，用於含有隱變量的概率模型的最大似然估計。它通過交替執行E步驟（期望步驟）和M步驟（最大化步驟）來優化模型參數。在實際應用中，EM算法廣泛應用於高斯混合模型、隱馬爾可夫模型、主題建模等領域。