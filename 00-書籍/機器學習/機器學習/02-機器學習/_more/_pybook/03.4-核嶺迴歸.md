### 核嶺迴歸（Kernel Ridge Regression, KRR）

核嶺迴歸是結合了核方法和嶺迴歸（Ridge Regression）的一種技術，它用於解決非線性迴歸問題。傳統的嶺迴歸使用線性模型，並通過正則化來減少過擬合。然而，對於非線性數據，核嶺迴歸通過使用核函數將數據映射到高維空間，使得線性模型能夠適應更複雜的數據結構。

#### 1. 基本概念

在標準的線性回歸中，我們尋找最佳的係數 \( \mathbf{w} \) 來最小化損失函數：

\[
\mathcal{L}(\mathbf{w}) = \| \mathbf{y} - X\mathbf{w} \|^2_2
\]

其中，\( X \) 是特徵矩陣，\( \mathbf{y} \) 是目標變數。當我們引入正則化時，損失函數變為：

\[
\mathcal{L}(\mathbf{w}) = \| \mathbf{y} - X\mathbf{w} \|^2_2 + \lambda \| \mathbf{w} \|^2_2
\]

其中，\( \lambda \) 是正則化參數，用來控制模型的複雜度。

核嶺迴歸則將這一方法擴展到非線性情況。在這裡，我們使用核函數 \( k(x_i, x_j) \) 來計算數據點之間的相似度，並將其映射到一個高維特徵空間，從而使得數據變得線性可分。

#### 2. 核嶺迴歸的數學模型

核嶺迴歸的目標是解決如下的優化問題：

\[
\min_{\mathbf{w}} \left[ \| \mathbf{y} - K\mathbf{w} \|^2_2 + \lambda \| \mathbf{w} \|^2_2 \right]
\]

其中，\( K \) 是核矩陣，元素 \( K_{ij} = k(x_i, x_j) \) 表示數據點 \( x_i \) 和 \( x_j \) 之間的相似度。

這個問題的解是：

\[
\mathbf{w} = (K + \lambda I)^{-1} \mathbf{y}
\]

其中，\( I \) 是單位矩陣，\( \lambda \) 是正則化參數，控制模型的複雜度。

#### 3. Python 實現

我們可以使用 `scikit-learn` 中的 `KernelRidge` 類來實現核嶺迴歸。以下是使用 RBF 核進行核嶺迴歸的範例：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.kernel_ridge import KernelRidge
from sklearn.gaussian_process.kernels import RBF

# 生成一個非線性數據集
np.random.seed(0)
X = np.sort(np.random.rand(100, 1), axis=0)
y = np.sin(2 * np.pi * X).ravel() + 0.1 * np.random.randn(100)

# 創建核嶺迴歸模型
krr = KernelRidge(alpha=1.0, kernel='rbf', gamma=1.0)

# 擬合模型
krr.fit(X, y)

# 預測
X_test = np.linspace(0, 1, 1000).reshape(-1, 1)
y_pred = krr.predict(X_test)

# 可視化結果
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='red', label='Training data')
plt.plot(X_test, y_pred, color='blue', label='Kernel Ridge Prediction')
plt.title('Kernel Ridge Regression')
plt.legend()
plt.show()
```

### 程式解釋：
1. **數據生成**：我們生成一組非線性數據，這些數據由正弦函數生成並加入了隨機噪聲。
2. **模型創建**：使用 `KernelRidge` 類來創建核嶺迴歸模型，並選擇 RBF 核。`alpha` 參數是正則化參數，`gamma` 是 RBF 核的參數。
3. **擬合與預測**：將模型擬合到訓練數據上，然後對新數據進行預測。
4. **可視化**：使用 `matplotlib` 來繪製訓練數據和預測結果。

#### 4. 小結

核嶺迴歸是一種強大的非線性迴歸方法，它結合了核方法和嶺迴歸的優點，可以有效地處理非線性問題。與線性迴歸不同，核嶺迴歸通過選擇合適的核函數（如 RBF 核）來擴展模型，能夠捕捉數據中的非線性結構。