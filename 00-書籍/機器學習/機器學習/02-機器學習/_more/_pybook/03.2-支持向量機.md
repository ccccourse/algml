### 支持向量機（Support Vector Machines, SVM）

支持向量機（SVM）是一種強大的分類方法，特別適用於高維空間的分類問題。它的基本目標是尋找一個超平面，能夠最大化地分開不同類別的數據點。SVM 不僅可以處理線性可分的情況，還可以透過核方法（kernel trick）來處理非線性可分的情況。

#### 1. 基本概念

SVM 的核心概念是找到一個超平面來最大化不同類別的間隔（margin）。這樣的超平面能夠將樣本分開，並且保持最大的分類邊界。

- **超平面（Hyperplane）**：在 \( n \)-維空間中，一個 \( (n-1) \)-維的平面，能夠將數據分成兩個部分。對於二分類問題，超平面是將數據點分開的線或面。
  
- **支持向量（Support Vectors）**：支持向量是離分類邊界最近的數據點。這些數據點決定了最佳超平面的位置。

- **最大間隔（Maximal Margin）**：SVM 目標是選擇一個超平面，使得正負類數據點的間隔最大化。這樣的間隔能夠提高模型的泛化能力。

#### 2. 數學模型

對於線性可分的情況，給定訓練數據 \( (x_i, y_i) \) 其中 \( x_i \in \mathbb{R}^n \) 為特徵，\( y_i \in \{-1, 1\} \) 為標籤，SVM 旨在找到一個超平面 \( w^T x + b = 0 \)，使得：
- 所有正類樣本 \( y_i = 1 \) 都滿足 \( w^T x_i + b \geq 1 \)
- 所有負類樣本 \( y_i = -1 \) 都滿足 \( w^T x_i + b \leq -1 \)

這樣的超平面能夠最大化兩類樣本的間隔，並且我們要最大化間隔，即最小化 \( \frac{1}{2} \| w \|^2 \)，同時滿足分類約束：
\[
\min_{w, b} \frac{1}{2} \| w \|^2 \quad \text{subject to} \quad y_i (w^T x_i + b) \geq 1, \forall i
\]

#### 3. 核方法

SVM 不僅限於線性可分的情況，還可以通過核方法處理非線性問題。核方法的核心思想是將數據映射到高維空間，在這個空間中，數據可能變得線性可分。

- **核函數**：核函數 \( k(x, x') \) 定義了兩個樣本點 \( x \) 和 \( x' \) 在高維空間中的內積，可以幫助我們計算映射後的內積。常見的核函數包括：
  - 線性核：\( k(x, x') = x^T x' \)
  - 多項式核：\( k(x, x') = (x^T x' + c)^d \)
  - 高斯徑向基核（RBF 核）：\( k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right) \)

通過使用核函數，SVM 可以在高維空間中進行分類，而不需要顯式地計算映射過程，這被稱為“核技巧”（kernel trick）。

#### 4. Python 實現

以下是使用 `scikit-learn` 庫實現支持向量機的範例：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# 生成非線性可分的數據集
X, y = datasets.make_circles(n_samples=100, factor=0.5, noise=0.1)

# 將數據分為訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 使用RBF核的SVM進行分類
svm_model = SVC(kernel='rbf', C=1, gamma='scale')
svm_model.fit(X_train, y_train)

# 打印模型在測試集上的準確率
accuracy = svm_model.score(X_test, y_test)
print(f"SVM 模型準確率：{accuracy * 100:.2f}%")

# 可視化結果
xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100),
                     np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100))

Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.75)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k', cmap=plt.cm.Paired)
plt.title("SVM with RBF Kernel on Nonlinear Data")
plt.show()
```

### 程式解釋：
1. **數據生成**：使用 `make_circles` 生成一個二分類的非線性數據集，數據具有圓形結構，這在原始空間中是非線性可分的。
2. **SVM 訓練**：利用 `SVC`（支持向量分類器）來訓練 SVM 模型，這裡使用的是 RBF 核函數來處理非線性邊界。
3. **可視化**：使用 `matplotlib` 可視化分類結果，並顯示 SVM 模型在決策邊界上的效果。

#### 5. 小結

支持向量機（SVM）是一種強大的分類工具，其通過最大化類別間的間隔來進行分類。SVM 可以處理線性問題，也可以利用核方法來處理非線性問題。透過核技巧，SVM 不僅能有效解決非線性問題，還能在高維空間中找到最佳的分類邊界。