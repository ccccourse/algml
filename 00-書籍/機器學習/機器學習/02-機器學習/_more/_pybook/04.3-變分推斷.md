### 變分推斷（Variational Inference, VI）

變分推斷是一種近似推理方法，廣泛應用於處理難以直接計算後驗分佈的問題。在許多統計模型中，直接計算後驗分佈可能是計算上不切實際的，尤其是當模型的複雜度很高或數據量非常大時。變分推斷通過將推理問題轉化為優化問題，來以近似的方式解決後驗分佈的推理。

#### 1. 基本概念

變分推斷的基本思想是，將目標後驗分佈 \( p(\mathbf{z} | \mathbf{x}) \) 近似為一個簡單的分佈 \( q(\mathbf{z}) \)，這個簡單的分佈需要在某個意義下最接近目標分佈。常見的方法是選擇 \( q(\mathbf{z}) \) 為一組簡單的分佈族，並且用 Kullback-Leibler (KL) 散度來衡量其與真實後驗分佈的差異。

變分推斷的目標是通過最小化 KL 散度來找到最佳的 \( q(\mathbf{z}) \)，即：

\[
\text{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) = \mathbb{E}_{q}[\log q(\mathbf{z}) - \log p(\mathbf{z}, \mathbf{x})]
\]

#### 2. 變分推斷的數學推導

假設我們有一個隱變量模型，並希望對隱變量 \( \mathbf{z} \) 進行推理。給定觀察數據 \( \mathbf{x} \)，我們的目標是計算後驗分佈 \( p(\mathbf{z} | \mathbf{x}) \)，但這通常難以直接計算。因此，我們引入一個變分分佈 \( q(\mathbf{z}) \)，並最小化以下目標：

\[
\mathcal{L}(q) = \mathbb{E}_{q}[\log p(\mathbf{x}, \mathbf{z}) - \log q(\mathbf{z})]
\]

這個式子是對數邊際似然的變分下界，通常稱為 ELBO（Evidence Lower Bound）。通過最大化 ELBO，我們可以使變分分佈 \( q(\mathbf{z}) \) 最接近目標後驗分佈。

#### 3. 優化過程

為了得到變分分佈 \( q(\mathbf{z}) \)，我們需要最大化 ELBO，這通常使用梯度下降法或其他優化方法來實現。具體步驟如下：

1. **選擇變分家族**：選擇一個簡單的分佈族來近似後驗分佈 \( p(\mathbf{z} | \mathbf{x}) \)，通常選擇族為可解的分佈，如高斯分佈、共軛分佈等。
   
2. **定義 ELBO**：計算 ELBO，即對數邊際似然的下界。

3. **最大化 ELBO**：使用優化方法（如梯度下降）來最大化 ELBO，從而找到最好的變分分佈。

4. **推理**：一旦得到了變分分佈 \( q(\mathbf{z}) \)，就可以用來進行推理，例如計算 \( \mathbf{z} \) 的預測分佈，或估計隱變量。

#### 4. Python 實現（使用 `PyTorch`）

以下是使用變分推斷的簡單 Python 程式，通過 `PyTorch` 實現變分推斷來估計隱變量的後驗分佈。此範例演示了如何在變分推斷中最小化 ELBO。

```python
import torch
import torch.optim as optim
from torch.distributions import Normal

# 定義模型的先驗分佈和觀察模型
class VariationalInferenceModel:
    def __init__(self):
        # 假設隱變量 z 的先驗是標準正態分佈
        self.prior = Normal(torch.tensor([0.0]), torch.tensor([1.0]))
        
        # 假設觀察數據 x 給定隱變量 z 的條件分佈是高斯
        self.likelihood = Normal(torch.tensor([0.0]), torch.tensor([1.0]))

    def log_likelihood(self, z, x):
        """計算對數似然"""
        return self.likelihood.log_prob(x).sum()

    def log_prior(self, z):
        """計算先驗的對數"""
        return self.prior.log_prob(z).sum()

# 定義變分分佈 q(z)
class VariationalDistribution:
    def __init__(self):
        # 假設 q(z) 是高斯分佈，初始均值為 0，標準差為 1
        self.mean = torch.tensor([0.0], requires_grad=True)
        self.log_var = torch.tensor([0.0], requires_grad=True)

    def sample(self):
        """從變分分佈中採樣"""
        std = torch.exp(0.5 * self.log_var)
        return Normal(self.mean, std).sample()

    def log_prob(self, z):
        """計算變分分佈的對數概率"""
        std = torch.exp(0.5 * self.log_var)
        return Normal(self.mean, std).log_prob(z).sum()

# 計算 ELBO
def elbo(model, q, x):
    """計算 ELBO，即變分下界"""
    z = q.sample()  # 從 q(z) 中採樣
    return model.log_likelihood(z, x) + model.log_prior(z) - q.log_prob(z)

# 優化過程
def variational_inference():
    # 初始化模型和變分分佈
    model = VariationalInferenceModel()
    q = VariationalDistribution()
    
    # 假設觀察數據 x 是來自標準正態分佈的樣本
    x = torch.tensor([1.0])  # 觀察數據
    
    # 使用 Adam 優化器來最小化 ELBO
    optimizer = optim.Adam([q.mean, q.log_var], lr=0.01)
    
    for step in range(1000):
        optimizer.zero_grad()
        # 計算 ELBO
        loss = -elbo(model, q, x)  # 最小化負 ELBO
        loss.backward()
        optimizer.step()
        
        if step % 100 == 0:
            print(f"Step {step}: Loss = {loss.item()}")
    
    # 返回學習到的變分分佈參數
    return q.mean, q.log_var

# 執行變分推斷
mean, log_var = variational_inference()
print(f"學習到的變分分佈均值：{mean.item()}")
print(f"學習到的變分分佈對數方差：{log_var.item()}")
```

#### 5. 程式解釋：

- **模型設計**：`VariationalInferenceModel` 類包含了先驗分佈和觀察數據的似然分佈。此處使用的是高斯分佈作為先驗和似然。
- **變分分佈**：`VariationalDistribution` 類定義了變分分佈 \( q(z) \)，假設其為高斯分佈，並且可以通過均值和對數方差來參數化。
- **ELBO 計算**：`elbo` 函數計算了變分下界，即 ELBO，並使用變分分佈 \( q(z) \) 和模型來計算。
- **優化過程**：使用 Adam 優化器來最大化 ELBO（最小化負 ELBO）。在每個步驟中，通過反向傳播來更新變分分佈的參數。

### 6. 小結

變分推斷是一種有效的推理方法，用於解決高維度模型中的後驗分佈計算問題。通過將推理問題轉化為優化問題，變分推斷提供了一種近似推理的解決方案，適用於許多難以處理的概率圖模型。在實際應用中，變分推斷被廣泛應用於主題模型、生成模型和深度學習中的變分自編碼器（VAE）等領域。