### 邏輯迴歸 (Logistic Regression)

邏輯迴歸是一種用於二分類問題的統計學方法，雖然名稱中有「迴歸」，但它實際上是一個分類模型。它用於預測一個事件的概率，該事件的結果只能是兩個可能的結果之一，通常表示為 \(0\) 或 \(1\)（例如：成功與失敗、是與否）。邏輯迴歸模型基於邏輯函數（Sigmoid函數），它將線性回歸的輸出映射到 \(0\) 與 \(1\) 之間。

#### 1. **模型假設**

在邏輯迴歸中，假設目標變數 \( y \) 取值為 \( 0 \) 或 \( 1 \)，且對應於輸入特徵向量 \( \mathbf{x} = (x_1, x_2, \dots, x_n)^\top \)，模型假設目標變數的概率 \( P(y = 1 | \mathbf{x}) \) 是一個線性組合的函數，但這個組合經過了非線性的映射，使其輸出限制在 \(0\) 和 \(1\) 之間。

具體地，邏輯迴歸模型假設 \( P(y = 1 | \mathbf{x}) \) 可以表示為邏輯函數（Sigmoid函數）的形式：

\[
P(y = 1 | \mathbf{x}) = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n)
\]

其中，\( \sigma(z) \) 是邏輯函數，定義為：

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

這樣，\( P(y = 1 | \mathbf{x}) \) 就是一個介於 \(0\) 和 \(1\) 之間的值，表示 \( y = 1 \) 的概率。

而 \( P(y = 0 | \mathbf{x}) \) 則為：

\[
P(y = 0 | \mathbf{x}) = 1 - P(y = 1 | \mathbf{x}) = 1 - \sigma(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)
\]

#### 2. **損失函數：對數似然函數**

邏輯迴歸的目標是學習最佳的回歸係數 \( \beta_0, \beta_1, \dots, \beta_n \)，使得模型對觀察到的數據的預測概率最大化。對於每一個訓練樣本 \( (x_i, y_i) \)，邏輯迴歸的預測概率為 \( P(y = 1 | x_i) = \sigma(\mathbf{x}_i^\top \beta) \)，因此可以寫出每個樣本的對數似然（Log-Likelihood）：

\[
\mathcal{L}(\beta) = \sum_{i=1}^m \left[ y_i \log(\sigma(\mathbf{x}_i^\top \beta)) + (1 - y_i) \log(1 - \sigma(\mathbf{x}_i^\top \beta)) \right]
\]

這是一個對數似然函數，其中：
- \( y_i \) 是第 \( i \) 個訓練樣本的實際標籤，取值為 \(0\) 或 \(1\)。
- \( \sigma(\mathbf{x}_i^\top \beta) \) 是模型預測第 \( i \) 個樣本為類別 \( 1 \) 的概率。

邏輯迴歸的目標是最大化對數似然函數，這樣可以獲得最佳的回歸係數。

#### 3. **最大化對數似然**

最大化對數似然函數等同於最小化負對數似然函數。對於多個訓練樣本，負對數似然函數為：

\[
J(\beta) = - \frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\sigma(\mathbf{x}_i^\top \beta)) + (1 - y_i) \log(1 - \sigma(\mathbf{x}_i^\top \beta)) \right]
\]

這是一個凸優化問題，目標是對 \( \beta \) 進行優化，使得該函數最小化。

#### 4. **梯度下降法**

由於對數似然函數是一個凸函數，我們可以使用梯度下降法來尋找最佳的回歸係數 \( \beta \)。梯度下降法的基本更新規則為：

\[
\beta^{(k+1)} = \beta^{(k)} - \alpha \nabla J(\beta^{(k)})
\]

其中 \( \alpha \) 是學習率，\( \nabla J(\beta) \) 是負對數似然函數的梯度。計算梯度：

\[
\nabla J(\beta) = \frac{1}{m} \sum_{i=1}^m \left[ \sigma(\mathbf{x}_i^\top \beta) - y_i \right] \mathbf{x}_i
\]

通過反覆更新回歸係數 \( \beta \)，梯度下降法將最小化負對數似然函數，並最終收斂到最佳的回歸係數。

#### 5. **正則化**

類似於線性迴歸，邏輯迴歸也可以使用正則化來防止過擬合。常見的正則化方法包括 L1 正則化（Lasso）和 L2 正則化（Ridge）。這些正則化方法在對數似然函數中加入額外的項，來限制回歸係數的大小。

- **L2 正則化**（Ridge 回歸）：正則化項為 \( \lambda \| \beta \|_2^2 \)。
- **L1 正則化**（Lasso 回歸）：正則化項為 \( \lambda \| \beta \|_1 \)。

正則化後的損失函數（對於 L2 正則化）為：

\[
J(\beta) = - \frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\sigma(\mathbf{x}_i^\top \beta)) + (1 - y_i) \log(1 - \sigma(\mathbf{x}_i^\top \beta)) \right] + \lambda \|\beta\|_2^2
\]

#### 6. **總結**

邏輯迴歸是一種用於二分類問題的概率模型，它利用邏輯函數將線性回歸的輸出映射到 \( 0 \) 和 \( 1 \) 之間，從而可以解釋為預測事件發生的概率。邏輯迴歸的訓練過程是最大化對數似然函數，並且可以通過梯度下降法來進行優化。正則化技術可用於提高模型的泛化能力。