### 序列建模 - RNN 理論（Recurrent Neural Networks）

遞歸神經網絡（RNN，Recurrent Neural Networks）是一種專門設計來處理序列數據的神經網絡架構，能夠捕捉序列中時間步之間的依賴關係。RNN廣泛應用於語言建模、機器翻譯、語音識別、時間序列預測等任務中，尤其適合於處理長度可變的輸入序列。

#### 2.1 RNN的基本結構

RNN的主要特點是其在每個時間步有一個隱藏層狀態，它將前一時刻的隱藏狀態與當前時間步的輸入一起用來計算當前時刻的隱藏狀態。這種反向傳遞信息的結構，使得RNN能夠捕捉長期的序列依賴關係。

在一個標準的RNN中，假設輸入序列為 \( x_1, x_2, ..., x_T \)，其對應的隱藏狀態序列為 \( h_1, h_2, ..., h_T \)，並且最終的輸出為 \( y_1, y_2, ..., y_T \)。

RNN的數學模型可以表示為：

\[
h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
\]
\[
y_t = W_{hy} h_t + b_y
\]

其中：

- \( h_t \)：在時間步 \( t \) 的隱藏狀態。
- \( x_t \)：時間步 \( t \) 的輸入。
- \( y_t \)：時間步 \( t \) 的輸出。
- \( W_{hh} \)：隱藏層到隱藏層的權重矩陣。
- \( W_{xh} \)：輸入層到隱藏層的權重矩陣。
- \( W_{hy} \)：隱藏層到輸出層的權重矩陣。
- \( b_h \)：隱藏層的偏置。
- \( b_y \)：輸出層的偏置。
- \( f(\cdot) \)：通常是激活函數，如 \( \tanh \) 或 \( ReLU \)。

#### 2.2 RNN的運作機制

在RNN中，每個時間步的隱藏狀態 \( h_t \) 由當前輸入 \( x_t \) 和前一時間步的隱藏狀態 \( h_{t-1} \) 通過一個非線性函數來計算。因此，RNN的關鍵是通過反向傳遞來捕捉序列中的長期依賴，並且在處理每個時間步時，網絡會維護一個隱藏狀態，這個隱藏狀態包含了從輸入序列中學到的上下文信息。

#### 2.3 RNN的訓練

RNN的訓練通常通過反向傳播算法進行。由於RNN是一種循環結構，因此訓練過程中會利用**反向傳播通過時間（Backpropagation Through Time, BPTT）**來更新參數。BPTT的核心是將反向傳播從每個時間步展開，直到所有時間步的梯度都被計算並更新。

BPTT的計算過程如下：

1. 前向傳播：將輸入序列從 \( x_1 \) 到 \( x_T \) 輸入網絡，計算隱藏層狀態 \( h_1, h_2, ..., h_T \) 和最終的輸出 \( y_1, y_2, ..., y_T \)。
2. 計算損失：基於真實標籤和預測輸出，計算損失函數，通常使用均方誤差或交叉熵等損失函數。
3. 反向傳播：從最終的時間步開始，計算梯度，並將梯度反向傳播到每個時間步。這個過程包括對每一層的權重進行微分，以更新網絡的參數。

#### 2.4 RNN的優缺點

- **優點**：
  - **處理變長序列的能力**：RNN能夠處理變長的輸入序列，並能夠記住歷史信息，因此在自然語言處理、語音識別等任務中表現出色。
  - **長期依賴關係的捕捉**：RNN能夠捕捉序列中前後時間步之間的長期依賴關係，這使得它在一些時間序列預測任務中具優勢。

- **缺點**：
  - **梯度消失與梯度爆炸問題**：由於RNN在訓練過程中需要反向傳播通過時間，這會導致梯度在長序列中逐漸消失或爆炸，從而使得模型難以學習長期依賴。這一問題使得RNN難以在長序列數據上進行有效學習。
  - **計算成本高**：由於RNN的計算依賴於每個時間步的隱藏狀態，這導致它在處理長序列時的計算開銷相對較大，尤其是在大規模數據集上訓練時。

#### 2.5 RNN的變體

為了克服標準RNN的一些問題，特別是梯度消失和梯度爆炸問題，研究者提出了幾種RNN的變體，其中最著名的包括：

- **長短期記憶網絡（LSTM, Long Short-Term Memory）**：LSTM通過引入一個特殊的記憶單元來有效地捕捉長期依賴，並通過門控機制來控制信息的遺忘和更新，從而克服了RNN中的梯度消失問題。
  
- **門控循環單元（GRU, Gated Recurrent Unit）**：GRU是LSTM的簡化版本，也通過門控機制來控制信息流，但相比LSTM結構更簡單，計算開銷較小，且在某些應用中表現相當不錯。

#### 2.6 RNN的應用

RNN在許多應用中都取得了成功，特別是在需要處理序列數據的領域。常見的應用包括：

- **語音識別**：將語音信號作為輸入，通過RNN學習語音到文字的映射。
- **語言建模**：根據先前的單詞預測序列中下文的單詞，應用於文本生成和機器翻譯等任務。
- **機器翻譯**：將源語言的單詞序列轉換為目標語言的單詞序列，RNN可用於這類序列到序列（Seq2Seq）任務。
- **時間序列預測**：RNN可用於股市預測、天氣預測等需要考慮時間依賴關係的問題。

#### 2.7 總結

RNN是一種強大的神經網絡架構，適用於處理序列數據，能夠有效地捕捉時間步之間的依賴關係。儘管RNN在捕捉長期依賴方面存在梯度消失和爆炸的問題，但其變體如LSTM和GRU提供了有效的解決方案，並使得RNN在語言建模、語音識別、機器翻譯等任務中取得了顯著成功。