### 多頭注意力（Multi-Head Attention）

多頭注意力（Multi-Head Attention）是Transformer模型中的一個關鍵組件，它使得模型能夠在不同的表示子空間中同時學習不同的依賴關係。這是自注意力機制的擴展，旨在提高模型捕捉序列中不同位置間複雜關聯的能力。

#### 1. 自注意力回顧

在介紹多頭注意力之前，我們需要先理解自注意力（Self-Attention）機制。自注意力機制的目的是讓模型在處理每個元素時，能夠根據序列中所有其他元素的信息來更新它的表示。這種機制有助於捕捉元素之間的長程依賴關係，特別適用於處理自然語言等序列數據。

自注意力的運作方式如下：
- **查詢（Query）**：序列中的每個元素生成一個查詢向量。
- **鍵（Key）**：每個元素還生成一個鍵向量。
- **值（Value）**：每個元素生成一個值向量。

自注意力的核心操作是計算查詢與所有鍵的相似度，然後根據這些相似度加權平均所有的值向量，從而得到新的表示。

自注意力的公式如下：
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
其中：
- \( Q \) 是查詢矩陣。
- \( K \) 是鍵矩陣。
- \( V \) 是值矩陣。
- \( d_k \) 是鍵向量的維度。

#### 2. 多頭注意力的核心思想

單一的自注意力機制有助於捕捉序列中某一種特定的依賴關係，但在複雜的任務中，我們需要從不同角度捕捉更豐富的信息。這正是多頭注意力的目的，它將多個自注意力機制並行運行，並對結果進行融合。

具體來說，多頭注意力是將查詢、鍵和值分別拆分成多個不同的“頭”，每個頭學習不同的表示子空間。每個頭都會獨立計算一個注意力權重並對值向量進行加權平均。最後，這些頭的結果會被拼接在一起並通過一個線性變換。

#### 3. 多頭注意力的運作過程

假設有 \( h \) 個頭，每個頭都會計算一個自注意力。具體步驟如下：

1. **拆分查詢、鍵和值**：將查詢、鍵和值矩陣拆分成 \( h \) 個子矩陣，每個子矩陣的維度是 \( \frac{d}{h} \)，其中 \( d \) 是查詢、鍵和值的維度，\( h \) 是頭的數量。
   
2. **計算每個頭的注意力**：對每個子矩陣進行自注意力計算，得到每個頭的注意力結果。

3. **拼接頭的結果**：將所有頭的結果按列拼接成一個大的矩陣。

4. **線性變換**：將拼接的結果經過一個線性層，得到最終的注意力輸出。

數學表示如下：
\[
\text{MultiHead}(Q, K, V) = \text{Concat}\left(\text{head}_1, \text{head}_2, \dots, \text{head}_h\right)W^O
\]
其中每個頭的計算公式為：
\[
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\]
- \( W_i^Q, W_i^K, W_i^V \) 是為每個頭學習的查詢、鍵和值的權重矩陣。
- \( W^O \) 是最終線性變換的權重矩陣。

#### 4. 多頭注意力的優點

- **捕捉多種關聯**：通過並行計算多個注意力頭，模型可以從不同的角度捕捉序列中元素之間的關聯。例如，某些頭可能專注於捕捉長程依賴，另一些頭可能專注於捕捉局部結構。
  
- **提高表達能力**：每個頭都有不同的表示子空間，這使得多頭注意力能夠在同一時間內學習到更多的語義信息。這增強了模型的表達能力，特別是在語言處理中。

- **增強模型的靈活性**：多頭注意力使得模型能夠更靈活地處理序列數據中的各種依賴關係，尤其是語言模型需要同時處理語法結構、語義信息和上下文關係。

#### 5. 多頭注意力在Transformer中的應用

在Transformer中，多頭注意力被應用於編碼器和解碼器的每一層。在編碼器中，每個自注意力層（Multi-Head Self Attention）處理輸入序列的各個位置之間的關係；在解碼器中，多頭注意力不僅捕捉目標序列內部的依賴關係，還能夠通過對編碼器輸出的注意力機制捕捉源序列與目標序列之間的關聯。

#### 6. 訓練和推理過程中的效率

- **並行處理**：多頭注意力的設計使得它可以在計算上進行並行化，這對加速訓練過程非常有利。
- **可擴展性**：由於每個頭都處理的是較小的表示子空間，這使得多頭注意力能夠有效處理較長的序列，同時保持較低的計算負擔。

#### 7. 結論

多頭注意力機制是Transformer架構中的關鍵創新之一，它通過並行計算多個自注意力頭，讓模型能夠在多個子空間中同時捕捉序列中元素間的不同關聯。這一特性使得多頭注意力在許多NLP任務中表現優異，並在語言模型、機器翻譯、文本生成等領域取得了顯著成果。