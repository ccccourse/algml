### 信息論基礎 (Fundamentals of Information Theory)

信息論（Information Theory）是一門研究信息的測量、傳遞、儲存和處理的學科。它由克勞德·香農（Claude Shannon）於1948年提出，主要關注的是如何量化信息、編碼與解碼信息，以及在不確定性條件下如何進行有效的通信。

#### 1.1 信息的量度

信息的基本單位是**比特（bit）**，它代表了二進制選擇（例如，"0" 或 "1"）所包含的最小信息量。根據香農的信息量度定義，信息量（或稱熵）是對隨機變數的不確定性的量化。對於一個隨機變數 \( X \) ，其信息量可以用如下公式計算：

\[
H(X) = - \sum_{x \in X} P(x) \log P(x)
\]

其中，\( P(x) \) 是事件 \( x \) 發生的概率，\( H(X) \) 則表示隨機變數 \( X \) 的**熵**，即其信息的不確定性。熵越高，代表信息的來源越不確定。

#### 1.2 互信息

**互信息（Mutual Information）** 衡量的是兩個隨機變數之間的信息共享量，即知道其中一個變數的信息能夠減少對另一個變數的不確定性。互信息的定義為：

\[
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} P(x, y) \log \frac{P(x, y)}{P(x) P(y)}
\]

互信息反映了兩個變數之間的相關性，若兩個變數完全相關，則它們之間的互信息為最大值；若兩個變數獨立，則互信息為零。

#### 1.3 香農熵與信息源編碼

香農熵也可以理解為信息的**最小平均碼長**，即對信息源進行編碼時，使用的最優編碼長度。在信息源編碼中，目的是將信息源的每個符號映射為一個唯一的二進制碼，並且使得編碼後的總長度盡量最短。在這一過程中，香農編碼理論給出了最短碼長的理論界限。

對於一個符號概率分佈 \( P(x_1, x_2, ..., x_n) \)，其最優編碼長度為：

\[
L(x_i) \geq - \log_2 P(x_i)
\]

這表示對每個符號，理論上最短的編碼長度與符號的出現概率呈反比，概率越高的符號，對應的編碼長度越短。

#### 1.4 信息的傳遞與通道容量

信息的傳遞需要一個**通信通道**，而通道的容量（Channel Capacity）表示該通道能夠無誤地傳遞的最大信息量。香農提出了通道容量的定義，並證明了在無噪聲的情況下，信息的最大傳輸速率等於通道的容量。通道容量 \( C \) 由以下公式給出：

\[
C = \max_{P(x)} I(X; Y)
\]

這表示通道的容量與信號的選擇（即發射符號的概率分佈）有關。香農定理指出，若使用合適的編碼方案，信息可以在信道容量以下的速率上進行無誤傳遞。

#### 1.5 香農定理與錯誤率

香農定理（Shannon's Theorem）是信息論的核心理論之一，該定理闡述了在有噪聲的信道中，信息的可靠傳遞是有界限的。香農定理表明，對於一個有噪聲的信道，若信號的傳輸速率低於信道的容量，則錯誤率可以趨近於零；但若信號的傳輸速率高於信道容量，則錯誤率將無限增大。具體來說，若信道的容量為 \( C \)，則無誤傳遞的最大速率為 \( C \)。

#### 1.6 语言模型中的信息论应用

在**語言模型**（Language Models）中，信息論的原理被廣泛應用於建模文本的結構和預測能力。具體來說，語言模型中的**熵**和**互信息**可以用來衡量文本序列的**不確定性**和**詞語之間的關聯性**。語言模型通常通過估算某個詞出現的條件概率來進行預測，這些條件概率的估計涉及大量的信息量度。

例如，給定一個文本序列，語言模型的目標是預測下一個詞 \( w_{t+1} \) 的概率：

\[
P(w_{t+1} | w_1, w_2, ..., w_t)
\]

其中，這個條件概率包含了文本中所有詞語的概率分佈，並且最終的目標是最大化文本的**互信息**，以減少語言模型的**熵**，從而提高預測的準確性。

#### 1.7 總結

信息論為語言模型提供了理論基礎，幫助我們量化和理解語言中的不確定性、詞語之間的相互關聯及其傳遞過程。在語言模型的設計中，使用信息量度（如熵、互信息）來優化模型的預測效果，這不僅是提升模型準確性的一個重要工具，也是理解語言結構的基礎。