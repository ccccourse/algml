### Transformer架構

**Transformer** 是一種基於注意力機制（Attention Mechanism）的神經網絡架構，首次在 2017 年由 Vaswani 等人提出，並成功地改變了自然語言處理（NLP）領域的模型設計，尤其是在機器翻譯任務中。Transformer 的主要特點是完全依賴注意力機制來捕捉序列中的長距離依賴，而不再像傳統的循環神經網絡（RNN）或長短期記憶（LSTM）那樣依賴於循環結構。

### Transformer架構概述

Transformer的核心思想是將序列的每個元素與其他元素之間的關聯通過 **自注意力（Self-Attention）** 機制來建模，這樣使得模型能夠同時關注序列中所有位置的信息。Transformer 的架構分為兩個主要部分：

1. **編碼器（Encoder）**：將源語言序列轉換為一個上下文向量。
2. **解碼器（Decoder）**：根據編碼器的輸出生成目標語言序列。

這些層由多個子層組成，並通過 **多頭注意力機制**（Multi-Head Attention）進行計算。

### Transformer結構詳解

#### 1. 編碼器（Encoder）

編碼器由多層相同的編碼器層組成，每一層包含以下幾個主要組件：

- **多頭自注意力層（Multi-Head Self-Attention）**：通過對輸入進行自注意力操作來計算詞語之間的依賴關係。
- **前饋神經網絡（Feed-Forward Neural Network）**：對每個位置的表示進行非線性變換。
- **層歸一化（Layer Normalization）**：進行正規化以促進訓練。
- **殘差連接（Residual Connection）**：在每層之間添加殘差連接，幫助緩解梯度消失問題。

每個編碼器層的結構如下：
1. **自注意力層**
2. **前饋神經網絡**
3. **殘差連接和層歸一化**

#### 2. 解碼器（Decoder）

解碼器也由多層相同的解碼器層組成，每一層包含以下幾個主要組件：

- **多頭自注意力層**：計算目標語言序列中詞語之間的依賴關係。
- **編碼器-解碼器注意力層（Encoder-Decoder Attention）**：將編碼器的輸出和解碼器的自注意力層結合，從而幫助解碼器生成目標語言的詞語。
- **前饋神經網絡**：對每個位置的表示進行非線性變換。
- **層歸一化**：對每層進行正規化。

每個解碼器層的結構如下：
1. **自注意力層**
2. **編碼器-解碼器注意力層**
3. **前饋神經網絡**
4. **殘差連接和層歸一化**

#### 3. 注意力機制（Attention Mechanism）

注意力機制是 Transformer 的核心，主要目的是根據輸入序列中每個位置的重要性，為其分配不同的權重。Transformer 使用的是 **縮放點積注意力（Scaled Dot-Product Attention）**，其計算方式如下：

- 輸入：查詢（Query）、鍵（Key）、值（Value）
- 計算過程：
  1. 計算查詢和鍵的點積。
  2. 通過縮放係數將點積結果縮放。
  3. 通過 softmax 函數將結果轉換為權重分佈。
  4. 將權重應用到值向量上，得到最終的注意力結果。

公式如下：
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
其中，\( Q \) 是查詢，\( K \) 是鍵，\( V \) 是值，\( d_k \) 是鍵向量的維度。

#### 4. 多頭注意力（Multi-Head Attention）

多頭注意力將查詢、鍵和值分成多個頭進行並行計算，然後將結果拼接起來，這樣可以使模型學習到不同的關注模式。公式如下：
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, \dots, head_h) W^O
\]
其中每個頭的計算是：
\[
head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\]
\( h \) 表示頭的數量，\( W^O \) 是輸出的權重矩陣。

### PyTorch範例：Transformer

以下是使用 **PyTorch** 實現 Transformer 模型的範例：

```python
import torch
import torch.nn as nn

# 定義Transformer模型
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_heads, num_layers, num_classes):
        super(TransformerModel, self).__init__()
        
        # 嵌入層
        self.embedding = nn.Embedding(vocab_size, embed_size)
        
        # Transformer編碼器和解碼器層
        self.transformer = nn.Transformer(
            d_model=embed_size,         # 嵌入維度
            nhead=num_heads,            # 注意力頭數
            num_encoder_layers=num_layers,  # 編碼器層數
            num_decoder_layers=num_layers,  # 解碼器層數
            dim_feedforward=hidden_size,    # 前饋層的隱藏層維度
            dropout=0.1                # Dropout比例
        )
        
        # 預測層
        self.fc_out = nn.Linear(embed_size, num_classes)

    def forward(self, src, tgt):
        src_emb = self.embedding(src)
        tgt_emb = self.embedding(tgt)
        
        # Transformer編碼解碼過程
        output = self.transformer(src_emb, tgt_emb)
        
        # 最後一層的輸出
        output = self.fc_out(output)
        
        return output

# 模型參數設定
vocab_size = 10000
embed_size = 512
hidden_size = 2048
num_heads = 8
num_layers = 6
num_classes = 10000
batch_size = 32
seq_len = 10

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 創建模型
model = TransformerModel(vocab_size, embed_size, hidden_size, num_heads, num_layers, num_classes).to(device)

# 隨機生成的源語言和目標語言序列（僅為演示）
src = torch.randint(0, vocab_size, (seq_len, batch_size)).to(device)
tgt = torch.randint(0, vocab_size, (seq_len, batch_size)).to(device)

# 前向傳遞
output = model(src, tgt)
print("Output shape:", output.shape)
```

### 模型解析：
1. **嵌入層（Embedding Layer）**：將每個詞語映射為固定維度的向量。
2. **Transformer層（Transformer Layer）**：基於注意力機制進行編碼和解碼，計算輸入序列的上下文關係。
3. **預測層（Output Layer）**：將Transformer的輸出映射到詞彙表的大小，用於預測最終的詞語。

### 訓練：
1. **損失函數**：通常使用 **交叉熵損失**（CrossEntropyLoss）來計算每個時間步的詞語生成錯誤。
2. **優化器**：可選擇 **Adam** 或 **SGD** 優化器。
3. **訓練過程**：通過反向傳播和梯度更新來調整模型的權重，從而最小化損失。

### 結論：
Transformer架構通過完全依賴自注意力機制，解決了RNN和LSTM在處理長距離依賴時的問題。多頭注意力和位置編碼的引入使得Transformer可以在多任務學習中表現出色，尤其是在機器翻譯等序列生成任務中，並且對並行處理有很好的支持。