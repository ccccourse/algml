### 多頭注意力（Multi-Head Attention）

多頭注意力是 **Transformer** 模型中的一個核心組件，旨在提升模型捕捉序列中不同語境信息的能力。相比單一注意力機制，多頭注意力允許模型在多個子空間中學習不同的注意力分佈，從而能夠對序列中不同部分的依賴進行更細緻的建模。

#### 基本概念

注意力機制本質上是對輸入序列中的元素進行加權平均，使得模型能夠根據輸入的上下文來聚焦於不同的詞語。在單頭注意力中，這個過程通常是基於一組查詢（Query）、鍵（Key）和值（Value）進行的。對於每一個查詢，注意力機制會計算出對應於所有鍵的權重，然後將這些權重應用到相應的值上，以得到最終的輸出。

而在 **多頭注意力** 中，我們並不是僅僅學習一組查詢、鍵、值對，而是學習多組不同的查詢、鍵和值。這些不同的頭會讓模型能夠在不同的子空間中學習信息，從而捕捉更多樣化的特徵和依賴關係。

#### 多頭注意力的工作原理

1. **線性變換**：
   - 對輸入的查詢（Query）、鍵（Key）和值（Value）進行線性變換，得到多個不同的頭。每個頭的查詢、鍵和值是通過不同的權重矩陣生成的。
   
2. **單頭注意力**：
   - 每個頭在自己的子空間中計算注意力，這是傳統的單頭注意力機制。具體來說，對於每個查詢，計算與所有鍵的匹配程度（通常是點積），然後將其應用到相應的值。

3. **拼接與線性映射**：
   - 每個頭的注意力結果會被拼接起來，然後進行一次線性變換，這樣模型就可以從不同的子空間中學到更多的信息。

#### 整體公式

1. **多頭注意力的計算過程**：
   - 假設我們有 \( h \) 個頭，每個頭的查詢 \( Q_i \)、鍵 \( K_i \)、值 \( V_i \) 都是從原始的輸入 \( Q, K, V \) 經過線性變換得到的。

2. **每個頭的注意力計算**：
   \[
   \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i
   \]
   其中 \( d_k \) 是鍵向量的維度。

3. **拼接並線性變換**：
   所有頭的注意力結果會被拼接成一個向量，然後進行線性映射：
   \[
   \text{MultiHeadAttention}(Q, K, V) = \text{Concat}(\text{Attention}_1, \text{Attention}_2, \dots, \text{Attention}_h) W_O
   \]
   其中 \( W_O \) 是最後的線性變換矩陣。

#### PyTorch 實現多頭注意力

以下是使用 PyTorch 實現多頭注意力的範例：

```python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.embed_size = embed_size
        self.head_dim = embed_size // num_heads
        
        # 確保embed_size能夠被num_heads整除
        assert embed_size % num_heads == 0
        
        # 定義查詢、鍵、值的線性變換
        self.query_linear = nn.Linear(embed_size, embed_size)
        self.key_linear = nn.Linear(embed_size, embed_size)
        self.value_linear = nn.Linear(embed_size, embed_size)
        
        # 最後的線性變換
        self.out_linear = nn.Linear(embed_size, embed_size)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 通過線性變換獲得Q, K, V
        Q = self.query_linear(query)
        K = self.key_linear(key)
        V = self.value_linear(value)
        
        # 拆分為多個頭
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 計算注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # 使用mask
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        attention = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        
        # 拼接多頭的結果
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)
        
        # 最後的線性變換
        output = self.out_linear(output)
        
        return output

# 使用範例
embed_size = 512
num_heads = 8
seq_len = 10
batch_size = 32

# 隨機生成的查詢、鍵、值
query = torch.rand(batch_size, seq_len, embed_size)
key = torch.rand(batch_size, seq_len, embed_size)
value = torch.rand(batch_size, seq_len, embed_size)

# 創建多頭注意力層並計算結果
multihead_attention = MultiHeadAttention(embed_size, num_heads)
output = multihead_attention(query, key, value)

print("Input shape:", query.shape)
print("Output shape:", output.shape)
```

### 多頭注意力的優勢

1. **捕捉不同的語境信息**：
   多頭注意力通過學習多個不同的注意力模式來捕捉序列中不同部分的關聯信息。例如，一個頭可以專注於捕捉局部信息，另一個頭可以關注長程依賴。

2. **並行計算**：
   相較於 RNN 或 LSTM，Transformer 可以更好地進行並行化，因為它不需要依賴先前時間步的輸出。這使得多頭注意力能夠在訓練和推理過程中更高效。

3. **增強表達能力**：
   每個頭都可以專注於學習不同的關聯性，從而使得模型的表達能力更強，能夠捕捉更加細緻的模式。

### 總結

多頭注意力是 Transformer 模型的一個關鍵創新，它使得模型能夠同時從多個不同的角度理解序列數據，從而更有效地捕捉長距離依賴。這種機制增強了模型的表達能力，並且使得 Transformer 在各種序列處理任務中表現出色。