### N-gram模型

N-gram模型是一種基於統計的語言模型，它基於序列中前 \(N-1\) 個詞語來預測當前詞語。這是一種簡單而有效的方法，用於捕捉語言中的局部依賴結構。N-gram模型是自然語言處理中最基礎的語言模型之一，並且廣泛應用於機器翻譯、語音識別等任務。

#### 1. 基本概念

在N-gram模型中，語句被表示為一個詞序列。對於一個N-gram模型，給定 \(N\) 的值，我們預測一個詞語時，只依賴於前 \(N-1\) 個詞語。比如：

- **Unigram（1-gram）**：每個詞語的概率是獨立的，無需考慮前面的詞語。
  \[
  P(w_1, w_2, \dots, w_T) = \prod_{t=1}^{T} P(w_t)
  \]
  
- **Bigram（2-gram）**：每個詞語的概率只依賴於前一個詞語。
  \[
  P(w_1, w_2, \dots, w_T) = P(w_1) \prod_{t=2}^{T} P(w_t | w_{t-1})
  \]
  
- **Trigram（3-gram）**：每個詞語的概率只依賴於前兩個詞語。
  \[
  P(w_1, w_2, \dots, w_T) = P(w_1) P(w_2 | w_1) \prod_{t=3}^{T} P(w_t | w_{t-2}, w_{t-1})
  \]
  
一般而言，對於任意的N-gram模型，我們有：
\[
P(w_1, w_2, \dots, w_T) = \prod_{t=1}^{T} P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1})
\]
這裡的 \(P(w_t | w_{t-N+1}, \dots, w_{t-1})\) 是詞語 \(w_t\) 給定前 \(N-1\) 個詞的條件概率。

#### 2. 訓練N-gram模型

訓練N-gram模型通常需要計算詞語對的出現頻率，並將其轉換為概率。在實際應用中，我們會通過語料庫來估算這些條件概率。對於N-gram模型來說，計算某個詞的條件概率通常是這樣的：

\[
P(w_t | w_{t-1}, \dots, w_{t-N+1}) = \frac{\text{Count}(w_{t-N+1}, \dots, w_{t-1}, w_t)}{\text{Count}(w_{t-N+1}, \dots, w_{t-1})}
\]

其中，`Count` 表示詞語出現的頻次。

#### 3. 平滑技術

由於語料庫有限，某些詞語序列可能在訓練數據中從未出現過，這將導致其概率為零。為了解決這個問題，N-gram模型中常用的平滑技術有：

- **拉普拉斯平滑（Laplace Smoothing）**：通過向所有的計數中加上一個常數 \( \alpha \) 來避免零概率問題：
  \[
  P(w_t | w_{t-1}, \dots, w_{t-N+1}) = \frac{\text{Count}(w_{t-N+1}, \dots, w_{t-1}, w_t) + \alpha}{\text{Count}(w_{t-N+1}, \dots, w_{t-1}) + \alpha V}
  \]
  其中，\(V\) 是詞彙表的大小，\(\alpha\) 是平滑參數。

- **Kneser-Ney平滑**：這是比拉普拉斯平滑更複雜的平滑方法，特別適用於長語言模型。

#### 4. 應用

N-gram模型廣泛應用於各種自然語言處理任務，包括：

- **語音識別**：通過預測下一個詞，幫助語音識別系統選擇最有可能的詞語。
- **機器翻譯**：N-gram模型被用於源語言和目標語言之間的對應翻譯。
- **文本生成**：通過學習語料庫中的詞序列來生成新的文本。

#### 5. Python範例：計算Bigram模型的概率

下面是利用Python來計算Bigram模型的概率：

```python
from collections import defaultdict

# 訓練語料
corpus = ["I am learning NLP", "I love machine learning", "I am learning Python"]

# 創建Bigram模型
bigram_model = defaultdict(lambda: defaultdict(int))
for sentence in corpus:
    words = sentence.split()
    for i in range(1, len(words)):
        bigram_model[words[i-1]][words[i]] += 1

# 計算Bigram的概率
def bigram_probability(word1, word2):
    count_word1 = sum(bigram_model[word1].values())
    count_bigram = bigram_model[word1][word2]
    return count_bigram / count_word1 if count_word1 > 0 else 0

# 測試概率
print(f"P('learning' | 'I'): {bigram_probability('I', 'learning')}")
print(f"P('machine' | 'love'): {bigram_probability('love', 'machine')}")
```

### 解釋：
1. 我們首先構建一個Bigram模型，這是一個字典（`bigram_model`），其中包含每對詞語的出現頻次。
2. 然後，我們定義了`bigram_probability`函數，用來計算一對詞語的條件概率。
3. 透過這樣的計算，我們可以獲得Bigram模型中任意一對詞語的概率。

### 小結

N-gram模型通過捕捉前 \(N-1\) 個詞語的上下文信息來預測當前詞語的概率。儘管N-gram模型在計算和實現上簡單，但其最大的挑戰是對大語料庫的需求和對長期依賴的捕捉不足。雖然如此，這種模型仍然是許多語言處理應用的基礎，並且在深度學習技術發展之前，曾是許多語言模型的主流方法。