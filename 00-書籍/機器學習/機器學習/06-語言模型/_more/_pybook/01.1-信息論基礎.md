### 信息論基礎

信息論（Information Theory）是研究信息的表示、傳輸和處理的一門學科，最早由克勞德·香農（Claude Shannon）於1948年創立。它的基本目標是量化信息的大小、傳遞和儲存效率。信息論在語言模型、數據壓縮、通信理論以及機器學習中扮演著重要角色。

### 基本概念

1. **信息量（Information Content）**：
   信息量衡量的是某個事件發生所帶來的不確定性。通常使用比特（bit）作為測量單位。若事件的概率為 \(P(x)\)，則該事件的信息量（通常稱為自信息）為：
   \[
   I(x) = -\log_2(P(x))
   \]
   其中，\(P(x)\) 是事件 \(x\) 發生的概率。

   - 當 \(P(x)\) 越大時，事件越常見，信息量越小；
   - 當 \(P(x)\) 越小時，事件越罕見，信息量越大。

2. **熵（Entropy）**：
   熵是用來度量一個隨機變量的平均信息量或不確定性的。對於隨機變量 \(X\)，其熵定義為：
   \[
   H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
   \]
   其中，\(P(x_i)\) 是 \(X\) 取值 \(x_i\) 的概率。熵越高，系統的不確定性越大。

3. **條件熵（Conditional Entropy）**：
   條件熵度量的是在知道某些信息的情況下，對另一個變量的不確定性。例如，給定 \(Y\) 的條件下，隨機變量 \(X\) 的熵定義為：
   \[
   H(X|Y) = - \sum_{i=1}^{n} P(x_i, y_j) \log_2 P(x_i | y_j)
   \]
   其中，\(P(x_i | y_j)\) 是 \(Y = y_j\) 給定下的 \(X\) 的條件概率。

4. **互信息（Mutual Information）**：
   互信息度量的是兩個隨機變量之間的相關性或共享信息量。兩個隨機變量 \(X\) 和 \(Y\) 之間的互信息定義為：
   \[
   I(X; Y) = H(X) + H(Y) - H(X, Y)
   \]
   其中，\(H(X, Y)\) 是 \(X\) 和 \(Y\) 的聯合熵。互信息越大，表示 \(X\) 和 \(Y\) 之間的依賴關係越強。

### 語言模型中的信息論應用

語言模型是自然語言處理中的核心，目的是估算一段文本中各個詞語的概率分佈。信息論在語言模型中的應用包括：

1. **語言模型的熵**：
   語言模型的目標之一是最小化給定一組語料庫的熵，即尋找一種方法來描述文本中單詞或符號的分佈，使其更加緊湊和有結構。在語言模型中，熵可以理解為對語言的壓縮能力：熵越低，說明語言中信息的冗餘度越小。

2. **馬可夫假設**：
   在統計語言模型中，最常見的是基於馬可夫假設（Markov Assumption），即假設給定前面的 \(n-1\) 個詞，當前詞的概率只與這些詞有關。這可以用條件熵來表達：
   \[
   P(w_n | w_1, w_2, \dots, w_{n-1}) \approx P(w_n | w_{n-1}, w_{n-2}, \dots, w_1)
   \]
   這種假設簡化了語言模型的訓練和計算，但仍能保留語言中的主要結構。

3. **交叉熵（Cross-Entropy）**：
   在語言模型訓練中，交叉熵常常被用來衡量模型預測分佈和真實分佈之間的差異。交叉熵對於語言模型來說是非常重要的，因為它量化了模型在預測一個詞語時所需的額外比特數：
   \[
   H(p, q) = - \sum_{i} P(x_i) \log q(x_i)
   \]
   其中，\(p(x_i)\) 是真實分佈，\(q(x_i)\) 是模型預測分佈。交叉熵越小，表示模型的預測越準確。

4. **KL散度（Kullback-Leibler Divergence）**：
   KL散度是衡量兩個概率分佈之間差異的指標。對於語言模型來說，KL散度可以衡量模型生成的詞語分佈與真實語料分佈之間的差異，這對模型的優化至關重要。KL散度定義為：
   \[
   D_{\text{KL}}(p || q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}
   \]
   其中，\(P(x_i)\) 是真實分佈，\(Q(x_i)\) 是模型的預測分佈。KL散度越小，意味著模型的預測更接近真實分佈。

### Python範例：計算語言模型的交叉熵

假設我們有一個簡單的詞語概率分佈，並想計算模型預測的交叉熵：

```python
import numpy as np

# 真實分佈 (例如: 4個單詞的詞頻)
p = np.array([0.1, 0.3, 0.4, 0.2])

# 模型預測的分佈
q = np.array([0.2, 0.2, 0.5, 0.1])

# 計算交叉熵
cross_entropy = -np.sum(p * np.log2(q))
print(f"Cross-Entropy: {cross_entropy}")
```

### 解釋：
1. `p` 是實際詞語分佈（真實語料的概率），例如在語料庫中某些詞語的出現頻率。
2. `q` 是模型預測的詞語分佈，即模型根據語言模型預測的詞語出現的機率。
3. `np.sum(p * np.log2(q))` 計算交叉熵，這衡量了模型預測與真實分佈之間的差異。

### 小結

信息論基礎是語言模型的核心，通過熵、交叉熵、KL散度等度量，可以有效評估和優化語言模型的表現。信息量、熵、條件熵等概念在語言建模中具有重要應用，並且在語音識別、機器翻譯等多個自然語言處理領域中扮演著關鍵角色。