以下是使用 PyTorch 實現 GPT 和 Transformer 的簡單範例，幫助理解它們的區別與應用。

### 1. Transformer 範例 (機器翻譯任務)
這裡是基於 Transformer 的一個簡單模型，使用 `nn.Transformer` 來完成機器翻譯任務（編碼-解碼模式）。

```python
import torch
import torch.nn as nn

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, output_size):
        super(TransformerModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.fc_out = nn.Linear(d_model, output_size)
    
    def forward(self, src, tgt):
        src_emb = self.embedding(src)
        tgt_emb = self.embedding(tgt)
        
        # src_emb, tgt_emb 的 shape: [seq_len, batch_size, d_model]
        transformer_out = self.transformer(src_emb, tgt_emb)
        
        # 最後一層的輸出, 進行線性映射
        output = self.fc_out(transformer_out[-1, :, :])
        return output

# 超參數設定
vocab_size = 10000  # 詞彙表大小
d_model = 512  # 嵌入維度
nhead = 8  # 注意力頭數
num_layers = 6  # 層數
output_size = vocab_size  # 輸出大小 (對於機器翻譯是詞彙表大小)

# 模型實例化
model = TransformerModel(vocab_size, d_model, nhead, num_layers, output_size)

# 假設有一個簡單的輸入 (src, tgt) 這裡 seq_len, batch_size 分別為 10 和 32
src = torch.randint(0, vocab_size, (10, 32))  # [seq_len, batch_size]
tgt = torch.randint(0, vocab_size, (10, 32))  # [seq_len, batch_size]

# 預測
output = model(src, tgt)
print(output.shape)  # 應該是 [batch_size, output_size]，即 [32, vocab_size]
```

### 2. GPT 範例 (自回歸語言模型)
GPT 使用的是 Transformer 解碼器部分進行自回歸生成，這裡展示一個簡單的 GPT 模型範例，並使用自回歸進行文本生成。

```python
import torch
import torch.nn as nn

class GPTModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super(GPTModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer_decoder = nn.TransformerDecoder(d_model=d_model, nhead=nhead, num_layers=num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)
        
    def forward(self, tgt, memory):
        tgt_emb = self.embedding(tgt)
        output = self.transformer_decoder(tgt_emb, memory)
        logits = self.fc_out(output)
        return logits

    def generate(self, memory, max_len):
        generated = torch.zeros(1, 1).long().to(memory.device)  # 預設起始 token
        for _ in range(max_len):
            output = self.forward(generated, memory)
            next_token = output.argmax(dim=-1)[:, -1]  # 取最大可能性詞
            generated = torch.cat((generated, next_token.unsqueeze(1)), dim=1)
        return generated

# 超參數設定
vocab_size = 10000  # 詞彙表大小
d_model = 512  # 嵌入維度
nhead = 8  # 注意力頭數
num_layers = 6  # 解碼層數

# 模型實例化
gpt_model = GPTModel(vocab_size, d_model, nhead, num_layers)

# 假設有一個初始的 memory (通常來自於預訓練階段的某個輸入)
memory = torch.randint(0, vocab_size, (10, 1))  # [seq_len, batch_size]

# 使用 GPT 生成文本 (最大生成長度為 20)
generated_text = gpt_model.generate(memory, max_len=20)
print(generated_text.shape)  # [1, 21] 代表生成了 21 個 token (包括初始 token)
```

### 3. GPT 和 Transformer 的差異

- **Transformer** 用於編碼器-解碼器模式，訓練時需要輸入源語言和目標語言的句子，並且能同時處理雙向上下文（如機器翻譯）。
- **GPT** 則是自回歸的生成模型，僅使用 Transformer 解碼器部分，訓練過程中通過預測序列的下一個詞來進行生成，並且在生成時只能依賴先前的上下文。

### 總結

1. **Transformer**：使用了編碼器和解碼器進行雙向上下文建模，適用於機器翻譯等任務。
2. **GPT**：只使用了解碼器進行自回歸生成，適合用於文本生成、語言建模等任務。

以上範例展示了這兩種模型的基礎結構和生成過程。你可以基於這些範例進行擴展，加入更多細節和優化來解決具體的問題。