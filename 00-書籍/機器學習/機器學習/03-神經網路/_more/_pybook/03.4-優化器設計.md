### 優化器設計

**優化器（Optimizer）**是訓練機器學習模型時，根據模型的誤差（損失）來更新權重和偏置的算法。不同的優化器使用不同的策略來選擇學習率和更新規則，以實現更快的收斂和更好的泛化能力。設計一個優化器通常需要考慮以下幾個方面：
1. **學習率（Learning Rate）**：決定每次權重更新的步伐大小。
2. **梯度信息**：如何使用梯度（一階導數）來更新權重。
3. **動量（Momentum）**：使用過去梯度的加權來減少振蕩，提高收斂速度。
4. **自適應學習率**：根據參數更新的歷史來調整學習率。

常見的優化器有 **梯度下降法（GD）**、**隨機梯度下降（SGD）**、**Adam 優化器**等。

#### 1. 自定義優化器設計
這裡將設計一個簡單的 **隨機梯度下降（SGD）優化器** 和 **帶動量的 SGD 優化器**，並進行簡單的訓練。首先，我們來看一個簡單的 SGD 優化器設計。

### 1.1 SGD 優化器設計

```python
import numpy as np

class SGDOptimizer:
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate  # 學習率
        
    def update(self, weights, gradients):
        """
        使用隨機梯度下降更新權重
        weights: 權重矩陣（例如神經網絡中的權重）
        gradients: 損失函數對權重的梯度
        """
        weights -= self.learning_rate * gradients  # 更新權重
        return weights

# 測試 SGD 優化器
def test_sgd():
    # 假設有一個簡單的線性回歸模型 y = wx + b
    # 假設初始權重為 1，學習率為 0.1
    weights = np.array([1.0])  # 初始權重
    learning_rate = 0.1
    optimizer = SGDOptimizer(learning_rate)
    
    # 假設一組簡單的梯度（例如對權重的導數）
    gradients = np.array([0.5])  # 模擬的梯度（簡單情況）
    
    # 使用優化器更新權重
    new_weights = optimizer.update(weights, gradients)
    print(f"Updated weights: {new_weights}")

test_sgd()
```

### 解析：
- **`SGDOptimizer`**：這是一個簡單的 SGD 優化器類，接受一個學習率參數，並在每次更新時根據梯度和學習率來調整權重。
- **`update` 方法**：將權重更新的規則表示為 `weights -= learning_rate * gradients`。
- 我們使用 `test_sgd` 函數來測試 SGD 優化器的行為，並顯示權重更新的結果。

### 1.2 帶動量的 SGD 優化器設計

在 **帶動量的 SGD（Momentum）** 中，我們不僅根據當前的梯度來更新權重，還會根據過去的梯度來加速收斂過程。動量會將之前的梯度加上權重來進行調整。

```python
class MomentumOptimizer:
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.learning_rate = learning_rate  # 學習率
        self.momentum = momentum  # 動量係數
        self.velocity = 0  # 初始動量為零
        
    def update(self, weights, gradients):
        """
        使用帶動量的 SGD 優化器更新權重
        weights: 權重矩陣（例如神經網絡中的權重）
        gradients: 損失函數對權重的梯度
        """
        self.velocity = self.momentum * self.velocity + self.learning_rate * gradients  # 計算動量
        weights -= self.velocity  # 使用動量來更新權重
        return weights

# 測試 Momentum 優化器
def test_momentum():
    # 假設初始權重為 1，學習率為 0.1，動量為 0.9
    weights = np.array([1.0])  # 初始權重
    learning_rate = 0.1
    momentum = 0.9
    optimizer = MomentumOptimizer(learning_rate, momentum)
    
    # 假設一組簡單的梯度（例如對權重的導數）
    gradients = np.array([0.5])  # 模擬的梯度（簡單情況）
    
    # 使用優化器更新權重
    new_weights = optimizer.update(weights, gradients)
    print(f"Updated weights with momentum: {new_weights}")

test_momentum()
```

### 解析：
- **`MomentumOptimizer`**：這是帶有動量的 SGD 優化器。`velocity` 變量用來儲存上一輪的梯度影響，並將這個影響加到當前的權重更新中。這有助於加速在正向方向上的收斂，並減少在平坦區域的振蕩。
- **`update` 方法**：根據當前的梯度和過去的梯度來更新權重，並引入了動量項來加速收斂。

### 1.3 Adam 優化器

**Adam**（Adaptive Moment Estimation）是當前最流行的優化器之一，它結合了動量方法和自適應學習率調整策略。Adam 優化器同時利用一階和二階動量來改進學習過程。

```python
import numpy as np

class AdamOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.learning_rate = learning_rate  # 學習率
        self.beta1 = beta1  # 一階動量
        self.beta2 = beta2  # 二階動量
        self.epsilon = epsilon  # 防止除零錯誤的常數
        self.m = 0  # 一階動量初始化
        self.v = 0  # 二階動量初始化
        self.t = 0  # 訓練步數

    def update(self, weights, gradients):
        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients  # 更新一階動量
        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)  # 更新二階動量

        m_hat = self.m / (1 - self.beta1 ** self.t)  # 偏差修正
        v_hat = self.v / (1 - self.beta2 ** self.t)  # 偏差修正

        weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)  # 更新權重
        return weights

# 測試 Adam 優化器
def test_adam():
    # 假設初始權重為 1，學習率為 0.001，beta1 和 beta2 分別為 0.9 和 0.999
    weights = np.array([1.0])  # 初始權重
    learning_rate = 0.001
    optimizer = AdamOptimizer(learning_rate)
    
    # 假設一組簡單的梯度（例如對權重的導數）
    gradients = np.array([0.5])  # 模擬的梯度（簡單情況）
    
    # 使用優化器更新權重
    new_weights = optimizer.update(weights, gradients)
    print(f"Updated weights with Adam: {new_weights}")

test_adam()
```

### 解析：
- **`AdamOptimizer`**：這是一個實現了 **Adam 優化器** 的類。它使用動量的概念來更新權重，並且通過自適應學習率來減少不穩定性。
- **`update` 方法**：計算一階和二階動量的偏差修正，並根據這些信息來更新權重。

### 小結：
- **SGD 優化器** 采用最簡單的策略，根據當前的梯度來更新權重。
- **帶動量的 SGD** 引入過去的梯度來加速收斂。
- **Adam 優化器** 則結合了動量和自適應學習率，適用於大多數的深度學習任