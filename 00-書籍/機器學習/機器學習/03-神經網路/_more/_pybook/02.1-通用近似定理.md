### 通用近似定理

通用近似定理（Universal Approximation Theorem）表明，多層感知器（MLP）在一定條件下可以逼近任意連續函數。這一定理是神經網路理論的基石，揭示了具有足夠隱藏層神經元的前饋神經網路具備極強的表示能力。

#### 1. 問題設定

通用近似定理可以表述為：

對於任何連續函數 \( f: \mathbb{R}^n \rightarrow \mathbb{R} \) 和任意 \( \epsilon > 0 \)，存在一個包含有限個隱藏神經元的多層感知器 \( g \) 和相應的參數集合 \( \theta \)，使得對於所有輸入 \( x \in \mathbb{R}^n \)：

\[
|f(x) - g(x; \theta)| < \epsilon
\]

換句話說，MLP 可以在任意小的誤差範圍內逼近任何連續函數。

#### 2. 梯度表述

- 單層的 MLP（包含一個隱藏層）的輸出可以表示為：

\[
g(x) = \sum_{j=1}^m v_j \sigma(w_j \cdot x + b_j)
\]

其中 \( m \) 是隱藏層神經元數，\( v_j \)、\( w_j \) 和 \( b_j \) 是權重和偏置，\( \sigma \) 是激活函數（通常選擇 Sigmoid 或 ReLU）。

- 該定理斷言，對於任意連續函數 \( f \) 和任意小的誤差 \( \epsilon \)，存在 \( m \) 個神經元和適當的參數使得上述等式成立。

#### 3. 激活函數的選擇

- Sigmoid 或其他類似的平滑、單調函數通常滿足該定理的要求。
- 使用 ReLU 函數，經過適當的調整，MLP 也能夠逼近任意連續函數。

#### 4. 數學證明

數學證明利用了數學分析和測度論的工具，特別是李普希茨連續函數和分段線性函數的性質：

- 通過將輸入空間分解為多個區域，每個區域內，輸出函數可以被一個簡單函數所逼近。
- 逐步構造每個區域的權重和偏置，使得網路輸出逼近目標函數。

#### 5. 限制和拓展

- 通用近似定理保證了存在這樣的網路，但不提供具體的構造方法或參數學習算法。
- 增加網路深度（多層）能夠顯著提升逼近能力，並減少所需的隱藏神經元數。

### Python 程式範例

以下是一個簡單的 Python 程式，使用 PyTorch 實現 MLP 來逼近一個連續函數（如 \( f(x) = \sin(x) \)）：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 定義 MLP 模型
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.activation = nn.Sigmoid()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        return x

# 創建數據集
x = np.linspace(-2 * np.pi, 2 * np.pi, 100)
y = np.sin(x)
x_train = torch.Tensor(x).unsqueeze(1)
y_train = torch.Tensor(y).unsqueeze(1)

# 初始化模型、損失函數和優化器
model = MLP(1, 10, 1)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 訓練模型
epochs = 1000
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

# 預測和可視化
predicted = model(x_train).detach().numpy()
plt.plot(x, y, label='Original function')
plt.plot(x, predicted, label='MLP approximation')
plt.legend()
plt.show()
```

這個範例展示了如何使用 MLP 來逼近 \( \sin(x) \) 函數，並可視化其逼近效果。