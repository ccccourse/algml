### 批量歸一化（Batch Normalization）

**批量歸一化**是一種對神經網路中的中間層進行正則化的方法。它通過在每個小批量數據中標準化每層的輸入，使得網路的訓練更加穩定和快速，並減少對權重初始化的敏感性。

#### 1. 批量歸一化的基本概念

批量歸一化的主要目的是將每層輸入的數據分佈轉化為零均值和單位方差，從而控制輸出範圍，穩定梯度流。

- **標準化**：對每個小批量的輸入 \( x \) 計算其均值 \( \mu_B \) 和方差 \( \sigma_B^2 \)，然後進行標準化：
  \[
  \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
  \]
  其中 \( \epsilon \) 是一個小值，用於避免除以零的情況。

- **縮放和平移**：為了保留模型的表現能力，批量歸一化引入了兩個可學習的參數 \( \gamma \) 和 \( \beta \)，對標準化後的輸入進行縮放和平移：
  \[
  y = \gamma \hat{x} + \beta
  \]
  這樣可以確保網路仍然能夠學習到任意的輸出分佈。

#### 2. 批量歸一化的優點

- **加速訓練**：通過減少內部協變移動（Internal Covariate Shift），使得網路的收斂速度加快。
- **提高穩定性**：減少對學習率和初始權重的敏感性，使得網路訓練更穩定。
- **減少過擬合**：批量歸一化具有一定的正則化效果，可以減少對其他正則化技術的依賴。

#### 3. Python 程式範例

下面是一個使用 PyTorch 的例子，展示如何在神經網路中使用批量歸一化層：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定義一個簡單的神經網路，加入批量歸一化層
class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x

# 創建網路和優化器
input_dim = 10
hidden_dim = 50
output_dim = 1
model = SimpleNN(input_dim, hidden_dim, output_dim)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 隨機生成一些數據來演示
x = torch.randn(100, input_dim)
y = torch.randn(100, output_dim)

# 訓練模型
criterion = nn.MSELoss()
for epoch in range(100):
    optimizer.zero_grad()
    outputs = model(x)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

在這個範例中，我們在每個全連接層後面加入了批量歸一化層 `nn.BatchNorm1d`。這有助於穩定網路的訓練過程，並加速收斂。