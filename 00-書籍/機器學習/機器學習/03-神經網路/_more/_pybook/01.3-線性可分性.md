### 線性可分性

**線性可分性**是指一組數據點可以通過一條直線（在二維情況下）、一個平面（在三維情況下）或一個超平面（在更高維度中）完全分開成兩個不同的類別。換句話說，若存在一條超平面能夠將兩類數據完全區分開來，則稱這組數據是線性可分的。

#### 數學表述

給定一個數據集 \( \{(x_i, y_i)\}_{i=1}^n \)，其中 \( x_i \in \mathbb{R}^d \) 是特徵向量， \( y_i \in \{-1, +1\} \) 是類別標籤。這個數據集是線性可分的，若存在一個權重向量 \( w \in \mathbb{R}^d \) 和一個偏置 \( b \in \mathbb{R} \)，使得對於所有的 \( i \)：

\[
y_i (w^T x_i + b) > 0
\]

即，所有標籤為 +1 的數據點在超平面 \( w^T x + b = 0 \) 的一側，所有標籤為 -1 的數據點在另一側。

#### 幾何解釋

- 在二維空間中，超平面是一條直線。
- 在三維空間中，超平面是一個平面。
- 在更高維空間中，超平面是多維空間中的一個分離界面。

線性可分性是許多線性分類算法（如感知器和支持向量機）的基本假設。

### 感知器算法示例

感知器算法是一種簡單的線性分類器，用於線性可分數據。它通過調整權重向量 \( w \) 和偏置 \( b \) 來找到能夠分離數據的超平面。

```python
import numpy as np

def perceptron(X, y, learning_rate=1.0, max_iter=1000):
    n_samples, n_features = X.shape
    w = np.zeros(n_features)
    b = 0
    
    for _ in range(max_iter):
        for i in range(n_samples):
            if y[i] * (np.dot(w, X[i]) + b) <= 0:
                w += learning_rate * y[i] * X[i]
                b += learning_rate * y[i]
    
    return w, b

# 示例數據
X = np.array([[2, 3], [1, 1], [2, 1], [4, 5], [5, 5], [4, 4]])
y = np.array([1, 1, 1, -1, -1, -1])

# 訓練感知器
w, b = perceptron(X, y)
print("權重:", w)
print("偏置:", b)

# 繪圖顯示結果
import matplotlib.pyplot as plt

# 畫出數據點
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')
x_boundary = np.linspace(0, 6, 100)
y_boundary = -(w[0] * x_boundary + b) / w[1]
plt.plot(x_boundary, y_boundary, 'k-')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Perceptron Linear Separation')
plt.show()
```

此範例使用感知器算法訓練一個線性分類器來分離簡單的二維線性可分數據，並繪製出分離超平面。