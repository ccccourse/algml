### 權重初始化

**權重初始化**是神經網路訓練中的一個關鍵步驟，對模型的收斂速度和最終性能有重大影響。良好的權重初始化能夠避免梯度消失或爆炸問題，並加速訓練過程。

#### 1. 權重初始化的重要性

- **避免梯度消失或爆炸**：如果權重初始化不當，網路的輸出可能會飽和，導致梯度消失或爆炸，從而使得反向傳播無法有效進行。

- **加速收斂**：良好的初始化能夠使網路快速找到合適的梯度方向，加速訓練過程。

- **穩定訓練**：適當的初始化能夠使訓練過程更加穩定，減少振蕩和收斂到次優解的可能性。

#### 2. 常見的權重初始化方法

- **零初始化**：將所有權重初始化為零。這種方法會導致所有神經元在每次更新中都執行相同的操作，從而喪失網路的學習能力，因此通常不使用。

- **隨機初始化**：從均勻分佈或高斯分佈中隨機生成初始權重。這是最基本的初始化方法，但需要對分佈的範圍進行仔細選擇。

- **Xavier 初始化**：將權重初始化為一個均勻或高斯分佈，分佈的方差根據輸入和輸出的單元數量進行調整。這種方法適用於激活函數為 Sigmoid 或 Tanh 的網路。

- **He 初始化**：針對 ReLU 和其變體設計的初始化方法，權重的方差根據輸入單元數量進行調整，能夠更好地處理梯度爆炸問題。

#### 3. Python 程式範例

下面是一個使用 PyTorch 的例子，演示如何使用不同的初始化方法來初始化神經網路的權重：

```python
import torch
import torch.nn as nn

# 定義一個簡單的神經網路
class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 初始化函數
def initialize_weights(m):
    if isinstance(m, nn.Linear):
        # Xavier 初始化
        nn.init.xavier_uniform_(m.weight)
        # He 初始化
        # nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
        m.bias.data.fill_(0.01)

# 創建網路並初始化權重
input_dim = 10
hidden_dim = 50
output_dim = 1
model = SimpleNN(input_dim, hidden_dim, output_dim)

# 使用初始化函數初始化權重
model.apply(initialize_weights)

# 打印初始化後的權重
for name, param in model.named_parameters():
    if param.requires_grad:
        print(f'{name}: {param.data}')
```

此範例中，我們定義了一個簡單的三層神經網路，並使用 `initialize_weights` 函數對網路的權重進行初始化。在函數中，可以選擇使用 Xavier 初始化或 He 初始化來初始化網路的權重。