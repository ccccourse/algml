### 感知器收斂性證明

感知器算法的收斂性證明是基於以下條件：數據集是線性可分的，並且使用的是固定的學習率。

#### 1. 問題設定

給定一個線性可分的數據集 \( \{(x_i, y_i)\}_{i=1}^n \)，其中 \( x_i \in \mathbb{R}^d \) 是特徵向量，\( y_i \in \{-1, +1\} \) 是類別標籤，存在一個權重向量 \( w^* \in \mathbb{R}^d \) 和一個偏置 \( b^* \in \mathbb{R} \)，使得對於所有 \( i \)：

\[
y_i (w^* \cdot x_i + b^*) > 0
\]

#### 2. 目標

證明感知器算法在有限次迭代後會收斂，並找到一個能夠正確分類所有樣本的權重向量 \( w \) 和偏置 \( b \)。

#### 3. 證明步驟

##### 步驟 1: 定義函數間隔

定義函數間隔 \( \gamma \) 為：

\[
\gamma = \min_i y_i (w^* \cdot x_i + b^*) > 0
\]

這表示每個樣本到超平面的最小距離。

##### 步驟 2: 定義最大範數

定義所有特徵向量的最大範數 \( R \) 為：

\[
R = \max_i \|x_i\|
\]

這表示樣本中最大向量的長度。

##### 步驟 3: 演算法收斂性

在每次迭代中，感知器算法更新規則為：

\[
w^{(t+1)} = w^{(t)} + y_i x_i \quad \text{如果 } y_i (w^{(t)} \cdot x_i + b^{(t)}) \leq 0
\]

我們需要證明，經過有限次迭代，算法將找到一個超平面正確分類所有樣本。

##### 步驟 4: 歸納不等式

首先，我們知道每次更新會使 \( w^{(t)} \cdot w^* \) 增大：

\[
w^{(t+1)} \cdot w^* = w^{(t)} \cdot w^* + y_i x_i \cdot w^* \geq w^{(t)} \cdot w^* + \gamma
\]

因此，在 \( t \) 次更新後：

\[
w^{(t)} \cdot w^* \geq t \gamma
\]

##### 步驟 5: 權重向量增長的上界

每次更新後，權重向量的範數增長如下：

\[
\|w^{(t+1)}\|^2 = \|w^{(t)}\|^2 + \|y_i x_i\|^2 + 2 y_i x_i \cdot w^{(t)} \leq \|w^{(t)}\|^2 + R^2
\]

因此，經過 \( t \) 次更新後：

\[
\|w^{(t)}\|^2 \leq t R^2
\]

##### 步驟 6: 兩個不等式結合

將這兩個不等式結合，我們得到：

\[
\|w^{(t)}\| \geq \frac{t \gamma}{\|w^*\|}
\]

和

\[
\|w^{(t)}\| \leq \sqrt{t} R
\]

從這兩個不等式中，我們得到：

\[
\frac{t \gamma}{\|w^*\|} \leq \sqrt{t} R
\]

平方兩邊並簡化得：

\[
t \leq \frac{R^2 \|w^*\|^2}{\gamma^2}
\]

這表明算法最多需要 \( \frac{R^2 \|w^*\|^2}{\gamma^2} \) 次更新就能找到一個正確分類所有樣本的超平面。

### 結論

感知器算法在數據集線性可分的條件下，必然在有限次迭代後收斂，找到一個能夠正確分類所有樣本的超平面。這就是感知器收斂性定理的數學證明。