### 激活函數分析

**激活函數**（Activation Function）是神經網路中每個神經元的重要組成部分，決定了輸入信號如何轉換為輸出信號。激活函數引入了非線性因素，使神經網路能夠處理複雜的非線性問題。下面是一些常見的激活函數及其分析。

#### 1. 階躍函數（Step Function）

階躍函數是感知器中使用的激活函數，定義為：
\[
f(z) = 
\begin{cases} 
1, & \text{if } z \geq 0 \\
0, & \text{if } z < 0
\end{cases}
\]
**優點**：
- 簡單易於實現。
  
**缺點**：
- 無法處理非線性問題。
- 對輸入的微小變化不敏感，導致梯度消失。

#### 2. Sigmoid函數

Sigmoid函數是一種S形曲線，定義為：
\[
f(z) = \frac{1}{1 + e^{-z}}
\]
**優點**：
- 平滑可導。
- 輸出範圍在 \( (0, 1) \) 之間，適合處理概率問題。

**缺點**：
- 梯度消失問題：輸入的極大或極小值會導致梯度接近0，影響模型學習效率。
- 計算成本較高，特別是指數運算。

#### 3. Tanh函數

Tanh函數是另一種S形曲線，定義為：
\[
f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]
**優點**：
- 平滑可導。
- 輸出範圍在 \( (-1, 1) \) 之間，中心對稱，適合處理零均值數據。

**缺點**：
- 同樣存在梯度消失問題。

#### 4. ReLU（Rectified Linear Unit）

ReLU函數是當前深度學習中最常用的激活函數，定義為：
\[
f(z) = \max(0, z)
\]
**優點**：
- 計算簡單，收斂速度快。
- 線性且非飽和，減少梯度消失問題。

**缺點**：
- 當輸入為負數時，梯度為0，可能導致“神經元死亡”問題。

#### 5. Leaky ReLU

Leaky ReLU是ReLU的一種變體，允許輸入為負時有一個小的斜率，定義為：
\[
f(z) = 
\begin{cases} 
z, & \text{if } z \geq 0 \\
\alpha z, & \text{if } z < 0
\end{cases}
\]
其中 \( \alpha \) 是一個小的常數。

**優點**：
- 解決ReLU的“神經元死亡”問題。

**缺點**：
- 需要手動調整 \( \alpha \) 參數。

#### 6. Softmax函數

Softmax函數通常用於多分類問題的輸出層，將輸入轉換為概率分佈，定義為：
\[
f(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]
**優點**：
- 將輸出轉換為概率分佈。

**缺點**：
- 易受輸入值範圍的影響，可能導致數值不穩定。

### 激活函數選擇指南

- **線性可分問題**：可以使用階躍函數或Sigmoid函數。
- **多層神經網路**：通常使用ReLU或其變體（如Leaky ReLU）來減少梯度消失問題。
- **輸出為概率**：使用Sigmoid（適合二分類）或Softmax（適合多分類）函數。

每個激活函數都有其特定的應用場景和局限性，選擇時應根據具體問題的需求進行調整。

### Python範例

使用`numpy`實現一些常見的激活函數：

```python
import numpy as np

def step_function(z):
    return np.where(z >= 0, 1, 0)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def tanh(z):
    return np.tanh(z)

def relu(z):
    return np.maximum(0, z)

def leaky_relu(z, alpha=0.01):
    return np.where(z >= 0, z, alpha * z)

def softmax(z):
    exps = np.exp(z - np.max(z))
    return exps / np.sum(exps, axis=0)

# 測試示例
z = np.array([-1.0, 0.0, 1.0])
print("Step Function:", step_function(z))
print("Sigmoid:", sigmoid(z))
print("Tanh:", tanh(z))
print("ReLU:", relu(z))
print("Leaky ReLU:", leaky_relu(z))
print("Softmax:", softmax(z))
```

此範例展示了如何使用Python實現和測試這些激活函數，便於理解其輸出行為。