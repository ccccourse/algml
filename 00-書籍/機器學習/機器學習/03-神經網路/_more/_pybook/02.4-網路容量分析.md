### 網路容量分析

**網路容量分析**是評估神經網路模型學習和表示能力的一個重要方面。網路容量通常指模型能夠擬合不同複雜度數據的能力。

#### 1. 網路容量的概念

- **容量（Capacity）**：表示神經網路能夠學習和表示多樣性函數的能力。容量越高，網路能夠擬合越複雜的函數或模式。

- **偏差-方差權衡（Bias-Variance Tradeoff）**：網路容量與偏差和方差有關。低容量網路（高偏差）可能會欠擬合數據，而高容量網路（高方差）可能會過擬合數據。

#### 2. 網路容量的度量方法

- **參數數量**：模型的容量通常與其參數的數量相關。參數越多，容量越大，網路能夠表示的函數也越複雜。
  
- **VC 維度（Vapnik-Chervonenkis Dimension）**：VC 維度是一個統計學的概念，用來度量模型的複雜度。它表示模型能夠在多大範圍內正確分類樣本。

- **Rademacher 複雜度**：這是一種測量模型能力的度量，表示模型對隨機標籤數據的擬合能力。

#### 3. 網路容量分析的重要性

- **避免過擬合**：容量過大的網路可能會記住訓練數據的每一個細節，導致在新數據上的表現不佳（過擬合）。

- **避免欠擬合**：容量過小的網路可能無法學習數據的基本模式，導致欠擬合。

- **正則化技術**：為了控制網路容量，可以使用正則化技術，如 L1 或 L2 正則化，或通過網路結構的選擇（如使用較小的網路）來限制容量。

#### 4. Python 程式範例

下面是一個簡單的 Python 程式，演示如何通過調整神經網路的層數和每層的神經元數量來影響網路的容量：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 生成回歸數據集
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 將數據轉換為張量
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# 定義一個簡單的神經網路
class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 定義模型、損失函數和優化器
hidden_dim = 100  # 調整這個值來改變網路的容量
model = SimpleNN(input_dim=10, hidden_dim=hidden_dim, output_dim=1)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 訓練過程
for epoch in range(100):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}, Loss: {loss.item()}')

# 測試過程
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, y_test)
    print(f'Test Loss: {test_loss.item()}')
```

此範例中，可以通過調整 `hidden_dim` 的值來改變網路的容量，從而觀察不同容量對訓練和測試損失的影響。較大的 `hidden_dim` 值會增加網路容量，使其能夠擬合更多樣化的數據，但也可能導致過擬合。