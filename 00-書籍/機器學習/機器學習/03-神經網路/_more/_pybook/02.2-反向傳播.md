### 反向傳播（Backpropagation）

反向傳播是神經網路中用於訓練模型的核心算法，用來計算每個參數的梯度，進而通過梯度下降法優化參數。該算法基於鏈式法則，逐層傳播誤差，更新網路的權重和偏置。

#### 1. 反向傳播的基本原理

反向傳播的主要目標是最小化損失函數 \( L \)：

\[
L = \frac{1}{n} \sum_{i=1}^{n} \ell(y_i, \hat{y}_i)
\]

其中 \( \ell \) 是單樣本的損失函數，\( y_i \) 是真實標籤，\( \hat{y}_i \) 是預測值，\( n \) 是樣本數。

- **前向傳播**：從輸入層到輸出層，計算每一層的輸出。
- **損失計算**：計算預測值與真實值之間的損失。
- **反向傳播**：從輸出層開始，計算損失相對於每一層參數的梯度。
- **參數更新**：根據計算出的梯度，使用優化算法（如梯度下降）更新網路參數。

#### 2. 反向傳播的數學公式

對於具有 \( L \) 層的神經網路，假設第 \( l \) 層的輸出為 \( a^l \)，權重為 \( W^l \)，偏置為 \( b^l \)，激活函數為 \( \sigma \)，則：

- 前向傳播公式：

\[
z^l = W^l a^{l-1} + b^l, \quad a^l = \sigma(z^l)
\]

- 反向傳播計算每層的梯度：

\[
\delta^L = \nabla_a \ell \cdot \sigma'(z^L)
\]

\[
\delta^l = ((W^{l+1})^T \delta^{l+1}) \cdot \sigma'(z^l)
\]

\[
\frac{\partial L}{\partial W^l} = \delta^l (a^{l-1})^T, \quad \frac{\partial L}{\partial b^l} = \delta^l
\]

其中 \( \delta^l \) 是第 \( l \) 層的誤差項，表示該層的輸出對最終損失的影響。

#### 3. Python 程式範例

以下是一個使用 PyTorch 手動實現反向傳播的簡單範例，演示如何訓練一個兩層的神經網路來逼近一個簡單函數：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 定義簡單的兩層神經網路
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.activation = nn.Sigmoid()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        return x

# 創建數據集
x = np.linspace(-2 * np.pi, 2 * np.pi, 100)
y = np.sin(x)
x_train = torch.Tensor(x).unsqueeze(1)
y_train = torch.Tensor(y).unsqueeze(1)

# 初始化模型、損失函數和優化器
model = SimpleNN(1, 10, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 手動實現反向傳播
epochs = 1000
for epoch in range(epochs):
    optimizer.zero_grad()
    
    # 前向傳播
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    
    # 反向傳播
    loss.backward()
    optimizer.step()
    
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

# 預測和可視化
predicted = model(x_train).detach().numpy()
plt.plot(x, y, label='Original function')
plt.plot(x, predicted, label='NN approximation')
plt.legend()
plt.show()
```

此範例中，`loss.backward()` 計算損失函數相對於每個參數的梯度，`optimizer.step()` 則根據計算出的梯度更新模型參數。這展示了反向傳播在模型訓練中的關鍵角色。