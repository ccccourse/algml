### 正則化方法

**正則化**是為了防止機器學習模型過擬合（overfitting）而採取的技術。它通過向損失函數中加入額外的項來約束模型的學習，使得模型不會過度複雜化，從而提高其在新數據上的泛化能力。

常見的正則化方法包括**L1 正則化**、**L2 正則化**、**Dropout** 和 **早停法**等。

#### 1. L1 正則化（Lasso）
L1 正則化是向損失函數中加入模型權重絕對值的總和。這樣做會鼓勵權重變得稀疏，即部分權重趨近於零，從而實現特徵選擇。

損失函數：
\[
J(\theta) = \text{Loss}(X, Y, \theta) + \lambda \sum_{i=1}^{n} |\theta_i|
\]
其中，\( \lambda \) 是正則化強度的超參數，\( \theta \) 是模型的權重。

#### 2. L2 正則化（Ridge）
L2 正則化是向損失函數中加入權重的平方和。這會鼓勵模型選擇較小的權重，從而減少過擬合的風險。

損失函數：
\[
J(\theta) = \text{Loss}(X, Y, \theta) + \lambda \sum_{i=1}^{n} \theta_i^2
\]
其中，\( \lambda \) 是正則化強度的超參數，\( \theta \) 是模型的權重。

#### 3. Dropout
Dropout 是一種隨機正則化方法，它在每次訓練時隨機丟棄一部分神經元的輸出，這樣可以防止模型依賴某些特定的特徵，從而提高泛化能力。這種方法特別適用於深度神經網絡。

#### 4. 早停法（Early Stopping）
早停法是一種基於驗證集的正則化方法。在訓練過程中，如果發現模型在驗證集上的性能停止提升，則提前終止訓練。這有助於防止模型在訓練集上過擬合。

### Python 範例：L2 正則化

下面是一個使用 **L2 正則化**（也稱為 Ridge 正則化）的簡單線性回歸模型範例，使用了 **scikit-learn** 套件：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 創建一個簡單的線性回歸數據集
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # 特徵
y = 2 * X + 1 + np.random.randn(100, 1) * 2  # 標籤，加上噪聲

# 將數據集劃分為訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定義一個帶有 L2 正則化的 Ridge 回歸模型
ridge_model = Ridge(alpha=1.0)  # alpha 是正則化強度（λ）

# 訓練模型
ridge_model.fit(X_train, y_train)

# 預測
y_pred = ridge_model.predict(X_test)

# 計算均方誤差
mse = mean_squared_error(y_test, y_pred)

# 輸出模型參數和預測誤差
print(f'係數：{ridge_model.coef_}')
print(f'截距：{ridge_model.intercept_}')
print(f'均方誤差：{mse:.4f}')

# 可視化結果
plt.scatter(X_test, y_test, color='blue', label='真實值')
plt.plot(X_test, y_pred, color='red', label='預測值')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Ridge Regression with L2 Regularization')
plt.legend()
plt.show()
```

#### 解析：
1. **數據生成**：我們創建了一個簡單的線性回歸數據集，`X` 為特徵，`y` 為標籤。
2. **模型訓練**：我們使用 `Ridge` 類來創建一個 L2 正則化的線性回歸模型，並用訓練集來訓練它。`alpha` 參數控制正則化的強度。
3. **預測與評估**：使用測試集來進行預測，並計算均方誤差（MSE）來評估模型的性能。
4. **可視化**：繪製了預測結果與真實值的對比圖。

#### 5. 其他正則化方法：
- **Dropout**（深度學習中常用）：
  ```python
  import torch
  import torch.nn as nn
  
  class SimpleNN(nn.Module):
      def __init__(self, input_dim, hidden_dim, output_dim):
          super(SimpleNN, self).__init__()
          self.fc1 = nn.Linear(input_dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, output_dim)
          self.dropout = nn.Dropout(p=0.5)  # 設定丟棄概率為 50%
  
      def forward(self, x):
          x = torch.relu(self.fc1(x))
          x = self.dropout(x)  # 在隱藏層後加入 Dropout
          x = self.fc2(x)
          return x
  ```

- **早停法**（使用 Keras）：
  ```python
  from keras.models import Sequential
  from keras.layers import Dense
  from keras.callbacks import EarlyStopping
  
  # 定義模型
  model = Sequential()
  model.add(Dense(64, input_dim=8, activation='relu'))
  model.add(Dense(1, activation='linear'))
  
  # 設置早停回調
  early_stopping = EarlyStopping(monitor='val_loss', patience=5)
  
  # 編譯模型
  model.compile(optimizer='adam', loss='mse')
  
  # 訓練模型並啟用早停
  model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])
  ```