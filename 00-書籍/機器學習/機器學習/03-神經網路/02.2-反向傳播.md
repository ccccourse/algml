### 反向傳播（Backpropagation）

反向傳播（Backpropagation）是訓練多層神經網絡中的一種高效的算法，它通過計算損失函數關於網絡參數（權重和偏置）的梯度來實現權重更新。這一算法是深度學習中最關鍵的部分，實現了網絡的自動微分和高效的學習過程。

反向傳播算法的核心思想是利用鏈式法則，將誤差從輸出層傳遞回每一層，並根據這些誤差來調整網絡中的參數。它是梯度下降法的一種實現方式，透過逐層計算梯度來更新網絡的權重。

#### 1. 基本步驟

反向傳播算法包括兩個主要階段：前向傳播和反向傳播。

- **前向傳播（Forward Propagation）**：
  在前向傳播過程中，輸入數據從輸入層開始，依次通過每一層，直到到達輸出層。在每一層中，神經元將來自前一層的輸入與權重相乘，並加上偏置，然後通過激活函數進行非線性變換。最終，網絡輸出被計算出來。

  具體而言，假設對於第 \( l \) 層的神經元，我們有：
  \[
  z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}
  \]
  \[
  a^{(l)} = \sigma(z^{(l)})
  \]
  其中，\( W^{(l)} \) 是第 \( l \) 層的權重矩陣，\( a^{(l-1)} \) 是前一層的輸出，\( b^{(l)} \) 是第 \( l \) 層的偏置，\( \sigma \) 是激活函數。

- **反向傳播（Backward Propagation）**：
  在反向傳播階段，目標是計算每一層權重和偏置的梯度，並利用這些梯度來更新網絡參數。這一過程涉及反向傳遞誤差，從輸出層開始，逐層傳遞回來。

  假設損失函數為 \( L \)，我們首先需要計算損失函數相對於最終層輸出的梯度 \( \frac{\partial L}{\partial a^{(L)}} \)。接著，通過鏈式法則計算每一層的誤差，並將其反向傳播回去。

  具體步驟如下：

  - **計算輸出層誤差**：
    在輸出層，誤差的計算基於損失函數對輸出激活的偏導數：
    \[
    \delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \cdot \sigma'(z^{(L)})
    \]
    其中，\( \delta^{(L)} \) 是第 \( L \) 層的誤差，\( \sigma'(z^{(L)}) \) 是第 \( L \) 層激活函數對輸入的導數。

  - **計算隱藏層誤差**：
    對於隱藏層，誤差通過反向傳播逐層計算。假設第 \( l \) 層的誤差 \( \delta^{(l)} \) 是由上一層誤差傳遞過來的：
    \[
    \delta^{(l)} = \left( W^{(l+1)} \right)^T \delta^{(l+1)} \cdot \sigma'(z^{(l)})
    \]
    其中，\( \left( W^{(l+1)} \right)^T \) 是第 \( l+1 \) 層的轉置權重矩陣，\( \delta^{(l+1)} \) 是第 \( l+1 \) 層的誤差，\( \sigma'(z^{(l)}) \) 是第 \( l \) 層激活函數對輸入的導數。

  - **計算梯度**：
    一旦誤差計算完成，我們可以計算每一層的權重和偏置的梯度。對於第 \( l \) 層，權重和偏置的梯度分別為：
    \[
    \frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T
    \]
    \[
    \frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}
    \]
    這些梯度指示了如何根據誤差來調整權重和偏置，以最小化損失函數。

#### 2. 反向傳播的數學推導

假設我們有一個多層感知器，包含 \( L \) 層，其中 \( W^{(l)} \) 和 \( b^{(l)} \) 分別是第 \( l \) 層的權重和偏置，\( a^{(l)} \) 是第 \( l \) 層的激活值，\( z^{(l)} \) 是該層的總輸入，損失函數為 \( L \)。我們通過以下步驟來推導反向傳播的公式。

- **前向傳播過程**：
  在每一層，我們有：
  \[
  z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}
  \]
  \[
  a^{(l)} = \sigma(z^{(l)})
  \]
  其中，\( \sigma \) 是激活函數，\( a^{(0)} \) 是輸入層的輸入數據。

- **反向傳播過程**：
  1. 計算輸出層的誤差：
     \[
     \delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \cdot \sigma'(z^{(L)})
     \]
  2. 計算隱藏層的誤差，逐層反向傳播：
     \[
     \delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \cdot \sigma'(z^{(l)})
     \]
  3. 計算梯度，更新權重和偏置：
     \[
     \frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T
     \]
     \[
     \frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}
     \]

#### 3. 反向傳播的優化

反向傳播算法通常與梯度下降法結合使用來優化神經網絡的參數。在每一次迭代中，通過計算梯度，並使用梯度下降來更新權重和偏置，最小化損失函數。具體的權重更新規則為：
\[
W^{(l)} := W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}
\]
\[
b^{(l)} := b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}
\]
其中，\( \eta \) 是學習率。

#### 4. 結論

反向傳播是一個高效且強大的算法，通過對損失函數的梯度計算，允許我們利用梯度下降法來訓練深層神經網絡。它利用鏈式法則逐層計算誤差，並通過權重和偏置的調整，使得網絡逐步學習到有效的特徵表示。反向傳播是深度學習成功的基礎之一。