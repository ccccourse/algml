### 變分自編碼器（Variational Autoencoder, VAE）

變分自編碼器（VAE）是一種生成模型，它結合了自編碼器和概率推斷的概念，能夠學習數據的潛在結構並生成新數據。與傳統的自編碼器不同，VAE 在編碼過程中對隱變量進行概率建模，並通過變分推斷來最大化對數似然，從而進行生成性建模。

### 1. VAE 的結構

VAE 模型由以下幾個組件構成：

- **編碼器（Encoder）**：將輸入數據映射到潛在空間的概率分佈，輸出隱變量的均值（\( \mu \)）和對數方差（\( \log \sigma^2 \)）。
  
- **潛在變量（Latent Variable）**：這些隱變量來自於編碼器輸出的分佈，通常是高斯分佈，並且會進行重參數化（reparameterization trick）以實現反向傳播。

- **解碼器（Decoder）**：將潛在變量映射回原始數據空間，進行數據的重建。

### 2. VAE 的損失函數

VAE 的損失函數包含兩部分：重建誤差和 KL 散度。

#### (1) 重建誤差（Reconstruction Loss）

重建誤差衡量的是模型從隱變量生成的數據與原始數據之間的差異，這部分通常使用均方誤差（MSE）或二元交叉熵（BCE）來衡量。

\[
L_{\text{reconstruction}} = - \mathbb{E}_{q(z|x)}[\log p(x|z)]
\]

#### (2) KL 散度（KL Divergence）

KL 散度是測量編碼器的後驗分佈（\( q(z|x) \)）與先驗分佈（通常是標準正態分佈 \( p(z) \)）之間的差異。KL 散度強制學習到的潛在變量分佈接近於先驗分佈。

\[
L_{\text{KL}} = D_{\text{KL}}(q(z|x) \parallel p(z))
\]

#### (3) 總損失

VAE 的總損失函數是重建誤差和 KL 散度的加權和：

\[
L_{\text{VAE}} = L_{\text{reconstruction}} + L_{\text{KL}}
\]

### 3. VAE 的推斷過程

在訓練過程中，VAE 使用變分推斷來近似後驗分佈，並對隱變量進行抽樣。由於這個過程中涉及隨機性，VAE 引入了重參數化技巧（reparameterization trick），使得從分佈中抽樣可以進行梯度下降。

具體而言，隱變量 \( z \) 是從編碼器的輸出 \( \mu(x) \) 和 \( \sigma(x) \) 中進行重參數化，並通過加入隨機噪聲 \( \epsilon \) 來實現：

\[
z = \mu(x) + \sigma(x) \cdot \epsilon
\]

其中，\( \epsilon \) 是從標準正態分佈中抽樣的噪聲，這樣的重參數化技巧允許梯度流動並促使模型進行優化。

### 4. Python 實現：變分自編碼器範例

以下是使用 PyTorch 實現的 VAE 的範例。這個模型將用於處理 MNIST 數據集，並生成手寫數字圖像。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# 定義變分自編碼器（VAE）模型
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        # 編碼器
        self.fc1 = nn.Linear(28*28, 400)
        self.fc21 = nn.Linear(400, 20)  # 潛在變量的均值
        self.fc22 = nn.Linear(400, 20)  # 潛在變量的對數方差
        
        # 解碼器
        self.fc3 = nn.Linear(20, 400)
        self.fc4 = nn.Linear(400, 28*28)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)  # 均值和對數方差

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps * std  # 重參數化

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))  # 使用Sigmoid來確保輸出在[0,1]範圍內

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 28*28))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# 定義VAE的損失函數
def loss_function(recon_x, x, mu, logvar):
    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')
    # KL 散度
    # p(z)是標準正態分佈，q(z|x)是編碼器輸出的後驗分佈
    # KL 散度是對分佈的匹配程度進行正則化
    # 公式為：KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    # 計算潛在變量的均值和方差之間的KL散度
    # KL散度的目的是讓後驗分佈q(z|x)與標準正態分佈相近
    # 然後鼓勵VAE生成更加多樣化的數據
    # 
    # 由於損失函數是總損失，因此在總損失中將兩者加起來
    # 這樣可以平衡生成數據和潛在分佈
    # 
    # KL散度公式中：(1 + log(sigma^2) - mu^2 - sigma^2) 是對各個維度的計算
    # 我們求得總的KL散度作為VAE的損失的一部分

    # 計算 KL 散度
    # KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    # mu: 潛在變量的均值
    # logvar: 潛在變量的對數方差
    # 
    # 對每一個batch中的樣本進行KL散度的計算
    # 
    # 關於logvar的詳細計算有助於生成多樣的數據
    # 
    # 然後將KL散度與重建損失相加，這樣可以促使VAE在生成時變得更有多樣性

    # KL散度部分
    # 這段計算KL散度
    #   #將KL散度和生成損失(重建損失)
    # 
    # KL散度公式的結構